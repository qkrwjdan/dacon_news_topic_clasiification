{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943bcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import gc\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8f5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data.csv\",encoding=\"utf-8\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8c2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab  \n",
    "tokenizer = Mecab()\n",
    "train[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in train[\"title\"]]\n",
    "test[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in test[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5018e1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "      <td>[인천, →, 핀란드, 항공기, 결항, …, 휴가철, 여행객, 분통]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "      <td>[실리콘밸리, 넘어서, 겠, 다, …, 구글, 15, 조, 원, 들여, 美, 전역,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "      <td>[이란, 외무, 긴장, 완화, 해결책, 은, 미국, 이, 경제, 전쟁, 멈추, 는, 것]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "      <td>[NYT, 클린턴, 측근, 韓, 기업, 특수, 관계, 조명, …, 공과, 사, 맞물...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "      <td>[시진핑, 트럼프, 에, 중미, 무역, 협상, 조속, 타결, 희망]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             title  topic_idx  \\\n",
       "0      0          인천→핀란드 항공기 결항…휴가철 여행객 분통          4   \n",
       "1      1    실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4   \n",
       "2      2    이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4   \n",
       "3      3  NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4   \n",
       "4      4         시진핑 트럼프에 중미 무역협상 조속 타결 희망          4   \n",
       "\n",
       "                                           tokenized  \n",
       "0             [인천, →, 핀란드, 항공기, 결항, …, 휴가철, 여행객, 분통]  \n",
       "1  [실리콘밸리, 넘어서, 겠, 다, …, 구글, 15, 조, 원, 들여, 美, 전역,...  \n",
       "2  [이란, 외무, 긴장, 완화, 해결책, 은, 미국, 이, 경제, 전쟁, 멈추, 는, 것]  \n",
       "3  [NYT, 클린턴, 측근, 韓, 기업, 특수, 관계, 조명, …, 공과, 사, 맞물...  \n",
       "4              [시진핑, 트럼프, 에, 중미, 무역, 협상, 조속, 타결, 희망]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f5dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenized in train[\"tokenized\"]:\n",
    "    for token in tokenized:\n",
    "        if len(token) == 1:\n",
    "            tokenized.remove(token)  \n",
    "            \n",
    "for tokenized in test[\"tokenized\"]:\n",
    "    for token in tokenized:\n",
    "        if len(token) == 1:\n",
    "            tokenized.remove(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4b714a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e938de60444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_list = pd.concat([train,test])\n",
    "vocab_list = vocab_list[\"tokenized\"]\n",
    "vocab_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdffa2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 31593\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "vocab = FreqDist(np.hstack(vocab_list))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c83efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 5000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de56bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "word_to_index['pad'] = 1\n",
    "word_to_index['unk'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3377b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "test_x = []\n",
    "\n",
    "for line in train[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    train_x.append(temp)\n",
    "\n",
    "for line in test[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    test_x.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b8f9f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334, 0, 2535, 4771, 0, 2962, 0], [0, 0, 18, 295, 154, 9, 0, 1615, 2963], [54, 697, 985, 746, 0, 77, 84, 402, 4567, 125], [3582, 2657, 2964, 62, 1498, 450, 2247, 0, 0, 2], [550, 32, 0, 397, 202, 0, 2478, 793], [1649, 280, 0, 266, 2888, 254, 1207, 99], [477, 2061, 26, 2062, 1185, 241, 1262, 2889, 2, 15], [296, 335, 1475, 0, 0, 677, 32, 936, 538, 0, 174, 0], [812, 589, 766, 2353, 324, 960, 138, 145, 169], [710, 0, 13, 32, 3123, 339, 159, 3436, 85, 857], [0, 0, 1852, 1853, 3437, 551, 0], [54, 72, 961, 0, 455, 2536, 1762, 14, 1944], [0, 106, 678, 280, 803, 4171, 1291, 4772, 3215], [3836, 986, 1443, 1730, 241, 0, 0, 1499, 282], [95, 62, 68, 568, 16, 1894, 1071, 380], [0, 0, 1794, 643, 25, 47, 0, 215], [254, 1135, 0, 3708, 210, 1561, 202, 3124], [1263, 285, 30, 1650, 4172, 776, 393, 906], [285, 3583, 1035, 174, 987, 711, 804, 51, 1019], [0, 21, 0, 0, 0, 17, 320, 2658, 976]]\n",
      "[[1399, 199, 57, 33, 0, 53, 1390, 333], [0, 1402, 0, 841, 414, 13, 3092], [133, 45, 166, 1097, 505, 2640, 4333, 2253, 282], [0, 542, 1283, 207, 4049, 3371, 2651, 0, 0], [0, 773, 419, 0, 2357, 3153, 340, 468, 25, 507], [4159, 1888, 143, 302, 0, 993, 994, 1192], [90, 2198, 2482, 266, 258, 3280, 0, 1011, 2], [513, 2381], [0, 0, 0, 6, 0, 1567, 1895, 79, 789, 2683, 2, 15], [415, 84, 0, 497, 25, 47, 2644, 16, 2960, 378], [1320, 147, 0, 4096, 1222, 4096, 0, 589, 198, 2147], [2494, 1868, 1107, 0, 276, 0, 3922, 276, 18, 2], [684, 0, 4228, 308, 1618, 1400, 111], [0, 4382, 3, 717, 680, 0, 43, 0], [37, 61, 17, 5, 1038, 107, 450, 600, 2604], [337, 17, 47, 78, 17, 0, 9, 1780, 1951, 267, 2], [2109, 55, 0, 0, 1139, 116], [2285, 1397, 4931, 1792, 325, 921, 339, 348, 2747, 1150], [2062, 0, 2480, 64, 178, 42, 1061], [164, 825, 0, 2320, 0, 232, 133, 823, 4687, 2644, 588]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[:20])\n",
    "print(test_x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "648ccef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in train_x)\n",
    "max_len = max(len(l) for l in test_x)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf1d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in train_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다.\n",
    "        \n",
    "for line in test_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2a52e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 18\n",
      "리뷰의 최소 길이 : 18\n",
      "리뷰의 평균 길이 : 18.000000\n",
      "리뷰의 최대 길이 : 18\n",
      "리뷰의 최소 길이 : 18\n",
      "리뷰의 평균 길이 : 18.000000\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 : %d' % max(len(l) for l in train_x))\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in train_x))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, train_x))/len(train_x)))\n",
    "      \n",
    "print('리뷰의 최대 길이 : %d' % max(len(l) for l in test_x))\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in test_x))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, test_x))/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c7435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "(45654, 7)\n"
     ]
    }
   ],
   "source": [
    "# 종속변수 데이터 전처리\n",
    "train_y = np_utils.to_categorical(train[\"topic_idx\"]) # Y_train 에 원-핫 인코딩\n",
    "print(train_y)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68d2a585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 334,    0, 2535, ...,    1,    1,    1],\n",
       "       [   0,    0,   18, ...,    1,    1,    1],\n",
       "       [  54,  697,  985, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [  23, 1880,  968, ...,    1,    1,    1],\n",
       "       [2989,    4,    0, ...,    1,    1,    1],\n",
       "       [ 968,    8,  244, ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42066995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 설정\n",
    "vocab_size = 5003 # 제일 많이 사용하는 사이즈\n",
    "embedding_dim = 200  \n",
    "max_length = 18    # 위에서 그래프 확인 후 정함\n",
    "padding_type='post'\n",
    "#oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32e8b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 18, 200)           1000600   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 18, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 18, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,334,815\n",
      "Trainable params: 1,334,815\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 양방향 LSTM 레이어를 사용한 모델 (model3) 정의\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length =max_length),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "        Dense(7, activation='softmax')    # 결과값이 0~4 이므로 Dense(5)\n",
    "    ])\n",
    "    \n",
    "model.compile(loss= 'categorical_crossentropy', #여러개 정답 중 하나 맞추는 문제이므로 손실 함수는 categorical_crossentropy\n",
    "              optimizer= 'adam',\n",
    "              metrics = ['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee549a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45654, 45654, 9131)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train[\"topic_idx\"]\n",
    "len(train_x),len(train_y), len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f180ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_result, test_pred_result = np.zeros((45654,7)),np.zeros((9131,7))\n",
    "best_val_train_pred, best_val_test_pred = np.zeros((45654,7)),np.zeros((9131,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85aa1f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 16s 159ms/step - loss: 1.1963 - accuracy: 0.5324 - val_loss: 0.6148 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61476, saving model to models/nn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.4903 - accuracy: 0.8398 - val_loss: 0.5419 - val_accuracy: 0.8215\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61476 to 0.54190, saving model to models/nn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 148ms/step - loss: 0.3826 - accuracy: 0.8764 - val_loss: 0.5345 - val_accuracy: 0.8236\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54190 to 0.53448, saving model to models/nn_model.h5\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 151ms/step - loss: 0.3230 - accuracy: 0.8964 - val_loss: 0.5748 - val_accuracy: 0.8180\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53448\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 11s 153ms/step - loss: 0.2882 - accuracy: 0.9063 - val_loss: 0.5910 - val_accuracy: 0.8154\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53448\n",
      "------------------\n",
      "training model for CV #2\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 17s 171ms/step - loss: 1.2162 - accuracy: 0.5380 - val_loss: 0.6603 - val_accuracy: 0.7782\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66028, saving model to models/nn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 158ms/step - loss: 0.5281 - accuracy: 0.8265 - val_loss: 0.5655 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66028 to 0.56551, saving model to models/nn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 158ms/step - loss: 0.4071 - accuracy: 0.8680 - val_loss: 0.5783 - val_accuracy: 0.8115\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56551\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 159ms/step - loss: 0.3488 - accuracy: 0.8866 - val_loss: 0.5980 - val_accuracy: 0.8064\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56551\n",
      "------------------\n",
      "training model for CV #3\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 17s 178ms/step - loss: 1.2047 - accuracy: 0.5412 - val_loss: 0.6178 - val_accuracy: 0.7999\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61777, saving model to models/nn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 12s 170ms/step - loss: 0.4868 - accuracy: 0.8411 - val_loss: 0.5645 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61777 to 0.56449, saving model to models/nn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 159ms/step - loss: 0.3840 - accuracy: 0.8748 - val_loss: 0.5888 - val_accuracy: 0.8048\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56449\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.3313 - accuracy: 0.8929 - val_loss: 0.6095 - val_accuracy: 0.8051\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56449\n",
      "------------------\n",
      "training model for CV #4\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 16s 162ms/step - loss: 1.1626 - accuracy: 0.5656 - val_loss: 0.5827 - val_accuracy: 0.8006\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58265, saving model to models/nn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.4843 - accuracy: 0.8422 - val_loss: 0.5468 - val_accuracy: 0.8159\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58265 to 0.54680, saving model to models/nn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 150ms/step - loss: 0.3869 - accuracy: 0.8750 - val_loss: 0.5581 - val_accuracy: 0.8132\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54680\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 156ms/step - loss: 0.3391 - accuracy: 0.8899 - val_loss: 0.5742 - val_accuracy: 0.8113\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54680\n",
      "------------------\n",
      "training model for CV #5\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 16s 164ms/step - loss: 1.2565 - accuracy: 0.5162 - val_loss: 0.7055 - val_accuracy: 0.7599\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70552, saving model to models/nn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 151ms/step - loss: 0.5326 - accuracy: 0.8263 - val_loss: 0.5615 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70552 to 0.56153, saving model to models/nn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 151ms/step - loss: 0.3930 - accuracy: 0.8739 - val_loss: 0.5544 - val_accuracy: 0.8180\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56153 to 0.55437, saving model to models/nn_model.h5\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.3282 - accuracy: 0.8931 - val_loss: 0.5840 - val_accuracy: 0.8141\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55437\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 11s 150ms/step - loss: 0.2831 - accuracy: 0.9098 - val_loss: 0.6224 - val_accuracy: 0.8088\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55437\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# 계층 교차 검증\n",
    "n_fold = 5  \n",
    "seed = 42\n",
    "MODEL_P = 'models/nn_model.h5'\n",
    "FEAT_CNT = 5\n",
    "\n",
    "cv = StratifiedKFold(n_splits = FEAT_CNT, shuffle=True, random_state=seed)\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(train_x, train_y), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    \n",
    "    model = Sequential([Embedding(vocab_size, embedding_dim, input_length =max_length),\n",
    "            tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "            tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "            tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "            Dense(7, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(loss= 'categorical_crossentropy', \n",
    "                  optimizer= 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    es=EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x[i_trn], \n",
    "        to_categorical(train_y[i_trn]),\n",
    "        validation_data=(train_x[i_val], to_categorical(train_y[i_val])),\n",
    "        epochs=10,\n",
    "        batch_size=512,\n",
    "        callbacks=[es, mc])\n",
    "    # feature 생성 1\n",
    "    train_pred_result[i_trn] = model.predict(train_x[i_trn])\n",
    "    test_pred_result += model.predict(test_x)/FEAT_CNT\n",
    "\n",
    "    # feature 생성 2\n",
    "    model = load_model(MODEL_P)\n",
    "    best_val_train_pred[i_val] = model.predict(train_x[i_val])\n",
    "    best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c5c85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_result2, test_pred_result2 = np.zeros((45654,7)),np.zeros((9131,7))\n",
    "best_val_train_pred2, best_val_test_pred2 = np.zeros((45654,7)),np.zeros((9131,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffcebb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.6859 - accuracy: 0.4313 - val_loss: 1.0037 - val_accuracy: 0.7420\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00370, saving model to models/dnn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.6568 - accuracy: 0.7906 - val_loss: 0.5322 - val_accuracy: 0.8216\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00370 to 0.53219, saving model to models/dnn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.4428 - accuracy: 0.8524 - val_loss: 0.5137 - val_accuracy: 0.8238\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53219 to 0.51366, saving model to models/dnn_model.h5\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.3745 - accuracy: 0.8749 - val_loss: 0.5147 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51366\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.3311 - accuracy: 0.8894 - val_loss: 0.5376 - val_accuracy: 0.8269\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51366\n",
      "------------------\n",
      "training model for CV #2\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 1.6673 - accuracy: 0.4277 - val_loss: 0.9588 - val_accuracy: 0.7432\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95880, saving model to models/dnn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.6483 - accuracy: 0.7932 - val_loss: 0.5434 - val_accuracy: 0.8228\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95880 to 0.54337, saving model to models/dnn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.4326 - accuracy: 0.8568 - val_loss: 0.5188 - val_accuracy: 0.8243\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54337 to 0.51876, saving model to models/dnn_model.h5\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.3647 - accuracy: 0.8786 - val_loss: 0.5352 - val_accuracy: 0.8186\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51876\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.3233 - accuracy: 0.8918 - val_loss: 0.5515 - val_accuracy: 0.8185\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51876\n",
      "------------------\n",
      "training model for CV #3\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 1.6800 - accuracy: 0.4118 - val_loss: 1.0183 - val_accuracy: 0.7158\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01831, saving model to models/dnn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.6750 - accuracy: 0.7854 - val_loss: 0.5788 - val_accuracy: 0.8126\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01831 to 0.57882, saving model to models/dnn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.4560 - accuracy: 0.8516 - val_loss: 0.5508 - val_accuracy: 0.8210\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57882 to 0.55083, saving model to models/dnn_model.h5\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.3826 - accuracy: 0.8743 - val_loss: 0.5562 - val_accuracy: 0.8189\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55083\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.3367 - accuracy: 0.8897 - val_loss: 0.5658 - val_accuracy: 0.8203\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55083\n",
      "------------------\n",
      "training model for CV #4\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 1.6930 - accuracy: 0.3790 - val_loss: 1.0866 - val_accuracy: 0.6578\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.08657, saving model to models/dnn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.7468 - accuracy: 0.7568 - val_loss: 0.5817 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.08657 to 0.58173, saving model to models/dnn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.4671 - accuracy: 0.8489 - val_loss: 0.5258 - val_accuracy: 0.8244\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58173 to 0.52581, saving model to models/dnn_model.h5\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.3810 - accuracy: 0.8751 - val_loss: 0.5178 - val_accuracy: 0.8274\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52581 to 0.51781, saving model to models/dnn_model.h5\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.3294 - accuracy: 0.8914 - val_loss: 0.5343 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51781\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2988 - accuracy: 0.9001 - val_loss: 0.5555 - val_accuracy: 0.8194\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51781\n",
      "------------------\n",
      "training model for CV #5\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 1.6909 - accuracy: 0.4024 - val_loss: 1.0271 - val_accuracy: 0.7076\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02707, saving model to models/dnn_model.h5\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.6738 - accuracy: 0.7848 - val_loss: 0.5412 - val_accuracy: 0.8175\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02707 to 0.54121, saving model to models/dnn_model.h5\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.4503 - accuracy: 0.8516 - val_loss: 0.5313 - val_accuracy: 0.8216\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54121 to 0.53130, saving model to models/dnn_model.h5\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.3835 - accuracy: 0.8742 - val_loss: 0.5343 - val_accuracy: 0.8228\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53130\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.3440 - accuracy: 0.8855 - val_loss: 0.5538 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53130\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# 계층 교차 검증\n",
    "n_fold = 5  \n",
    "seed = 42\n",
    "MODEL_P = 'models/dnn_model.h5'\n",
    "FEAT_CNT = 5\n",
    "\n",
    "cv = StratifiedKFold(n_splits = FEAT_CNT, shuffle=True, random_state=seed)\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(train_x, train_y), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    \n",
    "    model = Sequential([Embedding(vocab_size, embedding_dim, input_length =max_length),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            Dense(128,activation=\"relu\"),\n",
    "            Dense(128,activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(7, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(loss= 'categorical_crossentropy', \n",
    "                  optimizer= 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    es=EarlyStopping(monitor='val_loss', patience=2)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x[i_trn], \n",
    "        to_categorical(train_y[i_trn]),\n",
    "        validation_data=(train_x[i_val], to_categorical(train_y[i_val])),\n",
    "        epochs=10,\n",
    "        batch_size=512,\n",
    "        callbacks=[es, mc])\n",
    "    # feature 생성 1\n",
    "    train_pred_result2[i_trn] = model.predict(train_x[i_trn])\n",
    "    test_pred_result2 += model.predict(test_x)/FEAT_CNT\n",
    "\n",
    "    # feature 생성 2\n",
    "    model = load_model(MODEL_P)\n",
    "    best_val_train_pred2[i_val] = model.predict(train_x[i_val])\n",
    "    best_val_test_pred2 += model.predict(test_x)/FEAT_CNT\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b5b32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nn_train = np.hstack([train_pred_result, best_val_train_pred, \n",
    "                        train_pred_result2, best_val_train_pred2\n",
    "                        ])\n",
    "\n",
    "all_nn_test = np.hstack([test_pred_result, best_val_test_pred, \n",
    "                        test_pred_result2, best_val_test_pred2\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52de950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "      <td>[인천, 핀란드, 항공기, 결항, 휴가철, 여행객, 분통]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "      <td>[실리콘밸리, 넘어서, 다, 구글, 15, 원, 들여, 전역, 거점]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "      <td>[이란, 외무, 긴장, 완화, 해결책, 미국, 경제, 전쟁, 멈추, 것]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "      <td>[NYT, 클린턴, 측근, 기업, 특수, 관계, 조명, 공과, 맞물려, 종합]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "      <td>[시진핑, 트럼프, 중미, 무역, 협상, 조속, 타결, 희망]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             title  topic_idx  \\\n",
       "0      0          인천→핀란드 항공기 결항…휴가철 여행객 분통          4   \n",
       "1      1    실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4   \n",
       "2      2    이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4   \n",
       "3      3  NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4   \n",
       "4      4         시진핑 트럼프에 중미 무역협상 조속 타결 희망          4   \n",
       "\n",
       "                                     tokenized  \n",
       "0             [인천, 핀란드, 항공기, 결항, 휴가철, 여행객, 분통]  \n",
       "1       [실리콘밸리, 넘어서, 다, 구글, 15, 원, 들여, 전역, 거점]  \n",
       "2     [이란, 외무, 긴장, 완화, 해결책, 미국, 경제, 전쟁, 멈추, 것]  \n",
       "3  [NYT, 클린턴, 측근, 기업, 특수, 관계, 조명, 공과, 맞물려, 종합]  \n",
       "4           [시진핑, 트럼프, 중미, 무역, 협상, 조속, 타결, 희망]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a75485b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>유튜브 내달 2일까지 크리에이터 지원 공간 운영</td>\n",
       "      <td>[유튜브, 내달, 일, 까지, 크리에이터, 지원, 공간, 운영]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>어버이날 맑다가 흐려져…남부지방 옅은 황사</td>\n",
       "      <td>[어버이날, 다가, 흐려져, 남부, 지방, 은, 황사]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>내년부터 국가RD 평가 때 논문건수는 반영 않는다</td>\n",
       "      <td>[내년, 부터, 국가, RD, 평가, 논문, 건수, 반영, 는다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것</td>\n",
       "      <td>[김명자, 신임, 총, 회장, 원로, 젊, 과학자, 지혜, 모을]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간</td>\n",
       "      <td>[회색, 인간, 작가, 김동식, 심, 백, 새, 소설, 2, 출간]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                            title  \\\n",
       "0  45654       유튜브 내달 2일까지 크리에이터 지원 공간 운영   \n",
       "1  45655          어버이날 맑다가 흐려져…남부지방 옅은 황사   \n",
       "2  45656      내년부터 국가RD 평가 때 논문건수는 반영 않는다   \n",
       "3  45657  김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것   \n",
       "4  45658   회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간   \n",
       "\n",
       "                               tokenized  \n",
       "0    [유튜브, 내달, 일, 까지, 크리에이터, 지원, 공간, 운영]  \n",
       "1         [어버이날, 다가, 흐려져, 남부, 지방, 은, 황사]  \n",
       "2   [내년, 부터, 국가, RD, 평가, 논문, 건수, 반영, 는다]  \n",
       "3   [김명자, 신임, 총, 회장, 원로, 젊, 과학자, 지혜, 모을]  \n",
       "4  [회색, 인간, 작가, 김동식, 심, 백, 새, 소설, 2, 출간]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fc73903",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['index', 'title','tokenized']\n",
    "train_X = train.drop(cols_to_drop+['topic_idx'], axis=1).values\n",
    "test_X = test.drop(cols_to_drop, axis=1).values\n",
    "\n",
    "\n",
    "# print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a6342c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.hstack([train_X, all_nn_train])\n",
    "test_X = np.hstack([test_X, all_nn_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f718b38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45654, 28) (9131, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99275d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.82537\tvalid-mlogloss:1.82500\n",
      "[200]\ttrain-mlogloss:0.15633\tvalid-mlogloss:0.17238\n",
      "[400]\ttrain-mlogloss:0.12645\tvalid-mlogloss:0.16221\n",
      "[600]\ttrain-mlogloss:0.10792\tvalid-mlogloss:0.16078\n",
      "[643]\ttrain-mlogloss:0.10457\tvalid-mlogloss:0.16079\n",
      "train log loss 0.10457366169590804 valid log loss 0.16078699502335145\n",
      "rev 6.219408477997662\n",
      "[0]\ttrain-mlogloss:1.82516\tvalid-mlogloss:1.82520\n",
      "[200]\ttrain-mlogloss:0.15516\tvalid-mlogloss:0.17783\n",
      "[400]\ttrain-mlogloss:0.12458\tvalid-mlogloss:0.16954\n",
      "[600]\ttrain-mlogloss:0.10568\tvalid-mlogloss:0.16918\n",
      "[618]\ttrain-mlogloss:0.10424\tvalid-mlogloss:0.16922\n",
      "train log loss 0.10423648896318827 valid log loss 0.16922324570107186\n",
      "rev 5.909353622530513\n",
      "[0]\ttrain-mlogloss:1.82513\tvalid-mlogloss:1.82502\n",
      "[200]\ttrain-mlogloss:0.15322\tvalid-mlogloss:0.18491\n",
      "[400]\ttrain-mlogloss:0.12326\tvalid-mlogloss:0.17714\n",
      "[534]\ttrain-mlogloss:0.11039\tvalid-mlogloss:0.17687\n",
      "train log loss 0.1103092626503029 valid log loss 0.17688975450160221\n",
      "rev 5.653238667313218\n",
      "[0]\ttrain-mlogloss:1.82528\tvalid-mlogloss:1.82500\n",
      "[200]\ttrain-mlogloss:0.15581\tvalid-mlogloss:0.17567\n",
      "[400]\ttrain-mlogloss:0.12504\tvalid-mlogloss:0.16796\n",
      "[466]\ttrain-mlogloss:0.11808\tvalid-mlogloss:0.16809\n",
      "train log loss 0.11799522491084044 valid log loss 0.16811125458839388\n",
      "rev 5.948441717649511\n",
      "[0]\ttrain-mlogloss:1.82455\tvalid-mlogloss:1.82906\n",
      "[200]\ttrain-mlogloss:0.15366\tvalid-mlogloss:0.21236\n",
      "[400]\ttrain-mlogloss:0.12398\tvalid-mlogloss:0.20275\n",
      "[475]\ttrain-mlogloss:0.11635\tvalid-mlogloss:0.20301\n",
      "train log loss 0.1163449698834861 valid log loss 0.20300553192390025\n",
      "rev 4.92597413736915\n",
      "28.656416622860053\n",
      "local average valid loss 0.17560335634766394\n",
      "train log loss 0.11717227899863086\n"
     ]
    }
   ],
   "source": [
    "rnd = 42\n",
    "k_cnt = FEAT_CNT\n",
    "\n",
    "kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "\n",
    "test_pred = None\n",
    "weighted_test_pred = None\n",
    "org_train_pred = None\n",
    "avg_k_score = 0\n",
    "reverse_score = 0\n",
    "best_loss = 100\n",
    "best_single_pred = None\n",
    "\n",
    "train_Y = train_y\n",
    "\n",
    "for train_index, test_index in kf.split(train_X,train_Y):\n",
    "    X_train, X_test = train_X[train_index], train_X[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    \n",
    "    params = {\n",
    "            'colsample_bytree': 0.7,\n",
    "            'subsample': 0.8,\n",
    "            'eta': 0.04,\n",
    "            'max_depth': 3,\n",
    "            'eval_metric':'mlogloss',\n",
    "            'objective':'multi:softprob',\n",
    "            'num_class':7,\n",
    "            'tree_method':'gpu_hist'\n",
    "    }\n",
    "    \n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_test, y_test)\n",
    "    d_test = xgb.DMatrix(test_X)\n",
    "    \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=200)\n",
    "    \n",
    "    train_pred = m.predict(d_train)\n",
    "    valid_pred = m.predict(d_valid)\n",
    "    tmp_train_pred = m.predict(xgb.DMatrix(train_X))\n",
    "    \n",
    "    train_score = log_loss(y_train,train_pred)\n",
    "    valid_score = log_loss(y_test,valid_pred)\n",
    "    print('train log loss',train_score,'valid log loss',valid_score)\n",
    "    avg_k_score += valid_score\n",
    "    rev_valid_score = 1.0/valid_score\n",
    "    reverse_score += rev_valid_score\n",
    "    print('rev',rev_valid_score)\n",
    "    \n",
    "    if test_pred is None:\n",
    "        test_pred = m.predict(d_test)\n",
    "        weighted_test_pred = test_pred*rev_valid_score\n",
    "        org_train_pred = tmp_train_pred\n",
    "        best_loss = valid_score\n",
    "        best_single_pred = test_pred\n",
    "    else:\n",
    "        curr_pred = m.predict(d_test)\n",
    "        test_pred += curr_pred\n",
    "        weighted_test_pred += curr_pred*rev_valid_score\n",
    "        org_train_pred += tmp_train_pred\n",
    "\n",
    "        if valid_score < best_loss:\n",
    "            print('BETTER')\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = curr_pred\n",
    "\n",
    "test_pred = test_pred / k_cnt\n",
    "test_pred = np.round(test_pred,4)\n",
    "org_train_pred = org_train_pred / k_cnt\n",
    "avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss['0']=test_pred[:,0]\n",
    "submiss['1']=test_pred[:,1]\n",
    "submiss['2']=test_pred[:,2]\n",
    "submiss['3']=test_pred[:,3]\n",
    "submiss['4']=test_pred[:,4]\n",
    "submiss['5']=test_pred[:,5]\n",
    "submiss['6']=test_pred[:,6]\n",
    "submiss.to_csv(\"results/xgb_{}.csv\".format(k_cnt),index=False)\n",
    "print(reverse_score)\n",
    "\n",
    "# weigthed\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "weighted_test_pred = weighted_test_pred / reverse_score\n",
    "weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "submiss['0']=weighted_test_pred[:,0]\n",
    "submiss['1']=weighted_test_pred[:,1]\n",
    "submiss['2']=weighted_test_pred[:,2]\n",
    "submiss['3']=weighted_test_pred[:,3]\n",
    "submiss['4']=weighted_test_pred[:,4]\n",
    "submiss['5']=weighted_test_pred[:,5]\n",
    "submiss['6']=weighted_test_pred[:,6]\n",
    "submiss.to_csv(\"results/weighted_{}.csv\".format(k_cnt),index=False)\n",
    "\n",
    "# best single\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "weighted_test_pred = np.round(best_single_pred,4)\n",
    "submiss['0']=weighted_test_pred[:,0]\n",
    "submiss['1']=weighted_test_pred[:,1]\n",
    "submiss['2']=weighted_test_pred[:,2]\n",
    "submiss['3']=weighted_test_pred[:,3]\n",
    "submiss['4']=weighted_test_pred[:,4]\n",
    "submiss['4']=weighted_test_pred[:,5]\n",
    "submiss['4']=weighted_test_pred[:,6]\n",
    "submiss.to_csv(\"results/single_{}.csv\".format(k_cnt),index=False)\n",
    "\n",
    "# train log loss\n",
    "print('local average valid loss',avg_k_score)\n",
    "print('train log loss', log_loss(train_Y,org_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fcf3738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single = pd.read_csv(\"results/single_5.csv\")\n",
    "weighted = pd.read_csv(\"results/single_5.csv\")\n",
    "xgboosted = pd.read_csv(\"results/xgb_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "834776a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboosted[\"topic_idx\"] = [ np.argmax(li,axis=1) for li in xgboosted[[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]] ]\n",
    "idx_list = []\n",
    "for i in xgboosted.index:\n",
    "    val = xgboosted.loc[i,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]]\n",
    "    val.to_list()\n",
    "    idx = np.argmax(val)\n",
    "    idx_list.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1112f75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 6, 2, 3, 3, 5, 3, 4, 4]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3f28545",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss[\"topic_idx\"] = idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb74f287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  topic_idx\n",
       "0  45654          2\n",
       "1  45655          3\n",
       "2  45656          6\n",
       "3  45657          2\n",
       "4  45658          3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submiss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a87eeaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss.to_csv(\"results/xgboost_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3d572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
