{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0475cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b77578",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data.csv\",encoding=\"utf-8\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de497353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab  \n",
    "tokenizer = Mecab()\n",
    "train[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in train[\"title\"]]\n",
    "test[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in test[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28e2c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "      <td>[인천, →, 핀란드, 항공기, 결항, …, 휴가철, 여행객, 분통]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "      <td>[실리콘밸리, 넘어서, 겠, 다, …, 구글, 15, 조, 원, 들여, 美, 전역,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "      <td>[이란, 외무, 긴장, 완화, 해결책, 은, 미국, 이, 경제, 전쟁, 멈추, 는, 것]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "      <td>[NYT, 클린턴, 측근, 韓, 기업, 특수, 관계, 조명, …, 공과, 사, 맞물...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "      <td>[시진핑, 트럼프, 에, 중미, 무역, 협상, 조속, 타결, 희망]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             title  topic_idx  \\\n",
       "0      0          인천→핀란드 항공기 결항…휴가철 여행객 분통          4   \n",
       "1      1    실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4   \n",
       "2      2    이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4   \n",
       "3      3  NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4   \n",
       "4      4         시진핑 트럼프에 중미 무역협상 조속 타결 희망          4   \n",
       "\n",
       "                                           tokenized  \n",
       "0             [인천, →, 핀란드, 항공기, 결항, …, 휴가철, 여행객, 분통]  \n",
       "1  [실리콘밸리, 넘어서, 겠, 다, …, 구글, 15, 조, 원, 들여, 美, 전역,...  \n",
       "2  [이란, 외무, 긴장, 완화, 해결책, 은, 미국, 이, 경제, 전쟁, 멈추, 는, 것]  \n",
       "3  [NYT, 클린턴, 측근, 韓, 기업, 특수, 관계, 조명, …, 공과, 사, 맞물...  \n",
       "4              [시진핑, 트럼프, 에, 중미, 무역, 협상, 조속, 타결, 희망]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf82030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>유튜브 내달 2일까지 크리에이터 지원 공간 운영</td>\n",
       "      <td>[유튜브, 내달, 2, 일, 까지, 크리에이터, 지원, 공간, 운영]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>어버이날 맑다가 흐려져…남부지방 옅은 황사</td>\n",
       "      <td>[어버이날, 맑, 다가, 흐려져, …, 남부, 지방, 옅, 은, 황사]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>내년부터 국가RD 평가 때 논문건수는 반영 않는다</td>\n",
       "      <td>[내년, 부터, 국가, RD, 평가, 때, 논문, 건수, 는, 반영, 않, 는다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것</td>\n",
       "      <td>[김명자, 신임, 과, 총, 회장, 원로, 와, 젊, 은, 과학자, 지혜, 모을, 것]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간</td>\n",
       "      <td>[회색, 인간, 작가, 김동식, 양, 심, 고, 백, 등, 새, 소설, 집, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                            title  \\\n",
       "0  45654       유튜브 내달 2일까지 크리에이터 지원 공간 운영   \n",
       "1  45655          어버이날 맑다가 흐려져…남부지방 옅은 황사   \n",
       "2  45656      내년부터 국가RD 평가 때 논문건수는 반영 않는다   \n",
       "3  45657  김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것   \n",
       "4  45658   회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간   \n",
       "\n",
       "                                           tokenized  \n",
       "0             [유튜브, 내달, 2, 일, 까지, 크리에이터, 지원, 공간, 운영]  \n",
       "1            [어버이날, 맑, 다가, 흐려져, …, 남부, 지방, 옅, 은, 황사]  \n",
       "2      [내년, 부터, 국가, RD, 평가, 때, 논문, 건수, 는, 반영, 않, 는다]  \n",
       "3   [김명자, 신임, 과, 총, 회장, 원로, 와, 젊, 은, 과학자, 지혜, 모을, 것]  \n",
       "4  [회색, 인간, 작가, 김동식, 양, 심, 고, 백, 등, 새, 소설, 집, 2, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eaf6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenized in train[\"tokenized\"]:\n",
    "    for token in tokenized:\n",
    "        if len(token) == 1:\n",
    "            tokenized.remove(token)  \n",
    "            \n",
    "for tokenized in test[\"tokenized\"]:\n",
    "    for token in tokenized:\n",
    "        if len(token) == 1:\n",
    "            tokenized.remove(token)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be545956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "      <td>[인천, 핀란드, 항공기, 결항, 휴가철, 여행객, 분통]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "      <td>[실리콘밸리, 넘어서, 다, 구글, 15, 원, 들여, 전역, 거점]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "      <td>[이란, 외무, 긴장, 완화, 해결책, 미국, 경제, 전쟁, 멈추, 것]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "      <td>[NYT, 클린턴, 측근, 기업, 특수, 관계, 조명, 공과, 맞물려, 종합]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "      <td>[시진핑, 트럼프, 중미, 무역, 협상, 조속, 타결, 희망]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             title  topic_idx  \\\n",
       "0      0          인천→핀란드 항공기 결항…휴가철 여행객 분통          4   \n",
       "1      1    실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4   \n",
       "2      2    이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4   \n",
       "3      3  NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4   \n",
       "4      4         시진핑 트럼프에 중미 무역협상 조속 타결 희망          4   \n",
       "\n",
       "                                     tokenized  \n",
       "0             [인천, 핀란드, 항공기, 결항, 휴가철, 여행객, 분통]  \n",
       "1       [실리콘밸리, 넘어서, 다, 구글, 15, 원, 들여, 전역, 거점]  \n",
       "2     [이란, 외무, 긴장, 완화, 해결책, 미국, 경제, 전쟁, 멈추, 것]  \n",
       "3  [NYT, 클린턴, 측근, 기업, 특수, 관계, 조명, 공과, 맞물려, 종합]  \n",
       "4           [시진핑, 트럼프, 중미, 무역, 협상, 조속, 타결, 희망]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbcabccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>유튜브 내달 2일까지 크리에이터 지원 공간 운영</td>\n",
       "      <td>[유튜브, 내달, 일, 까지, 크리에이터, 지원, 공간, 운영]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>어버이날 맑다가 흐려져…남부지방 옅은 황사</td>\n",
       "      <td>[어버이날, 다가, 흐려져, 남부, 지방, 은, 황사]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>내년부터 국가RD 평가 때 논문건수는 반영 않는다</td>\n",
       "      <td>[내년, 부터, 국가, RD, 평가, 논문, 건수, 반영, 는다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것</td>\n",
       "      <td>[김명자, 신임, 총, 회장, 원로, 젊, 과학자, 지혜, 모을]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간</td>\n",
       "      <td>[회색, 인간, 작가, 김동식, 심, 백, 새, 소설, 2, 출간]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                            title  \\\n",
       "0  45654       유튜브 내달 2일까지 크리에이터 지원 공간 운영   \n",
       "1  45655          어버이날 맑다가 흐려져…남부지방 옅은 황사   \n",
       "2  45656      내년부터 국가RD 평가 때 논문건수는 반영 않는다   \n",
       "3  45657  김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것   \n",
       "4  45658   회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간   \n",
       "\n",
       "                               tokenized  \n",
       "0    [유튜브, 내달, 일, 까지, 크리에이터, 지원, 공간, 운영]  \n",
       "1         [어버이날, 다가, 흐려져, 남부, 지방, 은, 황사]  \n",
       "2   [내년, 부터, 국가, RD, 평가, 논문, 건수, 반영, 는다]  \n",
       "3   [김명자, 신임, 총, 회장, 원로, 젊, 과학자, 지혜, 모을]  \n",
       "4  [회색, 인간, 작가, 김동식, 심, 백, 새, 소설, 2, 출간]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbc3f003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      [인천, 핀란드, 항공기, 결항, 휴가철, 여행객, 분통]\n",
       "1                [실리콘밸리, 넘어서, 다, 구글, 15, 원, 들여, 전역, 거점]\n",
       "2              [이란, 외무, 긴장, 완화, 해결책, 미국, 경제, 전쟁, 멈추, 것]\n",
       "3           [NYT, 클린턴, 측근, 기업, 특수, 관계, 조명, 공과, 맞물려, 종합]\n",
       "4                    [시진핑, 트럼프, 중미, 무역, 협상, 조속, 타결, 희망]\n",
       "5                  [팔레스타인, 자, 구서, 16, 소년, 이스라엘, 총격, 사망]\n",
       "6              [인도, 48, 만, 파키스탄, 공습, 테러, 캠프, 폭격, 종합, 보]\n",
       "7     [대선, TV, 토론, 음담패설, 만회, 실패, 트럼프, 사과, 대신, 빌클린턴, ...\n",
       "8               [푸틴, 한반도, 상황, 진전, 위한, 방안, 김정은, 위원장, 논의]\n",
       "9             [특검, 면죄부, 은, 트럼프, 스캔들, 보도, 언론, 맹공, 국민, 적]\n",
       "10                      [오키, 나와서, 열린, 강제, 징용, 노동자, 추도식]\n",
       "11               [이란, 최고, 지도자, 모욕, 혐의, 미국인, 징역, 10, 선고]\n",
       "12                [카니발, 축제, 러, 자, 브라질, 리우, 대형, 유람선, 행렬]\n",
       "13               [올랜도, 병원, 최악, 총기, 테러, 부상자, 치료비, 받, 는다]\n",
       "14                       [대, 기업, 올해, 평균, ., 46, 임금, 인상]\n",
       "15                    [WMO, 엘니뇨, 여전히, 강력, 2, 분기, 소멸, 듯]\n",
       "16                [이스라엘, 네타냐후, 유대교, 병역, 문제, 연정, 협상, 진통]\n",
       "17                  [UAE, 사우디, 어, 호르무즈, 호위, 연합, 참여, 키로]\n",
       "18                [사우디, 오만, 유조선, 공격, 예멘, 반군, 연결, 이, 겨냥]\n",
       "19               [개천, 에서, 나와라, 사업가, 모교, 1, 천억, 장학금, 기부]\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.concat([train,test])\n",
    "vocab_list = vocab_list[\"tokenized\"]\n",
    "vocab_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9241a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 31593\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "vocab = FreqDist(np.hstack(vocab_list))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8380f697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 5000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f26157d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "word_to_index['pad'] = 1\n",
    "word_to_index['unk'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad5238aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "test_x = []\n",
    "\n",
    "for line in train[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    train_x.append(temp)\n",
    "\n",
    "for line in test[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    test_x.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f9988ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334, 0, 2535, 4771, 0, 2962, 0], [0, 0, 18, 295, 154, 9, 0, 1615, 2963], [54, 697, 985, 746, 0, 77, 84, 402, 4567, 125], [3582, 2657, 2964, 62, 1498, 450, 2247, 0, 0, 2], [550, 32, 0, 397, 202, 0, 2478, 793], [1649, 280, 0, 266, 2888, 254, 1207, 99], [477, 2061, 26, 2062, 1185, 241, 1262, 2889, 2, 15], [296, 335, 1475, 0, 0, 677, 32, 936, 538, 0, 174, 0], [812, 589, 766, 2353, 324, 960, 138, 145, 169], [710, 0, 13, 32, 3123, 339, 159, 3436, 85, 857], [0, 0, 1852, 1853, 3437, 551, 0], [54, 72, 961, 0, 455, 2536, 1762, 14, 1944], [0, 106, 678, 280, 803, 4171, 1291, 4772, 3215], [3836, 986, 1443, 1730, 241, 0, 0, 1499, 282], [95, 62, 68, 568, 16, 1894, 1071, 380], [0, 0, 1794, 643, 25, 47, 0, 215], [254, 1135, 0, 3708, 210, 1561, 202, 3124], [1263, 285, 30, 1650, 4172, 776, 393, 906], [285, 3583, 1035, 174, 987, 711, 804, 51, 1019], [0, 21, 0, 0, 0, 17, 320, 2658, 976]]\n",
      "[[1399, 199, 57, 33, 0, 53, 1390, 333], [0, 1402, 0, 841, 414, 13, 3092], [133, 45, 166, 1097, 505, 2640, 4333, 2253, 282], [0, 542, 1283, 207, 4049, 3371, 2651, 0, 0], [0, 773, 419, 0, 2357, 3153, 340, 468, 25, 507], [4159, 1888, 143, 302, 0, 993, 994, 1192], [90, 2198, 2482, 266, 258, 3280, 0, 1011, 2], [513, 2381], [0, 0, 0, 6, 0, 1567, 1895, 79, 789, 2683, 2, 15], [415, 84, 0, 497, 25, 47, 2644, 16, 2960, 378], [1320, 147, 0, 4096, 1222, 4096, 0, 589, 198, 2147], [2494, 1868, 1107, 0, 276, 0, 3922, 276, 18, 2], [684, 0, 4228, 308, 1618, 1400, 111], [0, 4382, 3, 717, 680, 0, 43, 0], [37, 61, 17, 5, 1038, 107, 450, 600, 2604], [337, 17, 47, 78, 17, 0, 9, 1780, 1951, 267, 2], [2109, 55, 0, 0, 1139, 116], [2285, 1397, 4931, 1792, 325, 921, 339, 348, 2747, 1150], [2062, 0, 2480, 64, 178, 42, 1061], [164, 825, 0, 2320, 0, 232, 133, 823, 4687, 2644, 588]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[:20])\n",
    "print(test_x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5834c654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in train_x)\n",
    "max_len = max(len(l) for l in test_x)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac3f5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in train_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다.\n",
    "        \n",
    "for line in test_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4f41be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 18\n",
      "리뷰의 최소 길이 : 18\n",
      "리뷰의 평균 길이 : 18.000000\n",
      "리뷰의 최대 길이 : 18\n",
      "리뷰의 최소 길이 : 18\n",
      "리뷰의 평균 길이 : 18.000000\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 : %d' % max(len(l) for l in train_x))\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in train_x))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, train_x))/len(train_x)))\n",
    "      \n",
    "print('리뷰의 최대 길이 : %d' % max(len(l) for l in test_x))\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in test_x))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, test_x))/len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad58533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "(45654, 7)\n"
     ]
    }
   ],
   "source": [
    "# 종속변수 데이터 전처리\n",
    "train_y = np_utils.to_categorical(train[\"topic_idx\"]) # Y_train 에 원-핫 인코딩\n",
    "print(train_y)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eafa4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45654"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c6ea936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 334,    0, 2535, ...,    1,    1,    1],\n",
       "       [   0,    0,   18, ...,    1,    1,    1],\n",
       "       [  54,  697,  985, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [  23, 1880,  968, ...,    1,    1,    1],\n",
       "       [2989,    4,    0, ...,    1,    1,    1],\n",
       "       [ 968,    8,  244, ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff0593a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 설정\n",
    "vocab_size = 5003 # 제일 많이 사용하는 사이즈\n",
    "embedding_dim = 200  \n",
    "max_length = 18    # 위에서 그래프 확인 후 정함\n",
    "padding_type='post'\n",
    "#oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8348b559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 18, 200)           1000600   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 18, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 18, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,334,815\n",
      "Trainable params: 1,334,815\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 양방향 LSTM 레이어를 사용한 모델 (model3) 정의\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length =max_length),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "        tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "        Dense(7, activation='softmax')    # 결과값이 0~4 이므로 Dense(5)\n",
    "    ])\n",
    "    \n",
    "model.compile(loss= 'categorical_crossentropy', #여러개 정답 중 하나 맞추는 문제이므로 손실 함수는 categorical_crossentropy\n",
    "              optimizer= 'adam',\n",
    "              metrics = ['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14362d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e2d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 실행해보기\n",
    "# history = model.fit(train_x, train_y, epochs=10, batch_size=100, validation_split= 0.2) \n",
    "#   # 양방향 LSTM 레이어에서는 batch size 를 100으로 잡고 50회 학습 해보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de43ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 학습 결과 확인\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title('loss of Bidirectional LSTM (model3) ', fontsize= 15)\n",
    "# plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "# plt.plot(history.history['val_loss'],'r--', label='val_loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title('accuracy of Bidirectional LSTM (model3) ', fontsize= 15)\n",
    "# plt.plot(history.history['accuracy'], 'g-', label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'],'k--', label='val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d5a55d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45654, 45654)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train[\"topic_idx\"]\n",
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9efa92d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 16s 158ms/step - loss: 1.2046 - accuracy: 0.5346 - val_loss: 0.6425 - val_accuracy: 0.7873\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 9s 131ms/step - loss: 0.5211 - accuracy: 0.8286 - val_loss: 0.5559 - val_accuracy: 0.8138\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.4111 - accuracy: 0.8687 - val_loss: 0.5625 - val_accuracy: 0.8160\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 10s 133ms/step - loss: 0.3538 - accuracy: 0.8869 - val_loss: 0.5839 - val_accuracy: 0.8097\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 10s 133ms/step - loss: 0.3159 - accuracy: 0.8988 - val_loss: 0.6201 - val_accuracy: 0.8058\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.2805 - accuracy: 0.9115 - val_loss: 0.6290 - val_accuracy: 0.8102\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.2534 - accuracy: 0.9207 - val_loss: 0.6669 - val_accuracy: 0.8073\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.2192 - accuracy: 0.9325 - val_loss: 0.7295 - val_accuracy: 0.8030\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 10s 145ms/step - loss: 0.1999 - accuracy: 0.9383 - val_loss: 0.7291 - val_accuracy: 0.7997\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 10s 136ms/step - loss: 0.1815 - accuracy: 0.9445 - val_loss: 0.7680 - val_accuracy: 0.7972\n",
      "training model for CV #2\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 10s 136ms/step - loss: 0.3007 - accuracy: 0.9126 - val_loss: 0.2142 - val_accuracy: 0.9349\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.2243 - accuracy: 0.9367 - val_loss: 0.2517 - val_accuracy: 0.9170\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.1868 - accuracy: 0.9479 - val_loss: 0.2947 - val_accuracy: 0.9023\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.1675 - accuracy: 0.9523 - val_loss: 0.3429 - val_accuracy: 0.8903\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.1500 - accuracy: 0.9567 - val_loss: 0.4044 - val_accuracy: 0.8782\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.1367 - accuracy: 0.9603 - val_loss: 0.4493 - val_accuracy: 0.8721\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.1275 - accuracy: 0.9626 - val_loss: 0.4839 - val_accuracy: 0.8645\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.1103 - accuracy: 0.9680 - val_loss: 0.5504 - val_accuracy: 0.8577\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.0974 - accuracy: 0.9706 - val_loss: 0.5889 - val_accuracy: 0.8485\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.0943 - accuracy: 0.9721 - val_loss: 0.6087 - val_accuracy: 0.8488\n",
      "training model for CV #3\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.2162 - accuracy: 0.9382 - val_loss: 0.1180 - val_accuracy: 0.9642\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.1449 - accuracy: 0.9585 - val_loss: 0.1245 - val_accuracy: 0.9598\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 154ms/step - loss: 0.1089 - accuracy: 0.9697 - val_loss: 0.1443 - val_accuracy: 0.9505\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 10s 137ms/step - loss: 0.0948 - accuracy: 0.9729 - val_loss: 0.1827 - val_accuracy: 0.9421\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 10s 135ms/step - loss: 0.0842 - accuracy: 0.9770 - val_loss: 0.2289 - val_accuracy: 0.9270\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 10s 134ms/step - loss: 0.0805 - accuracy: 0.9769 - val_loss: 0.2638 - val_accuracy: 0.9199\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 10s 133ms/step - loss: 0.0769 - accuracy: 0.9773 - val_loss: 0.3008 - val_accuracy: 0.9106\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.0701 - accuracy: 0.9798 - val_loss: 0.3299 - val_accuracy: 0.9060\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 11s 154ms/step - loss: 0.0652 - accuracy: 0.9797 - val_loss: 0.3548 - val_accuracy: 0.9060\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 11s 152ms/step - loss: 0.0579 - accuracy: 0.9829 - val_loss: 0.4161 - val_accuracy: 0.8969\n",
      "training model for CV #4\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 10s 146ms/step - loss: 0.1479 - accuracy: 0.9554 - val_loss: 0.0740 - val_accuracy: 0.9774\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 155ms/step - loss: 0.0864 - accuracy: 0.9732 - val_loss: 0.0794 - val_accuracy: 0.9732\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.0591 - accuracy: 0.9828 - val_loss: 0.0898 - val_accuracy: 0.9699\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 10s 144ms/step - loss: 0.0506 - accuracy: 0.9849 - val_loss: 0.1273 - val_accuracy: 0.9574\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.0480 - accuracy: 0.9849 - val_loss: 0.1305 - val_accuracy: 0.9552\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 10s 143ms/step - loss: 0.0414 - accuracy: 0.9877 - val_loss: 0.1467 - val_accuracy: 0.9578\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 10s 140ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 0.1826 - val_accuracy: 0.9463\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 10s 139ms/step - loss: 0.0446 - accuracy: 0.9861 - val_loss: 0.2049 - val_accuracy: 0.9435\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 10s 144ms/step - loss: 0.0465 - accuracy: 0.9855 - val_loss: 0.2254 - val_accuracy: 0.9391\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 10s 138ms/step - loss: 0.0374 - accuracy: 0.9893 - val_loss: 0.2564 - val_accuracy: 0.9324\n",
      "training model for CV #5\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 10s 140ms/step - loss: 0.0977 - accuracy: 0.9690 - val_loss: 0.0578 - val_accuracy: 0.9824\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.0566 - accuracy: 0.9825 - val_loss: 0.0613 - val_accuracy: 0.9782\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 148ms/step - loss: 0.0391 - accuracy: 0.9887 - val_loss: 0.0790 - val_accuracy: 0.9740\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 146ms/step - loss: 0.0281 - accuracy: 0.9919 - val_loss: 0.0781 - val_accuracy: 0.9726\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.0253 - accuracy: 0.9927 - val_loss: 0.0885 - val_accuracy: 0.9715\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 11s 146ms/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.1103 - val_accuracy: 0.9663\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.0292 - accuracy: 0.9911 - val_loss: 0.1189 - val_accuracy: 0.9641\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 11s 148ms/step - loss: 0.0277 - accuracy: 0.9919 - val_loss: 0.1254 - val_accuracy: 0.9631\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.1621 - val_accuracy: 0.9516\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.1719 - val_accuracy: 0.9525\n"
     ]
    }
   ],
   "source": [
    "# # 계층 교차 검증\n",
    "# n_fold = 5  \n",
    "# seed = 42\n",
    "\n",
    "# cv = StratifiedKFold(n_splits = n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "# for i, (i_trn, i_val) in enumerate(cv.split(train_x, train[\"topic_idx\"]), 1):\n",
    "#     print(f'training model for CV #{i}')\n",
    "\n",
    "#     model.fit(train_x[i_trn], \n",
    "#             to_categorical(train[\"topic_idx\"][i_trn]),\n",
    "#             validation_data=(train_x[i_val], to_categorical(train[\"topic_idx\"][i_val])),\n",
    "#             epochs=10,\n",
    "#             batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b5797ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 16s 163ms/step - loss: 1.2244 - accuracy: 0.5197 - val_loss: 0.6411 - val_accuracy: 0.7820\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 10s 144ms/step - loss: 0.5107 - accuracy: 0.8330 - val_loss: 0.5505 - val_accuracy: 0.8161\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 10s 144ms/step - loss: 0.3915 - accuracy: 0.8748 - val_loss: 0.5647 - val_accuracy: 0.8206\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 10s 145ms/step - loss: 0.3413 - accuracy: 0.8889 - val_loss: 0.5892 - val_accuracy: 0.8131\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.2979 - accuracy: 0.9036 - val_loss: 0.6150 - val_accuracy: 0.8092\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n",
      "training model for CV #2\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 11s 148ms/step - loss: 0.4340 - accuracy: 0.8601 - val_loss: 0.4307 - val_accuracy: 0.8592\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.3503 - accuracy: 0.8874 - val_loss: 0.4740 - val_accuracy: 0.8405\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 147ms/step - loss: 0.3087 - accuracy: 0.8995 - val_loss: 0.5087 - val_accuracy: 0.8295\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 153ms/step - loss: 0.2708 - accuracy: 0.9118 - val_loss: 0.5544 - val_accuracy: 0.8228\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n",
      "training model for CV #3\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.3827 - accuracy: 0.8758 - val_loss: 0.3674 - val_accuracy: 0.8748\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.3160 - accuracy: 0.8981 - val_loss: 0.4172 - val_accuracy: 0.8576\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.2835 - accuracy: 0.9079 - val_loss: 0.4835 - val_accuracy: 0.8359\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 149ms/step - loss: 0.2546 - accuracy: 0.9184 - val_loss: 0.5183 - val_accuracy: 0.8341\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n",
      "training model for CV #4\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 11s 160ms/step - loss: 0.3433 - accuracy: 0.8863 - val_loss: 0.3250 - val_accuracy: 0.8912\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 158ms/step - loss: 0.2938 - accuracy: 0.9050 - val_loss: 0.3842 - val_accuracy: 0.8691\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 11s 159ms/step - loss: 0.2616 - accuracy: 0.9152 - val_loss: 0.4290 - val_accuracy: 0.8571\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 11s 158ms/step - loss: 0.2389 - accuracy: 0.9228 - val_loss: 0.4805 - val_accuracy: 0.8462\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n",
      "training model for CV #5\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 11s 153ms/step - loss: 0.3149 - accuracy: 0.8967 - val_loss: 0.2888 - val_accuracy: 0.9036\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 11s 155ms/step - loss: 0.2748 - accuracy: 0.9092 - val_loss: 0.3485 - val_accuracy: 0.8760\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 12s 170ms/step - loss: 0.2410 - accuracy: 0.9238 - val_loss: 0.4230 - val_accuracy: 0.8619\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 13s 179ms/step - loss: 0.2247 - accuracy: 0.9280 - val_loss: 0.4607 - val_accuracy: 0.8474\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 계층 교차 검증\n",
    "n_fold = 5  \n",
    "seed = 42\n",
    "\n",
    "cv = StratifiedKFold(n_splits = n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "# 테스트데이터의 예측값 담을 곳 생성\n",
    "test_y = np.zeros((test_x.shape[0], 7))\n",
    "\n",
    "# 조기 종료 옵션 추가\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(train_x, train_y), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "\n",
    "    model.fit(train_x[i_trn], \n",
    "            to_categorical(train_y[i_trn]),\n",
    "            validation_data=(train_x[i_val], to_categorical(train_y[i_val])),\n",
    "            epochs=10,\n",
    "            batch_size=512,\n",
    "            callbacks=[es])     # 조기 종료 옵션\n",
    "                      \n",
    "    test_y += model.predict(test_x) / n_fold    # 나온 예측값들을 교차 검증 횟수로 나눈다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38cf6791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08602009, 0.0717154 , 0.45507176, ..., 0.04346492, 0.01626402,\n",
       "        0.0102361 ],\n",
       "       [0.00300351, 0.0018623 , 0.02537468, ..., 0.00472795, 0.00252248,\n",
       "        0.00157376],\n",
       "       [0.05173881, 0.03597972, 0.7157121 , ..., 0.01904189, 0.00242945,\n",
       "        0.16792241],\n",
       "       ...,\n",
       "       [0.00204276, 0.00104967, 0.04095534, ..., 0.02297982, 0.00180785,\n",
       "        0.00299678],\n",
       "       [0.31421448, 0.04062183, 0.55364918, ..., 0.03585994, 0.00165979,\n",
       "        0.0270768 ],\n",
       "       [0.0069064 , 0.01110192, 0.29724516, ..., 0.00847807, 0.00150495,\n",
       "        0.67217722]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6298a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = []\n",
    "for i in range(len(test_y)):\n",
    "    topic.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea12dfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>54780</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>54781</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>54782</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>54783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9130</th>\n",
       "      <td>54784</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  topic_idx\n",
       "0     45654          2\n",
       "1     45655          3\n",
       "2     45656          2\n",
       "3     45657          2\n",
       "4     45658          3\n",
       "...     ...        ...\n",
       "9126  54780          3\n",
       "9127  54781          6\n",
       "9128  54782          3\n",
       "9129  54783          2\n",
       "9130  54784          6\n",
       "\n",
       "[9131 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['topic_idx'] = topic\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a84aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('results/kfold-LSTM-2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "396f8f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08602009, 0.0717154 , 0.45507176, ..., 0.04346492, 0.01626402,\n",
       "        0.0102361 ],\n",
       "       [0.00300351, 0.0018623 , 0.02537468, ..., 0.00472795, 0.00252248,\n",
       "        0.00157376],\n",
       "       [0.05173881, 0.03597972, 0.7157121 , ..., 0.01904189, 0.00242945,\n",
       "        0.16792241],\n",
       "       ...,\n",
       "       [0.00204276, 0.00104967, 0.04095534, ..., 0.02297982, 0.00180785,\n",
       "        0.00299678],\n",
       "       [0.31421448, 0.04062183, 0.55364918, ..., 0.03585994, 0.00165979,\n",
       "        0.0270768 ],\n",
       "       [0.0069064 , 0.01110192, 0.29724516, ..., 0.00847807, 0.00150495,\n",
       "        0.67217722]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1de0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_df = pd.DataFrame(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3fb0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_df.to_csv('ensemble/kfold-lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488657a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_y_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff951b7b78fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_y_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_y_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f530049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
