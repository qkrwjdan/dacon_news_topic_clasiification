{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4b69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForMaskedLM, AutoModelForPreTraining, TrainingArguments, Trainer\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184c5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 17):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "seed_everything(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3445cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"klue/bert-base\"\n",
    "batch_size = 8\n",
    "MODEL_P = \"models/klue-bert-base-mlm.pth\"\n",
    "RANDOM_SEED = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c655743",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242445de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/train_data.csv\",index_col=False)\n",
    "dataset_augmented = pd.read_csv(\"data/train_data_m2m_translation.csv\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7d4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_augmented[\"topic_idx\"] = dataset[\"topic_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "356a12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = train_test_split(dataset,test_size = 0.1,random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ecd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_augmented_title = dataset_augmented[\"title\"][dataset_train[\"index\"]]\n",
    "train_dataset_augmented_topic_idx = dataset_augmented[\"topic_idx\"][dataset_train[\"index\"]]\n",
    "train_dataset_augmented = pd.DataFrame({'title' : train_dataset_augmented_title.tolist(), \"topic_idx\" : train_dataset_augmented_topic_idx.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc44e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.concat([dataset_train,train_dataset_augmented])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8711444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36615</th>\n",
       "      <td>36615.0</td>\n",
       "      <td>이란 외무 트럼프 볼턴에 들볶여 알렉산더도 못한 일 하려해</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16758</th>\n",
       "      <td>16758.0</td>\n",
       "      <td>영상 한국 부도위험지표 12년 만에 최저…북미회담 덕분</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30712</th>\n",
       "      <td>30712.0</td>\n",
       "      <td>도이치모터스 도이치파이낸셜 주식 160억원에 추가취득</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1407.0</td>\n",
       "      <td>서울 출신 학자의 외침 사대문 안만 서울이 아니다</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36067</th>\n",
       "      <td>36067.0</td>\n",
       "      <td>내일날씨 전국 대체로 맑고 더워…낮 최고 22∼31도</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                             title  topic_idx\n",
       "36615  36615.0  이란 외무 트럼프 볼턴에 들볶여 알렉산더도 못한 일 하려해          4\n",
       "16758  16758.0    영상 한국 부도위험지표 12년 만에 최저…북미회담 덕분          6\n",
       "30712  30712.0     도이치모터스 도이치파이낸셜 주식 160억원에 추가취득          1\n",
       "1407    1407.0       서울 출신 학자의 외침 사대문 안만 서울이 아니다          3\n",
       "36067  36067.0     내일날씨 전국 대체로 맑고 더워…낮 최고 22∼31도          3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efcb5102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "source": [
    "topic_token_dict = {0:4038,1:3674,2:3647,3:3697,4:3665,5:4559,6:3713}\n",
    "token_topic_dict = {4038 : 0, 3674 : 1, 3647 : 2, 3697 : 3, 3665 : 4, 4559 : 5, 3713 : 6}\n",
    "topic_dict = {0: \"과학\", 1:\"경제\", 2:\"사회\", 3:\"문화\", 4:\"세계\", 5:\"스포츠\", 6 : \"정치\"}\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for title in dataset_train[\"title\"]:\n",
    "    sentence = title + \".[SEP] 이 문장은 [MASK]\"\n",
    "    tmp.append(sentence)\n",
    "dataset_train[\"title\"] = tmp\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for title in dataset_val[\"title\"]:\n",
    "    sentence = title + \".[SEP] 이 문장은 [MASK]\"\n",
    "    tmp.append(sentence)\n",
    "dataset_val[\"title\"] = tmp\n",
    "    \n",
    "tmp = []\n",
    "for title in test[\"title\"]:\n",
    "    sentence = title + \".[SEP] 이 문장은 [MASK]\"\n",
    "    tmp.append(sentence)\n",
    "\n",
    "test[\"title\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12d0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36615</th>\n",
       "      <td>36615.0</td>\n",
       "      <td>이란 외무 트럼프 볼턴에 들볶여 알렉산더도 못한 일 하려해.[SEP] 이 문장은 [...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16758</th>\n",
       "      <td>16758.0</td>\n",
       "      <td>영상 한국 부도위험지표 12년 만에 최저…북미회담 덕분.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30712</th>\n",
       "      <td>30712.0</td>\n",
       "      <td>도이치모터스 도이치파이낸셜 주식 160억원에 추가취득.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1407.0</td>\n",
       "      <td>서울 출신 학자의 외침 사대문 안만 서울이 아니다.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36067</th>\n",
       "      <td>36067.0</td>\n",
       "      <td>내일날씨 전국 대체로 맑고 더워…낮 최고 22∼31도.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                              title  topic_idx\n",
       "36615  36615.0  이란 외무 트럼프 볼턴에 들볶여 알렉산더도 못한 일 하려해.[SEP] 이 문장은 [...          4\n",
       "16758  16758.0  영상 한국 부도위험지표 12년 만에 최저…북미회담 덕분.[SEP] 이 문장은 [MASK]          6\n",
       "30712  30712.0   도이치모터스 도이치파이낸셜 주식 160억원에 추가취득.[SEP] 이 문장은 [MASK]          1\n",
       "1407    1407.0     서울 출신 학자의 외침 사대문 안만 서울이 아니다.[SEP] 이 문장은 [MASK]          3\n",
       "36067  36067.0   내일날씨 전국 대체로 맑고 더워…낮 최고 22∼31도.[SEP] 이 문장은 [MASK]          3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f34c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(dataset,sent_key,label_key,tokenizer):\n",
    "    if label_key is None :\n",
    "        labels = [np.int64(0) for i in dataset[sent_key]]\n",
    "    else :\n",
    "        labels = [np.int64(i) for i in dataset[label_key]]\n",
    "    \n",
    "    sentences = tokenizer(dataset[sent_key].tolist(),truncation=True,padding=True)\n",
    "\n",
    "    input_ids = sentences.input_ids\n",
    "    token_type_ids = sentences.token_type_ids\n",
    "    attention_mask = sentences.attention_mask\n",
    "    masked_token_idx = []\n",
    "    \n",
    "    for input_id in input_ids:\n",
    "        masked_token_idx.append(input_id.index(4))\n",
    "    \n",
    "    return list([input_ids, token_type_ids, attention_mask, labels, masked_token_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11b7a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = bert_tokenize(dataset_train,\"title\",\"topic_idx\",tokenizer)\n",
    "validation_inputs = bert_tokenize(dataset_val,\"title\",\"topic_idx\",tokenizer)\n",
    "test_inputs = bert_tokenize(test,\"title\",None,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3114301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 35, 93)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_inputs[0][1]), len(validation_inputs[0][1]), len(train_inputs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440adfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs[0][1], train_inputs[1][1], train_inputs[2][1], train_inputs[3][1], train_inputs[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d836237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_inputs)):\n",
    "    train_inputs[i] = torch.tensor(train_inputs[i])\n",
    "    \n",
    "for i in range(len(validation_inputs)):\n",
    "    validation_inputs[i] = torch.tensor(validation_inputs[i])\n",
    "    \n",
    "for i in range(len(test_inputs)):\n",
    "    test_inputs[i] = torch.tensor(test_inputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad92295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(*train_inputs)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(*validation_inputs)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(*test_inputs)\n",
    "test_dataloader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06929504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForPreTraining.from_pretrained(model_checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1106f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 1\n",
    "\n",
    "num_warmup_steps = 0\n",
    "\n",
    "warmup_ratio = 0.1\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "warmup_step = int(num_training_steps * warmup_ratio)\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "689f229b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_20172/1262026550.py:9: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1,epochs+1,desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30fa383a7d244f8ac9211a6e8974d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_20172/1262026550.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0ddd544bd24e3b8676d2935133cd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tBatch : 0 Average Training loss : 0.010925743728876114\n",
      "\n",
      "\tBatch : 100 Average Training loss : 0.032129501970136046\n",
      "\n",
      "\tBatch : 200 Average Training loss : 0.03195290811558246\n",
      "\n",
      "\tBatch : 300 Average Training loss : 0.032417204623277\n",
      "\n",
      "\tBatch : 400 Average Training loss : 0.033534044069268555\n",
      "\n",
      "\tBatch : 500 Average Training loss : 0.03368914400642169\n",
      "\n",
      "\tBatch : 600 Average Training loss : 0.03387283127912249\n",
      "\n",
      "\tBatch : 700 Average Training loss : 0.03481746071300618\n",
      "\n",
      "\tBatch : 800 Average Training loss : 0.03412003802843281\n",
      "\n",
      "\tBatch : 900 Average Training loss : 0.03383348607732359\n",
      "\n",
      "\tBatch : 1000 Average Training loss : 0.03444243954336174\n",
      "\n",
      "\tBatch : 1100 Average Training loss : 0.03436295940588773\n",
      "\n",
      "\tBatch : 1200 Average Training loss : 0.034590639710288\n",
      "\n",
      "\tBatch : 1300 Average Training loss : 0.03406860911706064\n",
      "\n",
      "\tBatch : 1400 Average Training loss : 0.033646114022956086\n",
      "\n",
      "\tBatch : 1500 Average Training loss : 0.03424390977108746\n",
      "\n",
      "\tBatch : 1600 Average Training loss : 0.034230135370708833\n",
      "\n",
      "\tBatch : 1700 Average Training loss : 0.03416038152479334\n",
      "\n",
      "\tBatch : 1800 Average Training loss : 0.03402990055755588\n",
      "\n",
      "\tBatch : 1900 Average Training loss : 0.0338088394943281\n",
      "\n",
      "\tBatch : 2000 Average Training loss : 0.03398411331095479\n",
      "\n",
      "\tBatch : 2100 Average Training loss : 0.03402467979755758\n",
      "\n",
      "\tBatch : 2200 Average Training loss : 0.03405764646640651\n",
      "\n",
      "\tBatch : 2300 Average Training loss : 0.034138094237592134\n",
      "\n",
      "\tBatch : 2400 Average Training loss : 0.03417974307520952\n",
      "\n",
      "\tBatch : 2500 Average Training loss : 0.034293258195975215\n",
      "\n",
      "\tBatch : 2600 Average Training loss : 0.034284366173959545\n",
      "\n",
      "\tBatch : 2700 Average Training loss : 0.03448526948501501\n",
      "\n",
      "\tBatch : 2800 Average Training loss : 0.034498193264932975\n",
      "\n",
      "\tBatch : 2900 Average Training loss : 0.034529569599200415\n",
      "\n",
      "\tBatch : 3000 Average Training loss : 0.03435059809517366\n",
      "\n",
      "\tBatch : 3100 Average Training loss : 0.0344239713125143\n",
      "\n",
      "\tBatch : 3200 Average Training loss : 0.03482445879950642\n",
      "\n",
      "\tBatch : 3300 Average Training loss : 0.034892961843335\n",
      "\n",
      "\tBatch : 3400 Average Training loss : 0.035009910003365584\n",
      "\n",
      "\tBatch : 3500 Average Training loss : 0.034867493548698794\n",
      "\n",
      "\tBatch : 3600 Average Training loss : 0.03488114082212772\n",
      "\n",
      "\tBatch : 3700 Average Training loss : 0.03476049669761034\n",
      "\n",
      "\tBatch : 3800 Average Training loss : 0.03480416686941321\n",
      "\n",
      "\tBatch : 3900 Average Training loss : 0.034997104861320884\n",
      "\n",
      "\tBatch : 4000 Average Training loss : 0.0349921820859332\n",
      "\n",
      "\tBatch : 4100 Average Training loss : 0.03508375741967304\n",
      "\n",
      "\tBatch : 4200 Average Training loss : 0.0352984873232767\n",
      "\n",
      "\tBatch : 4300 Average Training loss : 0.03545585710981728\n",
      "\n",
      "\tBatch : 4400 Average Training loss : 0.035524906333841186\n",
      "\n",
      "\tBatch : 4500 Average Training loss : 0.03542844869190769\n",
      "\n",
      "\tBatch : 4600 Average Training loss : 0.03554401370718668\n",
      "\n",
      "\tBatch : 4700 Average Training loss : 0.03572470857248588\n",
      "\n",
      "\tBatch : 4800 Average Training loss : 0.03603016759920247\n",
      "\n",
      "\tBatch : 4900 Average Training loss : 0.03617018419480336\n",
      "\n",
      "\tBatch : 5000 Average Training loss : 0.03627669595682933\n",
      "\n",
      "\tBatch : 5100 Average Training loss : 0.03621472238343534\n",
      "\n",
      "\tBatch : 5200 Average Training loss : 0.03628507018178312\n",
      "\n",
      "\tBatch : 5300 Average Training loss : 0.03630803127928859\n",
      "\n",
      "\tBatch : 5400 Average Training loss : 0.036383189443141006\n",
      "\n",
      "\tBatch : 5500 Average Training loss : 0.03662327183464704\n",
      "\n",
      "\tBatch : 5600 Average Training loss : 0.03675860417191051\n",
      "\n",
      "\tBatch : 5700 Average Training loss : 0.036918847206634464\n",
      "\n",
      "\tBatch : 5800 Average Training loss : 0.03688446552138436\n",
      "\n",
      "\tBatch : 5900 Average Training loss : 0.036808756053659374\n",
      "\n",
      "\tBatch : 6000 Average Training loss : 0.03684626802454983\n",
      "\n",
      "\tBatch : 6100 Average Training loss : 0.03702373878819423\n",
      "\n",
      "\tBatch : 6200 Average Training loss : 0.03707449461636483\n",
      "\n",
      "\tBatch : 6300 Average Training loss : 0.037121865501071785\n",
      "\n",
      "\tBatch : 6400 Average Training loss : 0.0373327954168101\n",
      "\n",
      "\tBatch : 6500 Average Training loss : 0.03740000273403633\n",
      "\n",
      "\tBatch : 6600 Average Training loss : 0.03752554564556814\n",
      "\n",
      "\tBatch : 6700 Average Training loss : 0.03755197511765353\n",
      "\n",
      "\tBatch : 6800 Average Training loss : 0.03767771631617912\n",
      "\n",
      "\tBatch : 6900 Average Training loss : 0.03770367427621452\n",
      "\n",
      "\tBatch : 7000 Average Training loss : 0.037856361114430935\n",
      "\n",
      "\tBatch : 7100 Average Training loss : 0.037862913049174524\n",
      "\n",
      "\tBatch : 7200 Average Training loss : 0.03802348876624953\n",
      "\n",
      "\tBatch : 7300 Average Training loss : 0.0380939431345033\n",
      "\n",
      "\tBatch : 7400 Average Training loss : 0.03824495914715969\n",
      "\n",
      "\tBatch : 7500 Average Training loss : 0.038380740111710665\n",
      "\n",
      "\tBatch : 7600 Average Training loss : 0.038487648572171594\n",
      "\n",
      "\tBatch : 7700 Average Training loss : 0.0385275251827933\n",
      "\n",
      "\tBatch : 7800 Average Training loss : 0.038615887314178456\n",
      "\n",
      "\tBatch : 7900 Average Training loss : 0.03871514104407288\n",
      "\n",
      "\tBatch : 8000 Average Training loss : 0.03884020431109239\n",
      "\n",
      "\tBatch : 8100 Average Training loss : 0.038907457319018686\n",
      "\n",
      "\tBatch : 8200 Average Training loss : 0.039061271459868215\n",
      "\n",
      "\tBatch : 8300 Average Training loss : 0.039057827128050324\n",
      "\n",
      "\tBatch : 8400 Average Training loss : 0.039170810122067516\n",
      "\n",
      "\tBatch : 8500 Average Training loss : 0.03924977655129974\n",
      "\n",
      "\tBatch : 8600 Average Training loss : 0.039354232228389185\n",
      "\n",
      "\tBatch : 8700 Average Training loss : 0.03950139292258208\n",
      "\n",
      "\tBatch : 8800 Average Training loss : 0.03951865676622565\n",
      "\n",
      "\tBatch : 8900 Average Training loss : 0.03956654174424086\n",
      "\n",
      "\tBatch : 9000 Average Training loss : 0.0396873453535472\n",
      "\n",
      "\tBatch : 9100 Average Training loss : 0.0398206896018435\n",
      "\n",
      "\tBatch : 9200 Average Training loss : 0.03994103388964461\n",
      "\n",
      "\tBatch : 9300 Average Training loss : 0.04005209397845258\n",
      "\n",
      "\tBatch : 9400 Average Training loss : 0.04013976856915887\n",
      "\n",
      "\tBatch : 9500 Average Training loss : 0.04018854960406075\n",
      "\n",
      "\tBatch : 9600 Average Training loss : 0.04029966766165419\n",
      "\n",
      "\tBatch : 9700 Average Training loss : 0.04037538899190387\n",
      "\n",
      "\tBatch : 9800 Average Training loss : 0.0404457757814477\n",
      "\n",
      "\tBatch : 9900 Average Training loss : 0.04064790619139355\n",
      "\n",
      "\tBatch : 10000 Average Training loss : 0.04074447272678796\n",
      "\n",
      "\tBatch : 10100 Average Training loss : 0.04087964134893537\n",
      "\n",
      "\tBatch : 10200 Average Training loss : 0.040998410900825066\n",
      "\n",
      "\tAverage Training loss: 0.32839422753220426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_20172/1262026550.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(validation_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0b10ab243f4510b3819d82916ec74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tAccuracy : 0.8817345597897503\n",
      "\n",
      "\tAverage validation loss: 6.471789539336864\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "    batch_loss = 0\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx = batch\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        # calculate loss\n",
    "        logits_cls, logits_lm = outputs[1], outputs[0]\n",
    "        \n",
    "        # topic -> token \n",
    "        # Ex.6 -> 3713\n",
    "        mask_label = [topic_token_dict[lb] for lb in b_labels.to('cpu').numpy() ]\n",
    "        \n",
    "#         label_lms = []\n",
    "        \n",
    "        labels_lms = []\n",
    "        SEQUENCE_LENGTH = 93\n",
    "        \n",
    "        # label 만들기\n",
    "        for idx, label in zip(b_masked_token_idx.to('cpu').numpy(),mask_label):\n",
    "            labels_lm = np.full(SEQUENCE_LENGTH, dtype=np.int, fill_value=-1)\n",
    "            labels_lm[idx] = label\n",
    "            labels_lms.append(labels_lm)\n",
    "        label_lms_pt = torch.tensor(labels_lms,dtype=torch.int64).to(device)\n",
    "        \n",
    "        # lm loss 계산\n",
    "        loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), label_lms_pt.view(-1))\n",
    "        \n",
    "        # cls loss 계산\n",
    "        labels_cls = [1 for _ in range(len(b_input_ids))]\n",
    "        labels_cls = torch.tensor(labels_cls).to(device)\n",
    "        loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "        \n",
    "        loss = loss_cls + loss_lm\n",
    "        loss.backward()        \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('\\n\\tBatch : {} Average Training loss : {}'.format(step, batch_loss / (batch_size * step+1) ))\n",
    "        \n",
    "    avg_train_loss = batch_loss / len(train_dataloader)\n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "    \n",
    "    # eval\n",
    "    model.eval()\n",
    "    \n",
    "    predict_li = []\n",
    "    label_li = []\n",
    "    \n",
    "    for step, batch in enumerate(tqdm_notebook(validation_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx = batch\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        logits_cls, logits_lm = outputs[1], outputs[0]\n",
    "        logits_lm_np = logits_lm.to('cpu').detach().numpy()\n",
    "        pred = np.argmax(logits_lm_np,axis=2)\n",
    "\n",
    "        masked_token_idx_np = b_masked_token_idx.to('cpu').numpy()\n",
    "        \n",
    "        labels_np = b_labels.to('cpu').numpy()\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "            l = token_topic_dict[pred[i][masked_token_idx_np[i]]]\n",
    "            predict_li.append(l)\n",
    "\n",
    "        for l in labels_np:\n",
    "            label_li.append(l)\n",
    "\n",
    "        mask_label = [ topic_token_dict[lb] for lb in labels_np ]\n",
    "        \n",
    "        labels_lms = []\n",
    "        SEQUENCE_LENGTH = 35\n",
    "\n",
    "        for idx, label in zip(masked_token_idx_np,mask_label):\n",
    "            \n",
    "            labels_lm = np.full(SEQUENCE_LENGTH, dtype=np.int, fill_value=-1)\n",
    "            labels_lm[idx] = label\n",
    "            \n",
    "            labels_lms.append(labels_lm)\n",
    "\n",
    "        labels_lms_pt = torch.tensor(labels_lms,dtype=torch.int64).to(device)\n",
    "        loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), labels_lms_pt.view(-1))\n",
    "\n",
    "        labels_cls = [1 for _ in range(len(b_input_ids))]\n",
    "        labels_cls = torch.tensor(labels_cls).to(device)\n",
    "        loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "\n",
    "        loss = loss_cls + loss_lm\n",
    "        \n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    print(\"\\n\\tAccuracy : {}\".format(accuracy_score(predict_li,label_li)))\n",
    "    avg_validation_loss = batch_loss / len(validation_dataloader)\n",
    "    print(F'\\n\\tAverage validation loss: {avg_validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc96917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18f0431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_20172/1987576123.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b4d20043a44f5a90e4a0d1c74695a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_li = []\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx = batch\n",
    "    \n",
    "    outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits_cls, logits_lm = outputs[1], outputs[0]\n",
    "    \n",
    "    logits_lm = logits_lm.to('cpu').detach().numpy()\n",
    "    out = np.argmax(logits_lm,axis=2)\n",
    "    \n",
    "    masked_token_idx_np = b_masked_token_idx.to('cpu').numpy()\n",
    "    \n",
    "#     print(out)\n",
    "    \n",
    "    for i in range(len(out)):\n",
    "        l = token_topic_dict[out[i][masked_token_idx_np[i]]]\n",
    "        predict_li.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe1adc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9131"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1746abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('data/sample_submission.csv')\n",
    "# submission['topic_idx'] = predict_li\n",
    "# submission.to_csv(\"results/klue-bert-mlm-classification-1epoch-augmented-0805.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba144f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
