{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36ce525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, Conv1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from konlpy.tag import Mecab  \n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65008420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from KoBERT.kobert.utils import get_tokenizer\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f3dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data.csv\",encoding=\"utf-8\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30e3f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5003\n",
    "embedding_dim = 200  \n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a50f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_DATA = len(train)\n",
    "NUM_TEST_DATA = len(test)\n",
    "NUM_CLASSES = 7\n",
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4037e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lstm(train_df, test_df, vocab_size):\n",
    "    tokenizer = Mecab()\n",
    "    train_df[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in train_df[\"title\"]]\n",
    "    test_df[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in test_df[\"title\"]]\n",
    "    \n",
    "    vocab = FreqDist(np.hstack(train_df[\"tokenized\"]))\n",
    "    print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "    \n",
    "    vocab = vocab.most_common(vocab_size)\n",
    "    \n",
    "    word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "    word_to_index['pad'] = 1\n",
    "    word_to_index['unk'] = 0\n",
    "    \n",
    "    train_x = []\n",
    "    test_x = []\n",
    "\n",
    "    for line in train[\"tokenized\"]:\n",
    "        temp = []\n",
    "        for w in line:\n",
    "            try:\n",
    "                temp.append(word_to_index[w])\n",
    "            except KeyError:\n",
    "                temp.append(word_to_index['unk'])\n",
    "\n",
    "        train_x.append(temp)\n",
    "\n",
    "    for line in test[\"tokenized\"]:\n",
    "        temp = []\n",
    "        for w in line:\n",
    "            try:\n",
    "                temp.append(word_to_index[w])\n",
    "            except KeyError:\n",
    "                temp.append(word_to_index['unk'])\n",
    "\n",
    "        test_x.append(temp)\n",
    "        \n",
    "    max_len = max(len(l) for l in train_x)\n",
    "    max_len = max(len(l) for l in test_x)\n",
    "    print(max_len)\n",
    "    \n",
    "    for line in train_x:\n",
    "        if len(line) < max_len:\n",
    "            line += [word_to_index['pad']] * (max_len - len(line))\n",
    "\n",
    "    for line in test_x:\n",
    "        if len(line) < max_len:\n",
    "            line += [word_to_index['pad']] * (max_len - len(line))\n",
    "            \n",
    "#     train_y = np_utils.to_categorical(train[\"topic_idx\"])\n",
    "    train_y = train[\"topic_idx\"]\n",
    "    \n",
    "    train_x = np.array(train_x)\n",
    "    test_x = np.array(test_x)\n",
    "    \n",
    "    return train_x, train_y, test_x, max_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5deb03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_feature(train_df, test_df,rnd=1):\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'lstm_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_CLASSES)),np.zeros((NUM_TEST_DATA,NUM_CLASSES))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_CLASSES)),np.zeros((NUM_TEST_DATA,NUM_CLASSES))\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential([Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "                Dense(NUM_CLASSES, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7974f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 23s 133ms/step - loss: 1.0125 - accuracy: 0.6267 - val_loss: 0.6111 - val_accuracy: 0.8090\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61113, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 18s 126ms/step - loss: 0.4397 - accuracy: 0.8585 - val_loss: 0.5940 - val_accuracy: 0.8121\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61113 to 0.59395, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 136ms/step - loss: 0.3183 - accuracy: 0.9019 - val_loss: 0.5990 - val_accuracy: 0.8179\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59395\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 20s 136ms/step - loss: 0.2527 - accuracy: 0.9235 - val_loss: 0.6775 - val_accuracy: 0.8129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.59395\n",
      "------------------\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 140ms/step - loss: 0.9869 - accuracy: 0.6420 - val_loss: 0.5568 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55682, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 20s 139ms/step - loss: 0.4118 - accuracy: 0.8678 - val_loss: 0.5292 - val_accuracy: 0.8335\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55682 to 0.52919, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 132ms/step - loss: 0.3047 - accuracy: 0.9050 - val_loss: 0.5713 - val_accuracy: 0.8196\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52919\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 19s 132ms/step - loss: 0.2510 - accuracy: 0.9235 - val_loss: 0.6630 - val_accuracy: 0.8056\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52919\n",
      "------------------\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 141ms/step - loss: 1.0377 - accuracy: 0.5983 - val_loss: 0.6084 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60837, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 19s 133ms/step - loss: 0.4334 - accuracy: 0.8587 - val_loss: 0.5537 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60837 to 0.55365, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 134ms/step - loss: 0.2978 - accuracy: 0.9053 - val_loss: 0.6340 - val_accuracy: 0.8205\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55365\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 20s 137ms/step - loss: 0.2422 - accuracy: 0.9230 - val_loss: 0.7453 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55365\n",
      "------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 140ms/step - loss: 0.9987 - accuracy: 0.6309 - val_loss: 0.5177 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51769, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 21s 144ms/step - loss: 0.3923 - accuracy: 0.8735 - val_loss: 0.5182 - val_accuracy: 0.8354\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.51769\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 135ms/step - loss: 0.2904 - accuracy: 0.9093 - val_loss: 0.6456 - val_accuracy: 0.8090\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51769\n",
      "------------------\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 24s 142ms/step - loss: 1.0716 - accuracy: 0.5907 - val_loss: 0.6582 - val_accuracy: 0.7831\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65824, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 19s 133ms/step - loss: 0.4742 - accuracy: 0.8457 - val_loss: 0.5598 - val_accuracy: 0.8238\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65824 to 0.55976, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 132ms/step - loss: 0.3257 - accuracy: 0.8989 - val_loss: 0.6137 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55976\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 19s 134ms/step - loss: 0.2645 - accuracy: 0.9196 - val_loss: 0.6564 - val_accuracy: 0.8080\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55976\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "lstm_train1, lstm_test1, lstm_train2, lstm_test2 = get_lstm_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0af9a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnn_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'dnn_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential([Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                Dense(128,activation=\"relu\"),\n",
    "                Dense(128,activation=\"relu\"),\n",
    "                Dropout(0.2),\n",
    "                Dense(7, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a81289d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_LABELS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-de8dc9e3cf49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdnn_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dnn_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-40b59b0174c9>\u001b[0m in \u001b[0;36mget_dnn_feature\u001b[0;34m(train_df, test_df, rnd)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dnn_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TEST_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbest_val_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TEST_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mFEAT_CNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_LABELS' is not defined"
     ]
    }
   ],
   "source": [
    "dnn_train1, dnn_test1, dnn_train2, dnn_test2 = get_dnn_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'dnn_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len))\n",
    "        model.add(Conv1D(32,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ed4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train1, cnn_test1, cnn_train2, cnn_test2 = get_cnn_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1de40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_key, label_key, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i]) for i in dataset[sent_key]]\n",
    "        \n",
    "        if not label_key == None:\n",
    "            self.mode = \"train\"\n",
    "        else:\n",
    "            self.mode = \"test\"\n",
    "            \n",
    "        if self.mode == \"train\":\n",
    "            self.labels = [np.int32(i) for i in dataset[label_key]]\n",
    "        else:\n",
    "            self.labels = [np.int32(0) for i in dataset[sent_key]]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.mode == \"train\":\n",
    "            return (self.sentences[i] + (self.labels[i], ))\n",
    "        else:\n",
    "            return self.sentences[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=NUM_CLASSES,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "    \n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bad3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kobert_features(rnd=1):\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    MODEL_P = 'kobert-model.pth'\n",
    "    NUM_CLASSES = 7\n",
    "    NUM_LABELS = 7\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    max_len = 40\n",
    "    batch_size = 16\n",
    "    warmup_ratio = 0.1\n",
    "    num_epochs = 1\n",
    "    max_grad_norm = 1\n",
    "    log_interval = 200\n",
    "    learning_rate =  5e-5\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "    min_val_loss = np.Inf\n",
    "    n_epochs_stop = 2\n",
    "    \n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    dataset = pd.read_csv(\"data/train_data.csv\",index_col=False)\n",
    "    test = pd.read_csv(\"data/test_data.csv\",index_col=False)\n",
    "    dataset_train, dataset_val = train_test_split(dataset,test_size = 0.2,random_state = RANDOM_SEED)\n",
    "    \n",
    "    bertmodel, vocab = get_pytorch_kobert_model()    \n",
    "    tokenizer = get_tokenizer()\n",
    "    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "    \n",
    "    data_total = BERTDataset(dataset_train, \"title\", \"topic_idx\", tok, max_len, True, False)\n",
    "    data_train = BERTDataset(dataset_train, \"title\", \"topic_idx\", tok, max_len, True, False)\n",
    "    data_val = BERTDataset(dataset_val, \"title\", \"topic_idx\", tok, max_len, True, False)\n",
    "    data_test = BERTDataset(test, \"title\", None, tok, max_len, True, False)\n",
    "    \n",
    "    total_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "    train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "    val_dataloader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, num_workers=5)\n",
    "    test_dataloader = torch.utils.data.DataLoader(data_val, batch_size=batch_size,num_workers=5)\n",
    "    \n",
    "    model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "    \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    t_total = len(train_dataloader) * num_epochs\n",
    "    warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        train_acc = 0.0\n",
    "        test_acc = 0.0\n",
    "        model.train()\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            loss = loss_fn(out, label)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            train_acc += calc_accuracy(out, label)\n",
    "            if batch_id % log_interval == 0:\n",
    "                print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "        print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n",
    "\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            loss = loss_fn(out,label)\n",
    "\n",
    "            val_loss += loss.data.cpu().numpy()\n",
    "            test_acc += calc_accuracy(out, label)\n",
    "\n",
    "        print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "        print(\"epoch {} val_loss {}\".format(e+1, val_loss / (batch_id+1)))\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            torch.save(model, MODEL_P)\n",
    "            epochs_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "        else :\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!' )\n",
    "            break\n",
    "        else:\n",
    "            print(\"Keep going!\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    outs = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        for o in out.detach():\n",
    "            outs.append(o)\n",
    "    test_pred += outs\n",
    "    \n",
    "    outs = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(total_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        for o in out.detach():\n",
    "            outs.append(o)\n",
    "    train_pred += outs\n",
    "    \n",
    "    model = torch.load(MODEL_P)\n",
    "    model.eval()\n",
    "    \n",
    "    outs = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        for o in out.detach():\n",
    "            outs.append(o)\n",
    "    best_val_test_pred += outs\n",
    "    \n",
    "    outs = []\n",
    "    for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(total_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        for o in out.detach():\n",
    "            outs.append(o)\n",
    "    best_val_train_pred += outs\n",
    "            \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d59d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2dd38a4fad74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkobert_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobert_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobert_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobert_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kobert_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-267aa152b887>\u001b[0m in \u001b[0;36mget_kobert_features\u001b[0;34m(rnd)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdr_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mno_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LayerNorm.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination"
     ]
    }
   ],
   "source": [
    "kobert_train1, kobert_test1, kobert_train2, kobert_test2 = get_kobert_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f245d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
