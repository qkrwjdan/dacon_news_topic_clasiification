{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d4b84b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gluonnlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40884/4110738122.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgluonnlp\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gluonnlp'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from KoBERT.kobert.utils import get_tokenizer\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccbbc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdda8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30ceeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/train_data.csv\",index_col=False)\n",
    "# rd_augmentation = pd.read_csv(\"data/train_rd_augmentation.csv\",index_col=False)\n",
    "# rs_augmentation = pd.read_csv(\"data/train_rs_augmentation.csv\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649e5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = pd.concat([dataset,rd_augmentation,rs_augmentation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8901c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = total[[\"title\",\"topic_idx\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b3d6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45649</th>\n",
       "      <td>KB 공략 미국 선진국 스티펠 과 제휴 … IB 시장 금융</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45650</th>\n",
       "      <td>1 보 서울시 교육청 신종 코로나 확산 에 개학 연기 · 휴업 검토</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45651</th>\n",
       "      <td>게시판 키움증권 키움 2020 투자 실전 영웅전 대회</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45652</th>\n",
       "      <td>는 답변 하 배기동 국립 중앙 박물 관장</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45653</th>\n",
       "      <td>2020 인터넷 특별상 기자상 시상식 내달 1 일 개최 … 한국 김성후</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136962 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  topic_idx\n",
       "0                     인천→핀란드 항공기 결항…휴가철 여행객 분통          4\n",
       "1               실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4\n",
       "2               이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3             NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4\n",
       "4                    시진핑 트럼프에 중미 무역협상 조속 타결 희망          4\n",
       "...                                        ...        ...\n",
       "45649         KB 공략 미국 선진국 스티펠 과 제휴 … IB 시장 금융          1\n",
       "45650    1 보 서울시 교육청 신종 코로나 확산 에 개학 연기 · 휴업 검토          2\n",
       "45651            게시판 키움증권 키움 2020 투자 실전 영웅전 대회          1\n",
       "45652                   는 답변 하 배기동 국립 중앙 박물 관장          2\n",
       "45653  2020 인터넷 특별상 기자상 시상식 내달 1 일 개최 … 한국 김성후          2\n",
       "\n",
       "[136962 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edeb741",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = train_test_split(dataset,test_size = 0.2,random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357a870d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        index                            title  topic_idx\n",
       "25339  25339         더민주 서영교 여파 지역위원장 심사기준 강화          6\n",
       "24704  24704      맛집에 너그러운 한국인 해외여행서도 JMT 찾았다          3\n",
       "1834    1834        특징주 삼성물산 지배구조 이슈 부각에 강세종합          1\n",
       "17604  17604   생필품난 베네수엘라 콜롬비아와의 국경 1년 만에 재개방          4\n",
       "19362  19362        금태섭 국민 10명 중 8명 판결문 공개 원해          6\n",
       "...      ...                              ...        ...\n",
       "25631  25631      7번째 개관하는 소극장 운동 발원지 삼일로창고극장          3\n",
       "42297  42297    삼강엠앤티 516억원 규모 케미컬 탱크 3척 공급계약          1\n",
       "33174  33174    이란 외무장관 美 제재 해제하면 협상의 문 활짝 열려          4\n",
       "34959  34959     제주·남부 지방에 호우특보…완도 165.5㎜ 장대비          3\n",
       "10863  10863  영천 새마을금고 강도 범행 6시간만에 검거…범행동기 조사          2\n",
       "\n",
       "[36523 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb031bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import EasyDataAugmentation\n",
    "\n",
    "EDA = EasyDataAugmentation(\n",
    "    morpheme_analyzer=None, alpha_sr=0, alpha_ri=0.2, alpha_rs=0.2, prob_rd=0.2\n",
    ")\n",
    "\n",
    "def augment_data(dataset_df,EDA,repetition_num):\n",
    "\n",
    "    augmented_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for text, label in zip(dataset_df[\"title\"],dataset_df[\"topic_idx\"]):\n",
    "        augmenteds = EDA(data=text, p=None, repetition=repetition_num)\n",
    "\n",
    "        for aug in augmenteds:\n",
    "            augmented_list.append(aug)\n",
    "            label_list.append(label)\n",
    "\n",
    "    new_df = pd.DataFrame({\n",
    "        'title' : augmented_list,\n",
    "        'topic_idx' : label_list\n",
    "    })\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe363309",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = augment_data(dataset_train,EDA,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847ddd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.concat([dataset_train,aug_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91fa5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33284</th>\n",
       "      <td>33284</td>\n",
       "      <td>아시안게임 손흥민 보자…교민 응원 속에 태극전사 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20725</th>\n",
       "      <td>20725</td>\n",
       "      <td>최근 3년간 세종문화회관 매표 고객 중 71%가 여성</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44150</th>\n",
       "      <td>44150</td>\n",
       "      <td>오케스트라 연주로 만나는 픽사 애니메이션 대표작 16편</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8817</th>\n",
       "      <td>8817</td>\n",
       "      <td>관훈클럽 창립 62주년 기념식</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>886</td>\n",
       "      <td>파키스탄 시장에서 폭탄테러…최소 16명 사망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>280</td>\n",
       "      <td>손흥민 선발로 73분 활약…리그 8호골은 다음에</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7239</th>\n",
       "      <td>7239</td>\n",
       "      <td>힐러리 IS 트럼프가 대통령 되길 간절히 기도하고 있다</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29933</th>\n",
       "      <td>29933</td>\n",
       "      <td>손흥민의 토트넘 UEFA챔스리그서 레알 마드리드와 한 조</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>10038</td>\n",
       "      <td>신간 개헌전쟁·나의 형 체 게바라·핸드 투 마우스</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27713</th>\n",
       "      <td>27713</td>\n",
       "      <td>7전 7패 크리스털팰리스 심리상담 받을 생각 없어</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9131 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                            title  topic_idx\n",
       "33284  33284   아시안게임 손흥민 보자…교민 응원 속에 태극전사 ...          5\n",
       "20725  20725    최근 3년간 세종문화회관 매표 고객 중 71%가 여성          3\n",
       "44150  44150   오케스트라 연주로 만나는 픽사 애니메이션 대표작 16편          3\n",
       "8817    8817                 관훈클럽 창립 62주년 기념식          3\n",
       "886      886         파키스탄 시장에서 폭탄테러…최소 16명 사망          4\n",
       "...      ...                              ...        ...\n",
       "280      280       손흥민 선발로 73분 활약…리그 8호골은 다음에          5\n",
       "7239    7239   힐러리 IS 트럼프가 대통령 되길 간절히 기도하고 있다          4\n",
       "29933  29933  손흥민의 토트넘 UEFA챔스리그서 레알 마드리드와 한 조          5\n",
       "10038  10038      신간 개헌전쟁·나의 형 체 게바라·핸드 투 마우스          3\n",
       "27713  27713      7전 7패 크리스털팰리스 심리상담 받을 생각 없어          5\n",
       "\n",
       "[9131 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd65b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_key, label_key, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i]) for i in dataset[sent_key]]\n",
    "        \n",
    "        if not label_key == None:\n",
    "            self.mode = \"train\"\n",
    "        else:\n",
    "            self.mode = \"test\"\n",
    "            \n",
    "        if self.mode == \"train\":\n",
    "            self.labels = [np.int32(i) for i in dataset[label_key]]\n",
    "        else:\n",
    "            self.labels = [np.int32(0) for i in dataset[sent_key]]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.mode == \"train\":\n",
    "            return (self.sentences[i] + (self.labels[i], ))\n",
    "        else:\n",
    "            return self.sentences[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729d9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 40\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5\n",
    "NUM_CLASS = 7\n",
    "MODEL_P = \"models/kobert-0724-2.pth\"\n",
    "\n",
    "epochs_no_improve = 0\n",
    "min_val_loss = np.Inf\n",
    "n_epochs_stop = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5f8b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_total = BERTDataset(dataset, \"title\", \"topic_idx\", tok, max_len, True, False)\n",
    "data_train = BERTDataset(dataset_train, \"title\", \"topic_idx\", tok, max_len, True, False)\n",
    "data_val = BERTDataset(dataset_val, \"title\", \"topic_idx\", tok, max_len, True, False)\n",
    "data_test = BERTDataset(test, \"title\", None, tok, max_len, True, False)\n",
    "\n",
    "# total_dataloader = torch.utils.data.DataLoader(data_total, batch_size=batch_size, num_workers=5)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "val_dataloader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1995a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=NUM_CLASS,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd742e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf364dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c88c7f51e8840469cae92f14318e026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.942183017730713 train acc 0.25\n",
      "epoch 1 batch id 201 loss 1.8889559507369995 train acc 0.15640547263681592\n",
      "epoch 1 batch id 401 loss 1.6556187868118286 train acc 0.23675187032418954\n",
      "epoch 1 batch id 601 loss 0.8542299270629883 train acc 0.39548668885191346\n",
      "epoch 1 batch id 801 loss 0.6733460426330566 train acc 0.5038233458177278\n",
      "epoch 1 batch id 1001 loss 0.528282642364502 train acc 0.5720529470529471\n",
      "epoch 1 batch id 1201 loss 0.3402792811393738 train acc 0.6200041631973355\n",
      "epoch 1 batch id 1401 loss 0.32762253284454346 train acc 0.6550678087080657\n",
      "epoch 1 batch id 1601 loss 0.16090533137321472 train acc 0.6808635227982511\n",
      "epoch 1 batch id 1801 loss 0.5848179459571838 train acc 0.7021446418656302\n",
      "epoch 1 batch id 2001 loss 0.6720952987670898 train acc 0.7188280859570215\n",
      "epoch 1 batch id 2201 loss 0.803919792175293 train acc 0.73284870513403\n",
      "epoch 1 batch id 2401 loss 0.9471951723098755 train acc 0.7421386922115785\n",
      "epoch 1 batch id 2601 loss 0.20263555645942688 train acc 0.7488225682429834\n",
      "epoch 1 batch id 2801 loss 0.25275763869285583 train acc 0.756627097465191\n",
      "epoch 1 batch id 3001 loss 0.2198132425546646 train acc 0.7627874041986005\n",
      "epoch 1 batch id 3201 loss 2.0615146160125732 train acc 0.7680998125585754\n",
      "epoch 1 batch id 3401 loss 0.0736268162727356 train acc 0.772695530726257\n",
      "epoch 1 batch id 3601 loss 1.8551527261734009 train acc 0.7752534018328242\n",
      "epoch 1 batch id 3801 loss 0.28163161873817444 train acc 0.7772954485661668\n",
      "epoch 1 batch id 4001 loss 0.06329041719436646 train acc 0.7812421894526368\n",
      "epoch 1 batch id 4201 loss 1.2212142944335938 train acc 0.7840543918114734\n",
      "epoch 1 batch id 4401 loss 0.6967001557350159 train acc 0.7866678027720972\n",
      "epoch 1 batch id 4601 loss 0.042852967977523804 train acc 0.7893528580743316\n",
      "epoch 1 batch id 4801 loss 2.562601327896118 train acc 0.7908248281607998\n",
      "epoch 1 batch id 5001 loss 0.06776158511638641 train acc 0.79250399920016\n",
      "epoch 1 batch id 5201 loss 0.7924887537956238 train acc 0.7940660449913478\n",
      "epoch 1 batch id 5401 loss 0.08794999122619629 train acc 0.7952578226254398\n",
      "epoch 1 batch id 5601 loss 0.3696367144584656 train acc 0.7973799321549724\n",
      "epoch 1 batch id 5801 loss 0.9763067960739136 train acc 0.7986338562316841\n",
      "epoch 1 batch id 6001 loss 0.08854278177022934 train acc 0.8000229128478586\n",
      "epoch 1 batch id 6201 loss 0.5280631184577942 train acc 0.80083857442348\n",
      "epoch 1 batch id 6401 loss 0.9665566086769104 train acc 0.8018180753007342\n",
      "epoch 1 batch id 6601 loss 0.43909940123558044 train acc 0.8029370549916679\n",
      "epoch 1 batch id 6801 loss 0.008097963407635689 train acc 0.8047437876782826\n",
      "epoch 1 batch id 7001 loss 0.6323647499084473 train acc 0.8056170547064705\n",
      "epoch 1 batch id 7201 loss 3.402557373046875 train acc 0.8066587973892515\n",
      "epoch 1 batch id 7401 loss 2.0822484493255615 train acc 0.8073655587082826\n",
      "epoch 1 batch id 7601 loss 0.2709905207157135 train acc 0.8081173529798711\n",
      "epoch 1 batch id 7801 loss 0.4034920036792755 train acc 0.8085501858736059\n",
      "epoch 1 batch id 8001 loss 0.8557441830635071 train acc 0.8098831396075491\n",
      "epoch 1 batch id 8201 loss 0.0474398210644722 train acc 0.8107395439580539\n",
      "epoch 1 batch id 8401 loss 0.9293749928474426 train acc 0.8115923699559576\n",
      "epoch 1 batch id 8601 loss 1.9449007511138916 train acc 0.8118387396814324\n",
      "epoch 1 batch id 8801 loss 0.028159713372588158 train acc 0.812620724917623\n",
      "epoch 1 batch id 9001 loss 0.5644148588180542 train acc 0.81295133874014\n",
      "epoch 1 batch id 9201 loss 0.05630623176693916 train acc 0.8137702423649603\n",
      "epoch 1 batch id 9401 loss 0.010063395835459232 train acc 0.8142484842038081\n",
      "epoch 1 batch id 9601 loss 0.39850056171417236 train acc 0.8146351942505989\n",
      "epoch 1 batch id 9801 loss 0.01241912879049778 train acc 0.8149869911233547\n",
      "epoch 1 batch id 10001 loss 0.39801025390625 train acc 0.8153997100289971\n",
      "epoch 1 batch id 10201 loss 1.3453937768936157 train acc 0.8158575139692187\n",
      "epoch 1 batch id 10401 loss 0.008016367442905903 train acc 0.8160273050668205\n",
      "epoch 1 batch id 10601 loss 0.08238238841295242 train acc 0.8168156306008867\n",
      "epoch 1 batch id 10801 loss 1.7489910125732422 train acc 0.8170655494861587\n",
      "epoch 1 batch id 11001 loss 0.0370742529630661 train acc 0.8175790837196618\n",
      "epoch 1 batch id 11201 loss 1.235386848449707 train acc 0.8181691366842246\n",
      "epoch 1 batch id 11401 loss 0.20854663848876953 train acc 0.8184808350144724\n",
      "epoch 1 train acc 0.818539731908183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/ipykernel_launcher.py:49: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e540c5a9a29146f5ae8ba47d7b495719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.8588003502626971\n",
      "epoch 1 val_loss 0.5371990839929537\n",
      "Keep going!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f24b1c00f894434b229d8c2804b0642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.5677416920661926 train acc 0.8125\n",
      "epoch 2 batch id 201 loss 0.37380337715148926 train acc 0.8852611940298507\n",
      "epoch 2 batch id 401 loss 0.32049328088760376 train acc 0.8820137157107232\n",
      "epoch 2 batch id 601 loss 0.5624613761901855 train acc 0.8825915141430949\n",
      "epoch 2 batch id 801 loss 0.47818702459335327 train acc 0.8847534332084894\n",
      "epoch 2 batch id 1001 loss 0.33665427565574646 train acc 0.8873626373626373\n",
      "epoch 2 batch id 1201 loss 0.05148313194513321 train acc 0.891184429641965\n",
      "epoch 2 batch id 1401 loss 0.6474764347076416 train acc 0.8931566738044254\n",
      "epoch 2 batch id 1601 loss 0.050547949969768524 train acc 0.8941286695815116\n",
      "epoch 2 batch id 1801 loss 0.6650224924087524 train acc 0.8964117157134925\n",
      "epoch 2 batch id 2001 loss 0.8324832916259766 train acc 0.8969577711144427\n",
      "epoch 2 batch id 2201 loss 0.21268124878406525 train acc 0.898682417083144\n",
      "epoch 2 batch id 2401 loss 1.146361231803894 train acc 0.8979591836734694\n",
      "epoch 2 batch id 2601 loss 0.031900037080049515 train acc 0.8976595540176855\n",
      "epoch 2 batch id 2801 loss 0.2724049687385559 train acc 0.8983175651553017\n",
      "epoch 2 batch id 3001 loss 0.008574254810810089 train acc 0.8978882039320226\n",
      "epoch 2 batch id 3201 loss 1.7524962425231934 train acc 0.8982544517338332\n",
      "epoch 2 batch id 3401 loss 0.006836323998868465 train acc 0.8980079388415172\n",
      "epoch 2 batch id 3601 loss 1.329305648803711 train acc 0.8965565120799778\n",
      "epoch 2 batch id 3801 loss 0.37184393405914307 train acc 0.8956853459615891\n",
      "epoch 2 batch id 4001 loss 0.06264372169971466 train acc 0.8965883529117721\n",
      "epoch 2 batch id 4201 loss 1.241509199142456 train acc 0.8962598190906927\n",
      "epoch 2 batch id 4401 loss 0.2590246796607971 train acc 0.8963019768234493\n",
      "epoch 2 batch id 4601 loss 0.017579739913344383 train acc 0.8961095414040426\n",
      "epoch 2 batch id 4801 loss 1.5527870655059814 train acc 0.8957378671110185\n",
      "epoch 2 batch id 5001 loss 0.016587670892477036 train acc 0.8959708058388323\n",
      "epoch 2 batch id 5201 loss 1.0686711072921753 train acc 0.8953085945010575\n",
      "epoch 2 batch id 5401 loss 0.02369879186153412 train acc 0.8957021847805962\n",
      "epoch 2 batch id 5601 loss 0.006342736072838306 train acc 0.8959672379932155\n",
      "epoch 2 batch id 5801 loss 0.88371342420578 train acc 0.8960955007757283\n",
      "epoch 2 batch id 6001 loss 0.011503205634653568 train acc 0.8962881186468922\n",
      "epoch 2 batch id 6201 loss 0.013593343086540699 train acc 0.8961961780358007\n",
      "epoch 2 batch id 6401 loss 1.1163647174835205 train acc 0.8965981877831589\n",
      "epoch 2 batch id 6601 loss 0.16703858971595764 train acc 0.8967675352219361\n",
      "epoch 2 batch id 6801 loss 0.004640223458409309 train acc 0.8974599323628878\n",
      "epoch 2 batch id 7001 loss 0.016677826642990112 train acc 0.8976574775032138\n",
      "epoch 2 batch id 7201 loss 1.6822923421859741 train acc 0.8979568809887516\n",
      "epoch 2 batch id 7401 loss 0.928953230381012 train acc 0.8979192001080935\n",
      "epoch 2 batch id 7601 loss 0.3209051191806793 train acc 0.8980397316142613\n",
      "epoch 2 batch id 7801 loss 0.17104415595531464 train acc 0.8982662479169338\n",
      "epoch 2 batch id 8001 loss 0.021977810189127922 train acc 0.8983095863017123\n",
      "epoch 2 batch id 8201 loss 0.009448627009987831 train acc 0.8985489574442141\n",
      "epoch 2 batch id 8401 loss 0.05630452185869217 train acc 0.8985909415545769\n",
      "epoch 2 batch id 8601 loss 0.5531702041625977 train acc 0.8984783746076037\n",
      "epoch 2 batch id 8801 loss 0.009341292083263397 train acc 0.898427735484604\n",
      "epoch 2 batch id 9001 loss 0.3232022225856781 train acc 0.898615431618709\n",
      "epoch 2 batch id 9201 loss 0.0090131601318717 train acc 0.8990327138354527\n",
      "epoch 2 batch id 9401 loss 0.008082585409283638 train acc 0.8989269758536326\n",
      "epoch 2 batch id 9601 loss 0.45248520374298096 train acc 0.8990599937506509\n",
      "epoch 2 batch id 9801 loss 0.00508172856643796 train acc 0.8991493214978064\n",
      "epoch 2 batch id 10001 loss 0.38717928528785706 train acc 0.8991163383661633\n",
      "epoch 2 batch id 10201 loss 0.2590222954750061 train acc 0.8993481031271444\n",
      "epoch 2 batch id 10401 loss 0.25170037150382996 train acc 0.8992705028362658\n",
      "epoch 2 batch id 10601 loss 0.03917086869478226 train acc 0.8995790491463069\n",
      "epoch 2 batch id 10801 loss 1.293637752532959 train acc 0.899748865845755\n",
      "epoch 2 batch id 11001 loss 0.004895866382867098 train acc 0.89999772747932\n",
      "epoch 2 batch id 11201 loss 1.1619490385055542 train acc 0.900114945094188\n",
      "epoch 2 batch id 11401 loss 0.019979283213615417 train acc 0.9001019647399351\n",
      "epoch 2 train acc 0.9001445593131242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261b9f46c0b74b29a39237c8bbb31235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.8627408056042032\n",
      "epoch 2 val_loss 0.6258215382287439\n",
      "Keep going!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061a46443854a51855061be276757a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.07210821658372879 train acc 1.0\n",
      "epoch 3 batch id 201 loss 0.4218697249889374 train acc 0.9402985074626866\n",
      "epoch 3 batch id 401 loss 0.08517388999462128 train acc 0.934071072319202\n",
      "epoch 3 batch id 601 loss 0.5581591725349426 train acc 0.9355241264559068\n",
      "epoch 3 batch id 801 loss 0.13398027420043945 train acc 0.9371878901373284\n",
      "epoch 3 batch id 1001 loss 0.3495432734489441 train acc 0.9381868131868132\n",
      "epoch 3 batch id 1201 loss 0.5713797807693481 train acc 0.9400499583680266\n",
      "epoch 3 batch id 1401 loss 0.15126366913318634 train acc 0.9412473233404711\n",
      "epoch 3 batch id 1601 loss 0.038682159036397934 train acc 0.9421064959400375\n",
      "epoch 3 batch id 1801 loss 0.10061424970626831 train acc 0.9433300943920044\n",
      "epoch 3 batch id 2001 loss 0.07882269471883774 train acc 0.9441529235382309\n",
      "epoch 3 batch id 2201 loss 0.047816287726163864 train acc 0.94525215810995\n",
      "epoch 3 batch id 2401 loss 0.41999951004981995 train acc 0.9437473969179508\n",
      "epoch 3 batch id 2601 loss 0.006684298161417246 train acc 0.9434352172241446\n",
      "epoch 3 batch id 2801 loss 0.00444968743249774 train acc 0.9444171724384148\n",
      "epoch 3 batch id 3001 loss 0.5751953125 train acc 0.9441436187937354\n",
      "epoch 3 batch id 3201 loss 1.4989690780639648 train acc 0.9444314276788504\n",
      "epoch 3 batch id 3401 loss 0.002629657043144107 train acc 0.9443546015877683\n",
      "epoch 3 batch id 3601 loss 0.3439478278160095 train acc 0.9432796445431825\n",
      "epoch 3 batch id 3801 loss 0.016689736396074295 train acc 0.9431892922915023\n",
      "epoch 3 batch id 4001 loss 0.030921444296836853 train acc 0.9432485628592852\n",
      "epoch 3 batch id 4201 loss 0.0059654382057487965 train acc 0.9436443703880029\n",
      "epoch 3 batch id 4401 loss 0.005721239373087883 train acc 0.9434929561463303\n",
      "epoch 3 batch id 4601 loss 0.004926281049847603 train acc 0.9432867854814171\n",
      "epoch 3 batch id 4801 loss 0.42477840185165405 train acc 0.9430847740054156\n",
      "epoch 3 batch id 5001 loss 0.004265742842108011 train acc 0.943498800239952\n",
      "epoch 3 batch id 5201 loss 0.862747311592102 train acc 0.9431840030763314\n",
      "epoch 3 batch id 5401 loss 0.026935912668704987 train acc 0.9432396778374376\n",
      "epoch 3 batch id 5601 loss 0.0026766068767756224 train acc 0.9435145509730405\n",
      "epoch 3 batch id 5801 loss 1.3336405754089355 train acc 0.9435765385278401\n",
      "epoch 3 batch id 6001 loss 0.003307393752038479 train acc 0.9435094150974838\n",
      "epoch 3 batch id 6201 loss 0.005243547726422548 train acc 0.9433659893565554\n",
      "epoch 3 batch id 6401 loss 0.8755865097045898 train acc 0.9432803468208093\n",
      "epoch 3 batch id 6601 loss 0.009225876070559025 train acc 0.9433040448416906\n",
      "epoch 3 batch id 6801 loss 0.002014988102018833 train acc 0.943730701367446\n",
      "epoch 3 batch id 7001 loss 0.020105846226215363 train acc 0.9436776889015855\n",
      "epoch 3 batch id 7201 loss 1.0152397155761719 train acc 0.9437057353145396\n",
      "epoch 3 batch id 7401 loss 0.8023143410682678 train acc 0.9437322659100121\n",
      "epoch 3 batch id 7601 loss 0.0070180147886276245 train acc 0.9439629653992896\n",
      "epoch 3 batch id 7801 loss 0.007673459127545357 train acc 0.9440856941417767\n",
      "epoch 3 batch id 8001 loss 0.003088932717218995 train acc 0.9441944756905387\n",
      "epoch 3 batch id 8201 loss 0.004728497471660376 train acc 0.9441683940982807\n",
      "epoch 3 batch id 8401 loss 0.003928918391466141 train acc 0.9440319604808951\n",
      "epoch 3 batch id 8601 loss 0.16144876182079315 train acc 0.9441271363794908\n",
      "epoch 3 batch id 8801 loss 0.004292564000934362 train acc 0.9442463924554028\n",
      "epoch 3 batch id 9001 loss 0.014575407840311527 train acc 0.9441798133540718\n",
      "epoch 3 batch id 9201 loss 0.002127836225554347 train acc 0.9446119986957939\n",
      "epoch 3 batch id 9401 loss 0.003294181078672409 train acc 0.9447266248271461\n",
      "epoch 3 batch id 9601 loss 0.5259650349617004 train acc 0.9448299656285803\n",
      "epoch 3 batch id 9801 loss 0.002640452701598406 train acc 0.9448589429650036\n",
      "epoch 3 batch id 10001 loss 0.0032429026905447245 train acc 0.9450804919508049\n",
      "epoch 3 batch id 10201 loss 0.004623960703611374 train acc 0.9452320850896971\n",
      "epoch 3 batch id 10401 loss 0.0025870606768876314 train acc 0.945245649456783\n",
      "epoch 3 batch id 10601 loss 0.0022816809359937906 train acc 0.9454945288180361\n",
      "epoch 3 batch id 10801 loss 2.268423080444336 train acc 0.9454275067123414\n",
      "epoch 3 batch id 11001 loss 0.0015236919280141592 train acc 0.9457322061630761\n",
      "epoch 3 batch id 11201 loss 0.007295041810721159 train acc 0.9458753682706901\n",
      "epoch 3 batch id 11401 loss 0.003131468314677477 train acc 0.9458216384527673\n",
      "epoch 3 train acc 0.9458614420886631\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2987db4750d4e19accf99e11e46c555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.8661339754816112\n",
      "epoch 3 val_loss 0.6897610888181127\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        \n",
    "        label = label.long().to(device)\n",
    "        \n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n",
    "        \n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        \n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out,label)\n",
    "        \n",
    "        val_loss += loss.data.cpu().numpy()\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    \n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    print(\"epoch {} val_loss {}\".format(e+1, val_loss / (batch_id+1)))\n",
    "        \n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save(model, MODEL_P)\n",
    "        epochs_no_improve = 0\n",
    "        min_val_loss = val_loss\n",
    "    else :\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve == n_epochs_stop:\n",
    "        print('Early stopping!')\n",
    "        early_stop = True\n",
    "        break\n",
    "    else:\n",
    "        print(\"Keep going!\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6773dbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4719430916b486487ca3ae5230a7708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outs = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "    for o in out.detach():\n",
    "        outs.append(o.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11f82a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kobert_test_pred = pd.DataFrame(outs)\n",
    "kobert_test_pred.to_csv(\"results/kobert_test_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb7f596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e289c861f3a342a3800d507492070bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outs = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(total_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "    for o in out.detach():\n",
    "        outs.append(o.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdb3e367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a895635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kobert_train_pred = pd.DataFrame(outs)\n",
    "kobert_train_pred.to_csv(\"results/kobert_train_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e416b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed215603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81752838cd6b42bcbc09bdfefeea95d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outs = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "    for o in out.detach():\n",
    "        outs.append(o.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec91a724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.34231424331665,\n",
       " -0.20525412261486053,\n",
       " 1.046838402748108,\n",
       " 0.006420684978365898,\n",
       " -2.0384185314178467,\n",
       " -3.0738162994384766,\n",
       " -2.5687575340270996]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a51c2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "kobert_best_test_pred = pd.DataFrame(outs)\n",
    "kobert_best_test_pred.to_csv(\"results/kobert_best_test_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccff33b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a335f9253414e88b11bd3d238892a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outs = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(total_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "    for o in out.detach():\n",
    "        outs.append(o.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17b61a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kobert_train_pred = pd.DataFrame(outs)\n",
    "kobert_train_pred.to_csv(\"results/kobert_best_train_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "657e1ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/dacon-new-classification/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cade94976c1341b29a07ddf9ddddb1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "    _,max_indices = torch.max(out,1)\n",
    "    for idx in max_indices.cpu().numpy():\n",
    "        pred.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24650ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['topic_idx'] = pred\n",
    "submission.to_csv(\"results/kobert-aumented2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f746c4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1898d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
