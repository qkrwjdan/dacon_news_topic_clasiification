{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3db2c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, Conv1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import gc\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39dc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data.csv\",encoding=\"utf-8\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ae053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab  \n",
    "tokenizer = Mecab()\n",
    "train[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in train[\"title\"]]\n",
    "test[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in test[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387f7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [인천, →, 핀란드, 항공기, 결항, …, 휴가철, 여행객, 분통]\n",
       "1     [실리콘밸리, 넘어서, 겠, 다, …, 구글, 15, 조, 원, 들여, 美, 전역,...\n",
       "2     [이란, 외무, 긴장, 완화, 해결책, 은, 미국, 이, 경제, 전쟁, 멈추, 는, 것]\n",
       "3     [NYT, 클린턴, 측근, 韓, 기업, 특수, 관계, 조명, …, 공과, 사, 맞물...\n",
       "4                 [시진핑, 트럼프, 에, 중미, 무역, 협상, 조속, 타결, 희망]\n",
       "5     [팔레스타인, 가, 자, 지, 구서, 16, 세, 소년, 이스라엘, 군, 총격, 에...\n",
       "6     [인도, 48, 년, 만, 에, 파키스탄, 공습, …, 테러, 캠프, 폭격, 종합,...\n",
       "7     [美, 대선, TV, 토론, 음담패설, 만회, 실패, 트럼프, …, 사과, 대신, ...\n",
       "8            [푸틴, 한반도, 상황, 진전, 위한, 방안, 김정은, 위원장, 과, 논의]\n",
       "9     [특검, 면죄부, 받, 은, 트럼프, 스캔들, 보도, 언론, 맹공, …, 국민, 의...\n",
       "10                   [日, 오키, 나와서, 열린, 강제, 징용, 노동자, 추도식]\n",
       "11      [이란, 서, 최고, 지도자, 모욕, 혐의, 미국인, 에, 징역, 10, 년, 선고]\n",
       "12    [카니발, 축제, 보, 러, 가, 자, …, 브라질, 리우, 에, 대형, 유람선, 행렬]\n",
       "13         [美, 올랜도, 병원, 최악, 총기, 테러, 부상자, 치료비, 안, 받, 는다]\n",
       "14              [日, 대, 기업, 올해, 평균, 2, ., 46, %, 임금, 인상]\n",
       "15           [WMO, 엘니뇨, 여전히, 강력, …, 2, 분기, 엔, 소멸, 될, 듯]\n",
       "16          [이스라엘, 네타냐후, 유대교, 도, 병역, 문제, 로, 연정, 협상, 진통]\n",
       "17         [UAE, 사우디, 이, 어, 美, 호르무즈, 호위, 연합, 에, 참여, 키로]\n",
       "18    [사우디, 군, 오만, 해, 유조선, 공격, 예멘, 반군, 과, 연결, …, 이, ...\n",
       "19    [개천, 에서, 용, 나와라, 美, 사업가, 모교, 에, 1, 천억, 원, 장학금,...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.concat([train,test])\n",
    "vocab_list = vocab_list[\"tokenized\"]\n",
    "vocab_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3029223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 31861\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "vocab = FreqDist(np.hstack(vocab_list))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6df3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 5000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b416b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "word_to_index['pad'] = 1\n",
    "word_to_index['unk'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b958b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "test_x = []\n",
    "\n",
    "for line in train[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    train_x.append(temp)\n",
    "\n",
    "for line in test[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    test_x.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2e4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in train_x)\n",
    "max_len = max(len(l) for l in test_x)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5fec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in train_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다.\n",
    "        \n",
    "for line in test_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074ae936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np_utils.to_categorical(train[\"topic_idx\"]) # Y_train 에 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c8ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78d99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5003\n",
    "embedding_dim = 200  \n",
    "max_length = max_len\n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4cc0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 7\n",
    "NUM_TRAIN_DATA = len(train)\n",
    "NUM_TEST_DATA = len(test)\n",
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b84382d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lstm(train_df, test_df, vocab_size):\n",
    "    tokenizer = Mecab()\n",
    "    train_df[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in train_df[\"title\"]]\n",
    "    test_df[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in test_df[\"title\"]]\n",
    "    \n",
    "    vocab = FreqDist(np.hstack(train_df[\"tokenized\"]))\n",
    "    print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "    \n",
    "    vocab = vocab.most_common(vocab_size)\n",
    "    \n",
    "    word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "    word_to_index['pad'] = 1\n",
    "    word_to_index['unk'] = 0\n",
    "    \n",
    "    train_x = []\n",
    "    test_x = []\n",
    "\n",
    "    for line in train[\"tokenized\"]:\n",
    "        temp = []\n",
    "        for w in line:\n",
    "            try:\n",
    "                temp.append(word_to_index[w])\n",
    "            except KeyError:\n",
    "                temp.append(word_to_index['unk'])\n",
    "\n",
    "        train_x.append(temp)\n",
    "\n",
    "    for line in test[\"tokenized\"]:\n",
    "        temp = []\n",
    "        for w in line:\n",
    "            try:\n",
    "                temp.append(word_to_index[w])\n",
    "            except KeyError:\n",
    "                temp.append(word_to_index['unk'])\n",
    "\n",
    "        test_x.append(temp)\n",
    "        \n",
    "    max_len = max(len(l) for l in train_x)\n",
    "    max_len = max(len(l) for l in test_x)\n",
    "    print(max_len)\n",
    "    \n",
    "    for line in train_x:\n",
    "        if len(line) < max_len:\n",
    "            line += [word_to_index['pad']] * (max_len - len(line))\n",
    "\n",
    "    for line in test_x:\n",
    "        if len(line) < max_len:\n",
    "            line += [word_to_index['pad']] * (max_len - len(line))\n",
    "            \n",
    "#     train_y = np_utils.to_categorical(train[\"topic_idx\"])\n",
    "    train_y = train[\"topic_idx\"]\n",
    "    \n",
    "    train_x = np.array(train_x)\n",
    "    test_x = np.array(test_x)\n",
    "    \n",
    "    return train_x, train_y, test_x, max_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6bcff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, max_len = preprocessing_lstm(train, test, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8beeb74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'lstm_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential([Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "                Dense(NUM_CLASSES, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d40bba77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 135ms/step - loss: 0.9932 - accuracy: 0.6285 - val_loss: 0.5817 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58175, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 18s 129ms/step - loss: 0.4165 - accuracy: 0.8633 - val_loss: 0.5678 - val_accuracy: 0.8204\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58175 to 0.56777, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 135ms/step - loss: 0.3157 - accuracy: 0.9017 - val_loss: 0.6469 - val_accuracy: 0.7981\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56777\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 19s 135ms/step - loss: 0.2607 - accuracy: 0.9183 - val_loss: 0.6112 - val_accuracy: 0.8240\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56777\n",
      "------------------\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 25s 148ms/step - loss: 1.1069 - accuracy: 0.5816 - val_loss: 0.6184 - val_accuracy: 0.7984\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61836, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 20s 143ms/step - loss: 0.4552 - accuracy: 0.8532 - val_loss: 0.5498 - val_accuracy: 0.8216\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61836 to 0.54976, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 20s 137ms/step - loss: 0.3206 - accuracy: 0.9004 - val_loss: 0.5854 - val_accuracy: 0.8215\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54976\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 22s 151ms/step - loss: 0.2661 - accuracy: 0.9192 - val_loss: 0.6217 - val_accuracy: 0.8233\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54976\n",
      "------------------\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_30 (Bidirectio (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_31 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_32 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 142ms/step - loss: 1.0745 - accuracy: 0.5887 - val_loss: 0.6553 - val_accuracy: 0.7914\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65535, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 20s 141ms/step - loss: 0.4545 - accuracy: 0.8531 - val_loss: 0.5665 - val_accuracy: 0.8299\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65535 to 0.56655, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 20s 137ms/step - loss: 0.3140 - accuracy: 0.9023 - val_loss: 0.6411 - val_accuracy: 0.8126\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56655\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 20s 140ms/step - loss: 0.2522 - accuracy: 0.9233 - val_loss: 0.6866 - val_accuracy: 0.8101\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56655\n",
      "------------------\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_35 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 26s 153ms/step - loss: 1.0416 - accuracy: 0.6108 - val_loss: 0.6147 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61466, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 20s 140ms/step - loss: 0.4466 - accuracy: 0.8548 - val_loss: 0.5251 - val_accuracy: 0.8369\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61466 to 0.52511, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 20s 137ms/step - loss: 0.3086 - accuracy: 0.9023 - val_loss: 0.5798 - val_accuracy: 0.8260\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52511\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.2447 - accuracy: 0.9239 - val_loss: 0.6577 - val_accuracy: 0.8120\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52511\n",
      "------------------\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_37 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_38 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 22s 127ms/step - loss: 1.0498 - accuracy: 0.6066 - val_loss: 0.6346 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63464, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 17s 118ms/step - loss: 0.4390 - accuracy: 0.8593 - val_loss: 0.5484 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63464 to 0.54841, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 17s 118ms/step - loss: 0.3030 - accuracy: 0.9063 - val_loss: 0.5812 - val_accuracy: 0.8250\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54841\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 17s 120ms/step - loss: 0.2461 - accuracy: 0.9238 - val_loss: 0.6229 - val_accuracy: 0.8158\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54841\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "lstm_train1, lstm_test1, lstm_train2, lstm_test2 = get_lstm_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6e62aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnn_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'dnn_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential([Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                Dense(128,activation=\"relu\"),\n",
    "                Dense(128,activation=\"relu\"),\n",
    "                Dropout(0.2),\n",
    "                Dense(7, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d7d3431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.3755 - accuracy: 0.4920 - val_loss: 0.6911 - val_accuracy: 0.7567\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69109, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 17ms/step - loss: 0.5121 - accuracy: 0.8298 - val_loss: 0.5754 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69109 to 0.57535, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.3738 - accuracy: 0.8792 - val_loss: 0.5647 - val_accuracy: 0.8166\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57535 to 0.56468, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 14ms/step - loss: 0.2957 - accuracy: 0.9060 - val_loss: 0.5761 - val_accuracy: 0.8232\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56468\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2375 - accuracy: 0.9255 - val_loss: 0.6093 - val_accuracy: 0.8190\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56468\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 14ms/step - loss: 1.4134 - accuracy: 0.4540 - val_loss: 0.7161 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71615, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.5283 - accuracy: 0.8260 - val_loss: 0.5414 - val_accuracy: 0.8090\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71615 to 0.54140, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.3714 - accuracy: 0.8816 - val_loss: 0.5395 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54140 to 0.53950, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.2893 - accuracy: 0.9110 - val_loss: 0.5421 - val_accuracy: 0.8240\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53950\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2326 - accuracy: 0.9277 - val_loss: 0.5720 - val_accuracy: 0.8248\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53950\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 17ms/step - loss: 1.3816 - accuracy: 0.4860 - val_loss: 0.7259 - val_accuracy: 0.7537\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72588, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 14ms/step - loss: 0.5170 - accuracy: 0.8300 - val_loss: 0.5860 - val_accuracy: 0.8102\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72588 to 0.58596, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.3635 - accuracy: 0.8823 - val_loss: 0.5676 - val_accuracy: 0.8272\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58596 to 0.56761, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 14ms/step - loss: 0.2847 - accuracy: 0.9098 - val_loss: 0.5775 - val_accuracy: 0.8304\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56761\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.2264 - accuracy: 0.9291 - val_loss: 0.6057 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56761\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.4083 - accuracy: 0.4726 - val_loss: 0.7102 - val_accuracy: 0.7539\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71015, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.5482 - accuracy: 0.8167 - val_loss: 0.5708 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71015 to 0.57076, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.4066 - accuracy: 0.8691 - val_loss: 0.5626 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57076 to 0.56263, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.3267 - accuracy: 0.8980 - val_loss: 0.5862 - val_accuracy: 0.8157\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56263\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 14ms/step - loss: 0.2711 - accuracy: 0.9159 - val_loss: 0.5990 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56263\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 1.4165 - accuracy: 0.4608 - val_loss: 0.7366 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73658, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.5249 - accuracy: 0.8246 - val_loss: 0.5288 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73658 to 0.52881, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.3551 - accuracy: 0.8855 - val_loss: 0.5333 - val_accuracy: 0.8243\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52881\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 13ms/step - loss: 0.2738 - accuracy: 0.9112 - val_loss: 0.5590 - val_accuracy: 0.8250\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52881\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "dnn_train1, dnn_test1, dnn_train2, dnn_test2 = get_dnn_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2d8da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'dnn_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len))\n",
    "        model.add(Conv1D(32,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ff1c71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 20ms/step - loss: 1.6207 - accuracy: 0.3767 - val_loss: 1.2171 - val_accuracy: 0.5162\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21708, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.8009 - accuracy: 0.7264 - val_loss: 0.7204 - val_accuracy: 0.7392\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21708 to 0.72043, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.5440 - accuracy: 0.8244 - val_loss: 0.6269 - val_accuracy: 0.7841\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.72043 to 0.62686, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.4298 - accuracy: 0.8656 - val_loss: 0.6008 - val_accuracy: 0.8017\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62686 to 0.60078, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 18ms/step - loss: 0.3482 - accuracy: 0.8946 - val_loss: 0.6046 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.60078\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.2912 - accuracy: 0.9107 - val_loss: 0.6210 - val_accuracy: 0.8146\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.60078\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 20ms/step - loss: 1.6695 - accuracy: 0.3439 - val_loss: 1.2200 - val_accuracy: 0.5361\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.22004, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.9048 - accuracy: 0.6895 - val_loss: 0.7775 - val_accuracy: 0.7373\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.22004 to 0.77753, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.6342 - accuracy: 0.8025 - val_loss: 0.6607 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.77753 to 0.66067, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.4976 - accuracy: 0.8477 - val_loss: 0.6127 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.66067 to 0.61270, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.4082 - accuracy: 0.8760 - val_loss: 0.5972 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61270 to 0.59724, saving model to dnn_model.h5\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.3409 - accuracy: 0.8977 - val_loss: 0.6002 - val_accuracy: 0.8140\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59724\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.2919 - accuracy: 0.9128 - val_loss: 0.6136 - val_accuracy: 0.8155\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59724\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 1.5754 - accuracy: 0.3963 - val_loss: 1.0286 - val_accuracy: 0.6015\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02861, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.7106 - accuracy: 0.7646 - val_loss: 0.6648 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02861 to 0.66484, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.4875 - accuracy: 0.8476 - val_loss: 0.6163 - val_accuracy: 0.8025\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.66484 to 0.61632, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3940 - accuracy: 0.8787 - val_loss: 0.6184 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61632\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3308 - accuracy: 0.9009 - val_loss: 0.6304 - val_accuracy: 0.8164\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61632\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 1.5999 - accuracy: 0.4014 - val_loss: 1.0897 - val_accuracy: 0.5666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.08965, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 23ms/step - loss: 0.7595 - accuracy: 0.7480 - val_loss: 0.6737 - val_accuracy: 0.7646\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.08965 to 0.67371, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 23ms/step - loss: 0.5226 - accuracy: 0.8379 - val_loss: 0.5787 - val_accuracy: 0.8104\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67371 to 0.57870, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.4079 - accuracy: 0.8750 - val_loss: 0.5563 - val_accuracy: 0.8196\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.57870 to 0.55630, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3355 - accuracy: 0.8971 - val_loss: 0.5599 - val_accuracy: 0.8239\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55630\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.2814 - accuracy: 0.9142 - val_loss: 0.5735 - val_accuracy: 0.8262\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.55630\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 1.5939 - accuracy: 0.3838 - val_loss: 1.2169 - val_accuracy: 0.4735\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21685, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 0.8806 - accuracy: 0.6924 - val_loss: 0.7753 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21685 to 0.77529, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 0.5665 - accuracy: 0.8146 - val_loss: 0.6110 - val_accuracy: 0.7874\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.77529 to 0.61099, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.4177 - accuracy: 0.8693 - val_loss: 0.5658 - val_accuracy: 0.8100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.61099 to 0.56582, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3341 - accuracy: 0.8946 - val_loss: 0.5684 - val_accuracy: 0.8168\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56582\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 0.2770 - accuracy: 0.9147 - val_loss: 0.5906 - val_accuracy: 0.8151\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56582\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "cnn_train1, cnn_test1, cnn_train2, cnn_test2 = get_cnn_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0ad8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nn_train = np.hstack([lstm_train1, lstm_train2, \n",
    "                        dnn_train1, dnn_train2,\n",
    "                          cnn_train1, cnn_train2\n",
    "                        ])\n",
    "\n",
    "all_nn_test = np.hstack([lstm_test1, lstm_test2, \n",
    "                        dnn_test1, dnn_test2,\n",
    "                         cnn_test1, cnn_test2\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b86770ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['index', 'title','tokenized']\n",
    "train_X = train.drop(cols_to_drop+['topic_idx'], axis=1).values\n",
    "test_X = test.drop(cols_to_drop, axis=1).values\n",
    "\n",
    "\n",
    "# print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3a41bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.hstack([train_X, all_nn_train])\n",
    "test_X = np.hstack([test_X, all_nn_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "343efcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45654, 42) (9131, 42)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52de08db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.84640\tvalid-mlogloss:1.84626\n",
      "[200]\ttrain-mlogloss:0.41434\tvalid-mlogloss:0.43447\n",
      "[400]\ttrain-mlogloss:0.37831\tvalid-mlogloss:0.42658\n",
      "[600]\ttrain-mlogloss:0.35116\tvalid-mlogloss:0.42510\n",
      "[700]\ttrain-mlogloss:0.33908\tvalid-mlogloss:0.42507\n",
      "train log loss 0.33896605335426017 valid log loss 0.42508008223199656\n",
      "rev 2.352497898158937\n",
      "[0]\ttrain-mlogloss:1.84580\tvalid-mlogloss:1.84683\n",
      "[200]\ttrain-mlogloss:0.40977\tvalid-mlogloss:0.45171\n",
      "[400]\ttrain-mlogloss:0.37392\tvalid-mlogloss:0.44345\n",
      "[600]\ttrain-mlogloss:0.34636\tvalid-mlogloss:0.44210\n",
      "[706]\ttrain-mlogloss:0.33387\tvalid-mlogloss:0.44219\n",
      "train log loss 0.33375851362396747 valid log loss 0.4421902112723416\n",
      "rev 2.2614702327368064\n",
      "[0]\ttrain-mlogloss:1.84581\tvalid-mlogloss:1.84672\n",
      "[200]\ttrain-mlogloss:0.40983\tvalid-mlogloss:0.45437\n",
      "[400]\ttrain-mlogloss:0.37321\tvalid-mlogloss:0.44742\n",
      "[600]\ttrain-mlogloss:0.34592\tvalid-mlogloss:0.44645\n",
      "[668]\ttrain-mlogloss:0.33783\tvalid-mlogloss:0.44667\n",
      "train log loss 0.33783024450705385 valid log loss 0.44667297947322077\n",
      "rev 2.2387743292180775\n",
      "[0]\ttrain-mlogloss:1.84612\tvalid-mlogloss:1.84631\n",
      "[200]\ttrain-mlogloss:0.41469\tvalid-mlogloss:0.43623\n",
      "[400]\ttrain-mlogloss:0.37855\tvalid-mlogloss:0.42774\n",
      "[600]\ttrain-mlogloss:0.35136\tvalid-mlogloss:0.42574\n",
      "[660]\ttrain-mlogloss:0.34402\tvalid-mlogloss:0.42581\n",
      "train log loss 0.34401767034894715 valid log loss 0.4258095039230095\n",
      "rev 2.348468013952102\n",
      "[0]\ttrain-mlogloss:1.84617\tvalid-mlogloss:1.84655\n",
      "[200]\ttrain-mlogloss:0.41121\tvalid-mlogloss:0.44502\n",
      "[400]\ttrain-mlogloss:0.37520\tvalid-mlogloss:0.43900\n",
      "[561]\ttrain-mlogloss:0.35300\tvalid-mlogloss:0.43853\n",
      "train log loss 0.35287266434551673 valid log loss 0.4385376867776713\n",
      "rev 2.2803057300454483\n",
      "11.481516204111372\n",
      "local average valid loss 0.4356580927356479\n",
      "train log loss 0.35211274699914236\n"
     ]
    }
   ],
   "source": [
    "rnd = 42\n",
    "k_cnt = 5\n",
    "\n",
    "kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "\n",
    "test_pred = None\n",
    "weighted_test_pred = None\n",
    "org_train_pred = None\n",
    "avg_k_score = 0\n",
    "reverse_score = 0\n",
    "best_loss = 100\n",
    "best_single_pred = None\n",
    "\n",
    "train_Y = train_y\n",
    "\n",
    "for train_index, test_index in kf.split(train_X,train_Y):\n",
    "    X_train, X_test = train_X[train_index], train_X[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    \n",
    "    params = {\n",
    "            'colsample_bytree': 0.7,\n",
    "            'subsample': 0.8,\n",
    "            'eta': 0.04,\n",
    "            'max_depth': 3,\n",
    "            'eval_metric':'mlogloss',\n",
    "            'objective':'multi:softprob',\n",
    "            'num_class':7,\n",
    "            'tree_method':'gpu_hist'\n",
    "    }\n",
    "    \n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_test, y_test)\n",
    "    d_test = xgb.DMatrix(test_X)\n",
    "    \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=200)\n",
    "    \n",
    "    train_pred = m.predict(d_train)\n",
    "    valid_pred = m.predict(d_valid)\n",
    "    tmp_train_pred = m.predict(xgb.DMatrix(train_X))\n",
    "    \n",
    "    train_score = log_loss(y_train,train_pred)\n",
    "    valid_score = log_loss(y_test,valid_pred)\n",
    "    print('train log loss',train_score,'valid log loss',valid_score)\n",
    "    avg_k_score += valid_score\n",
    "    rev_valid_score = 1.0/valid_score\n",
    "    reverse_score += rev_valid_score\n",
    "    print('rev',rev_valid_score)\n",
    "    \n",
    "    if test_pred is None:\n",
    "        test_pred = m.predict(d_test)\n",
    "        weighted_test_pred = test_pred*rev_valid_score\n",
    "        org_train_pred = tmp_train_pred\n",
    "        best_loss = valid_score\n",
    "        best_single_pred = test_pred\n",
    "    else:\n",
    "        curr_pred = m.predict(d_test)\n",
    "        test_pred += curr_pred\n",
    "        weighted_test_pred += curr_pred*rev_valid_score\n",
    "        org_train_pred += tmp_train_pred\n",
    "\n",
    "        if valid_score < best_loss:\n",
    "            print('BETTER')\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = curr_pred\n",
    "\n",
    "test_pred = test_pred / k_cnt\n",
    "test_pred = np.round(test_pred,4)\n",
    "org_train_pred = org_train_pred / k_cnt\n",
    "avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss['0']=test_pred[:,0]\n",
    "submiss['1']=test_pred[:,1]\n",
    "submiss['2']=test_pred[:,2]\n",
    "submiss['3']=test_pred[:,3]\n",
    "submiss['4']=test_pred[:,4]\n",
    "submiss['5']=test_pred[:,5]\n",
    "submiss['6']=test_pred[:,6]\n",
    "submiss.to_csv(\"results/xgb_{}_2.csv\".format(k_cnt),index=False)\n",
    "print(reverse_score)\n",
    "\n",
    "# weigthed\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "weighted_test_pred = weighted_test_pred / reverse_score\n",
    "weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "submiss['0']=weighted_test_pred[:,0]\n",
    "submiss['1']=weighted_test_pred[:,1]\n",
    "submiss['2']=weighted_test_pred[:,2]\n",
    "submiss['3']=weighted_test_pred[:,3]\n",
    "submiss['4']=weighted_test_pred[:,4]\n",
    "submiss['5']=weighted_test_pred[:,5]\n",
    "submiss['6']=weighted_test_pred[:,6]\n",
    "submiss.to_csv(\"results/weighted_{}_2.csv\".format(k_cnt),index=False)\n",
    "\n",
    "# best single\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "weighted_test_pred = np.round(best_single_pred,4)\n",
    "submiss['0']=weighted_test_pred[:,0]\n",
    "submiss['1']=weighted_test_pred[:,1]\n",
    "submiss['2']=weighted_test_pred[:,2]\n",
    "submiss['3']=weighted_test_pred[:,3]\n",
    "submiss['4']=weighted_test_pred[:,4]\n",
    "submiss['4']=weighted_test_pred[:,5]\n",
    "submiss['4']=weighted_test_pred[:,6]\n",
    "submiss.to_csv(\"results/single_{}_2.csv\".format(k_cnt),index=False)\n",
    "\n",
    "# train log loss\n",
    "print('local average valid loss',avg_k_score)\n",
    "print('train log loss', log_loss(train_Y,org_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5310d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single = pd.read_csv(\"results/single_5_2.csv\")\n",
    "weighted = pd.read_csv(\"results/weighted_5_2.csv\")\n",
    "xgboosted = pd.read_csv(\"results/xgb_5_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37f9361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = []\n",
    "for i in xgboosted.index:\n",
    "    val = xgboosted.loc[i,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]]\n",
    "    val.to_list()\n",
    "    idx = np.argmax(val)\n",
    "    idx_list.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "967d4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss[\"topic_idx\"] = idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20450ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  topic_idx\n",
       "0  45654          2\n",
       "1  45655          3\n",
       "2  45656          2\n",
       "3  45657          2\n",
       "4  45658          3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submiss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4fcb1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss.to_csv(\"results/xgboost_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b8ff78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = []\n",
    "for i in weighted.index:\n",
    "    val = weighted.loc[i,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]]\n",
    "    val.to_list()\n",
    "    idx = np.argmax(val)\n",
    "    idx_list.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c96c6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss[\"topic_idx\"] = idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "08908fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  topic_idx\n",
       "0  45654          2\n",
       "1  45655          3\n",
       "2  45656          2\n",
       "3  45657          2\n",
       "4  45658          3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submiss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68b82952",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss.to_csv(\"results/weighted_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6870a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
