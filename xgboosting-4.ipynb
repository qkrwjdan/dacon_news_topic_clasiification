{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db2c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, Conv1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk import FreqDist\n",
    "from konlpy.tag import Mecab  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39dc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_data.csv\",encoding=\"utf-8\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ae053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab  \n",
    "tokenizer = Mecab()\n",
    "train[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in train[\"title\"]]\n",
    "test[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in test[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387f7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [인천, →, 핀란드, 항공기, 결항, …, 휴가철, 여행객, 분통]\n",
       "1     [실리콘밸리, 넘어서, 겠, 다, …, 구글, 15, 조, 원, 들여, 美, 전역,...\n",
       "2     [이란, 외무, 긴장, 완화, 해결책, 은, 미국, 이, 경제, 전쟁, 멈추, 는, 것]\n",
       "3     [NYT, 클린턴, 측근, 韓, 기업, 특수, 관계, 조명, …, 공과, 사, 맞물...\n",
       "4                 [시진핑, 트럼프, 에, 중미, 무역, 협상, 조속, 타결, 희망]\n",
       "5     [팔레스타인, 가, 자, 지, 구서, 16, 세, 소년, 이스라엘, 군, 총격, 에...\n",
       "6     [인도, 48, 년, 만, 에, 파키스탄, 공습, …, 테러, 캠프, 폭격, 종합,...\n",
       "7     [美, 대선, TV, 토론, 음담패설, 만회, 실패, 트럼프, …, 사과, 대신, ...\n",
       "8            [푸틴, 한반도, 상황, 진전, 위한, 방안, 김정은, 위원장, 과, 논의]\n",
       "9     [특검, 면죄부, 받, 은, 트럼프, 스캔들, 보도, 언론, 맹공, …, 국민, 의...\n",
       "10                   [日, 오키, 나와서, 열린, 강제, 징용, 노동자, 추도식]\n",
       "11      [이란, 서, 최고, 지도자, 모욕, 혐의, 미국인, 에, 징역, 10, 년, 선고]\n",
       "12    [카니발, 축제, 보, 러, 가, 자, …, 브라질, 리우, 에, 대형, 유람선, 행렬]\n",
       "13         [美, 올랜도, 병원, 최악, 총기, 테러, 부상자, 치료비, 안, 받, 는다]\n",
       "14              [日, 대, 기업, 올해, 평균, 2, ., 46, %, 임금, 인상]\n",
       "15           [WMO, 엘니뇨, 여전히, 강력, …, 2, 분기, 엔, 소멸, 될, 듯]\n",
       "16          [이스라엘, 네타냐후, 유대교, 도, 병역, 문제, 로, 연정, 협상, 진통]\n",
       "17         [UAE, 사우디, 이, 어, 美, 호르무즈, 호위, 연합, 에, 참여, 키로]\n",
       "18    [사우디, 군, 오만, 해, 유조선, 공격, 예멘, 반군, 과, 연결, …, 이, ...\n",
       "19    [개천, 에서, 용, 나와라, 美, 사업가, 모교, 에, 1, 천억, 원, 장학금,...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = pd.concat([train,test])\n",
    "vocab_list = vocab_list[\"tokenized\"]\n",
    "vocab_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3029223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 31861\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "vocab = FreqDist(np.hstack(vocab_list))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6df3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 5000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b416b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "word_to_index['pad'] = 1\n",
    "word_to_index['unk'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b958b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "test_x = []\n",
    "\n",
    "for line in train[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    train_x.append(temp)\n",
    "\n",
    "for line in test[\"tokenized\"]: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    test_x.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2e4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in train_x)\n",
    "max_len = max(len(l) for l in test_x)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5fec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in train_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다.\n",
    "        \n",
    "for line in test_x:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074ae936",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np_utils.to_categorical(train[\"topic_idx\"]) # Y_train 에 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c8ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78d99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5003\n",
    "embedding_dim = 200  \n",
    "max_length = 26\n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4cc0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 7\n",
    "NUM_LABELS = 7\n",
    "NUM_TRAIN_DATA = len(train)\n",
    "NUM_TEST_DATA = len(test)\n",
    "VOCAB_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84382d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lstm(train_df, test_df, vocab_size):\n",
    "    tokenizer = Mecab()\n",
    "    train_df[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in train_df[\"title\"]]\n",
    "    test_df[\"tokenized\"] = [tokenizer.morphs(sentence) for sentence in test_df[\"title\"]]\n",
    "    \n",
    "    vocab = FreqDist(np.hstack(train_df[\"tokenized\"]))\n",
    "    print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "    \n",
    "    vocab = vocab.most_common(vocab_size)\n",
    "    \n",
    "    word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}\n",
    "    word_to_index['pad'] = 1\n",
    "    word_to_index['unk'] = 0\n",
    "    \n",
    "    train_x = []\n",
    "    test_x = []\n",
    "\n",
    "    for line in train[\"tokenized\"]:\n",
    "        temp = []\n",
    "        for w in line:\n",
    "            try:\n",
    "                temp.append(word_to_index[w])\n",
    "            except KeyError:\n",
    "                temp.append(word_to_index['unk'])\n",
    "\n",
    "        train_x.append(temp)\n",
    "\n",
    "    for line in test[\"tokenized\"]:\n",
    "        temp = []\n",
    "        for w in line:\n",
    "            try:\n",
    "                temp.append(word_to_index[w])\n",
    "            except KeyError:\n",
    "                temp.append(word_to_index['unk'])\n",
    "\n",
    "        test_x.append(temp)\n",
    "        \n",
    "    max_len = max(len(l) for l in train_x)\n",
    "    max_len = max(len(l) for l in test_x)\n",
    "    print(max_len)\n",
    "    \n",
    "    for line in train_x:\n",
    "        if len(line) < max_len:\n",
    "            line += [word_to_index['pad']] * (max_len - len(line))\n",
    "\n",
    "    for line in test_x:\n",
    "        if len(line) < max_len:\n",
    "            line += [word_to_index['pad']] * (max_len - len(line))\n",
    "            \n",
    "#     train_y = np_utils.to_categorical(train[\"topic_idx\"])\n",
    "    train_y = train[\"topic_idx\"]\n",
    "    \n",
    "    train_x = np.array(train_x)\n",
    "    test_x = np.array(test_x)\n",
    "    \n",
    "    return train_x, train_y, test_x, max_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bcff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, max_len = preprocessing_lstm(train, test, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8beeb74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'lstm_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential([Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True)),\n",
    "                tf.keras.layers.Bidirectional(LSTM(units = 64)),\n",
    "                Dense(NUM_CLASSES, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d40bba77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 23s 131ms/step - loss: 1.0294 - accuracy: 0.6235 - val_loss: 0.6388 - val_accuracy: 0.7913\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63882, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 18s 125ms/step - loss: 0.4275 - accuracy: 0.8613 - val_loss: 0.5394 - val_accuracy: 0.8290\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63882 to 0.53941, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 133ms/step - loss: 0.3009 - accuracy: 0.9065 - val_loss: 0.6018 - val_accuracy: 0.8205\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53941\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 19s 131ms/step - loss: 0.2384 - accuracy: 0.9268 - val_loss: 0.6918 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53941\n",
      "------------------\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 135ms/step - loss: 1.0155 - accuracy: 0.6283 - val_loss: 0.5704 - val_accuracy: 0.8196\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57044, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 18s 128ms/step - loss: 0.4260 - accuracy: 0.8643 - val_loss: 0.5582 - val_accuracy: 0.8210\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57044 to 0.55816, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 18s 129ms/step - loss: 0.3171 - accuracy: 0.9020 - val_loss: 0.5698 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.55816\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 18s 129ms/step - loss: 0.2575 - accuracy: 0.9211 - val_loss: 0.5982 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55816\n",
      "------------------\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 135ms/step - loss: 1.0023 - accuracy: 0.6231 - val_loss: 0.5919 - val_accuracy: 0.8170\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59188, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 18s 129ms/step - loss: 0.4189 - accuracy: 0.8649 - val_loss: 0.5612 - val_accuracy: 0.8339\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59188 to 0.56119, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.3027 - accuracy: 0.9051 - val_loss: 0.6051 - val_accuracy: 0.8229\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56119\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.2562 - accuracy: 0.9202 - val_loss: 0.6366 - val_accuracy: 0.8226\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56119\n",
      "------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 24s 137ms/step - loss: 1.0026 - accuracy: 0.6220 - val_loss: 0.5333 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53334, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.3984 - accuracy: 0.8699 - val_loss: 0.5214 - val_accuracy: 0.8381\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53334 to 0.52138, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.2892 - accuracy: 0.9076 - val_loss: 0.6176 - val_accuracy: 0.8166\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.52138\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.2494 - accuracy: 0.9215 - val_loss: 0.6533 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52138\n",
      "------------------\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 26, 200)           2000400   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 26, 128)           135680    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 26, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,334,615\n",
      "Trainable params: 2,334,615\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 24s 136ms/step - loss: 1.0287 - accuracy: 0.6276 - val_loss: 0.6695 - val_accuracy: 0.7780\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66950, saving model to lstm_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.4547 - accuracy: 0.8524 - val_loss: 0.5609 - val_accuracy: 0.8207\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66950 to 0.56093, saving model to lstm_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 19s 132ms/step - loss: 0.3154 - accuracy: 0.9042 - val_loss: 0.6454 - val_accuracy: 0.8043\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.56093\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 20s 137ms/step - loss: 0.2620 - accuracy: 0.9208 - val_loss: 0.7122 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.56093\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "lstm_train1, lstm_test1, lstm_train2, lstm_test2 = get_lstm_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e62aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnn_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'dnn_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential([Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                Dense(128,activation=\"relu\"),\n",
    "                Dense(128,activation=\"relu\"),\n",
    "                Dropout(0.2),\n",
    "                Dense(7, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7d3431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 1.4222 - accuracy: 0.4808 - val_loss: 0.7588 - val_accuracy: 0.7395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.75884, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.5542 - accuracy: 0.8194 - val_loss: 0.5706 - val_accuracy: 0.8053\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.75884 to 0.57059, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.3942 - accuracy: 0.8740 - val_loss: 0.5437 - val_accuracy: 0.8180\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57059 to 0.54372, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.3073 - accuracy: 0.9028 - val_loss: 0.5555 - val_accuracy: 0.8249\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54372\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2480 - accuracy: 0.9220 - val_loss: 0.5826 - val_accuracy: 0.8264\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54372\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 16ms/step - loss: 1.3777 - accuracy: 0.4905 - val_loss: 0.6498 - val_accuracy: 0.7774\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64982, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.4957 - accuracy: 0.8359 - val_loss: 0.5035 - val_accuracy: 0.8233\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64982 to 0.50349, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.3492 - accuracy: 0.8877 - val_loss: 0.5030 - val_accuracy: 0.8305\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50349 to 0.50302, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2742 - accuracy: 0.9132 - val_loss: 0.5207 - val_accuracy: 0.8334\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.50302\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2214 - accuracy: 0.9312 - val_loss: 0.5692 - val_accuracy: 0.8304\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50302\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 16ms/step - loss: 1.4594 - accuracy: 0.4448 - val_loss: 0.8335 - val_accuracy: 0.7108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.83348, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.6205 - accuracy: 0.7942 - val_loss: 0.6502 - val_accuracy: 0.7972\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.83348 to 0.65024, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.4320 - accuracy: 0.8643 - val_loss: 0.6086 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65024 to 0.60856, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.3290 - accuracy: 0.8960 - val_loss: 0.5985 - val_accuracy: 0.8212\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60856 to 0.59846, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2598 - accuracy: 0.9194 - val_loss: 0.6090 - val_accuracy: 0.8271\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59846\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2114 - accuracy: 0.9348 - val_loss: 0.6450 - val_accuracy: 0.8254\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59846\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 1.3918 - accuracy: 0.4974 - val_loss: 0.6703 - val_accuracy: 0.7673\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67033, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.5190 - accuracy: 0.8283 - val_loss: 0.5343 - val_accuracy: 0.8183\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67033 to 0.53429, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.3718 - accuracy: 0.8816 - val_loss: 0.5247 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53429 to 0.52469, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.2906 - accuracy: 0.9082 - val_loss: 0.5360 - val_accuracy: 0.8295\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52469\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2335 - accuracy: 0.9271 - val_loss: 0.5573 - val_accuracy: 0.8323\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52469\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 16ms/step - loss: 1.4135 - accuracy: 0.4599 - val_loss: 0.7938 - val_accuracy: 0.7244\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79376, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 17ms/step - loss: 0.5940 - accuracy: 0.8048 - val_loss: 0.6139 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79376 to 0.61389, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.4275 - accuracy: 0.8634 - val_loss: 0.5819 - val_accuracy: 0.8120\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61389 to 0.58194, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 16ms/step - loss: 0.3373 - accuracy: 0.8917 - val_loss: 0.6035 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58194\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 0.2718 - accuracy: 0.9146 - val_loss: 0.6245 - val_accuracy: 0.8099\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58194\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "dnn_train1, dnn_test1, dnn_train2, dnn_test2 = get_dnn_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d8da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_feature(train_df, test_df,rnd=1):\n",
    "    train_pred, test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((NUM_TRAIN_DATA,NUM_LABELS)),np.zeros((NUM_TEST_DATA,NUM_LABELS))\n",
    "    \n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 10000\n",
    "    embedding_dim = 200\n",
    "    MODEL_P = 'dnn_model.h5'\n",
    "    NUM_CLASSES = 7\n",
    "    \n",
    "    train_x, train_y, test_x, max_len = preprocessing_lstm(train_df, test_df, NUM_WORDS)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_x,train_y):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS+2, embedding_dim, input_length=max_len))\n",
    "        model.add(Conv1D(32,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(train_x[train_index], to_categorical(train_y[train_index]), \n",
    "                  validation_data=(train_x[test_index], to_categorical(train_y[test_index])),\n",
    "                  batch_size=256, epochs=10,\n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(train_x[test_index])\n",
    "        best_val_test_pred += model.predict(test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ff1c71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 30903\n",
      "26\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 1.6152 - accuracy: 0.3853 - val_loss: 1.0592 - val_accuracy: 0.6192\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05923, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.7644 - accuracy: 0.7501 - val_loss: 0.6804 - val_accuracy: 0.7655\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05923 to 0.68043, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.5268 - accuracy: 0.8347 - val_loss: 0.5959 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68043 to 0.59587, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 23ms/step - loss: 0.4058 - accuracy: 0.8748 - val_loss: 0.5732 - val_accuracy: 0.8128\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59587 to 0.57316, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3280 - accuracy: 0.9004 - val_loss: 0.5870 - val_accuracy: 0.8170\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57316\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 0.2741 - accuracy: 0.9178 - val_loss: 0.6113 - val_accuracy: 0.8152\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57316\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 4s 25ms/step - loss: 1.5856 - accuracy: 0.4036 - val_loss: 1.0463 - val_accuracy: 0.5910\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04626, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 24ms/step - loss: 0.7497 - accuracy: 0.7497 - val_loss: 0.6651 - val_accuracy: 0.7630\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04626 to 0.66506, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 24ms/step - loss: 0.5303 - accuracy: 0.8309 - val_loss: 0.5847 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.66506 to 0.58472, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 24ms/step - loss: 0.4159 - accuracy: 0.8721 - val_loss: 0.5538 - val_accuracy: 0.8151\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58472 to 0.55385, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3393 - accuracy: 0.8953 - val_loss: 0.5496 - val_accuracy: 0.8204\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55385 to 0.54962, saving model to dnn_model.h5\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.2807 - accuracy: 0.9129 - val_loss: 0.5566 - val_accuracy: 0.8250\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.54962\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.2385 - accuracy: 0.9269 - val_loss: 0.5842 - val_accuracy: 0.8227\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.54962\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 1.6490 - accuracy: 0.3642 - val_loss: 1.0657 - val_accuracy: 0.6803\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.06570, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.7428 - accuracy: 0.7572 - val_loss: 0.6918 - val_accuracy: 0.7709\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.06570 to 0.69175, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.5235 - accuracy: 0.8325 - val_loss: 0.6380 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69175 to 0.63800, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 23ms/step - loss: 0.4166 - accuracy: 0.8697 - val_loss: 0.6301 - val_accuracy: 0.8094\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63800 to 0.63008, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3454 - accuracy: 0.8960 - val_loss: 0.6325 - val_accuracy: 0.8158\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63008\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 23ms/step - loss: 0.2881 - accuracy: 0.9142 - val_loss: 0.6389 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.63008\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 1.6296 - accuracy: 0.3831 - val_loss: 1.1912 - val_accuracy: 0.4943\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.19115, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.8492 - accuracy: 0.7036 - val_loss: 0.7125 - val_accuracy: 0.7399\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.19115 to 0.71248, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.5306 - accuracy: 0.8324 - val_loss: 0.5587 - val_accuracy: 0.8093\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71248 to 0.55872, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.3972 - accuracy: 0.8776 - val_loss: 0.5265 - val_accuracy: 0.8296\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55872 to 0.52652, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.3210 - accuracy: 0.9012 - val_loss: 0.5283 - val_accuracy: 0.8323\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52652\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.2632 - accuracy: 0.9204 - val_loss: 0.5528 - val_accuracy: 0.8322\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.52652\n",
      "------------------\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 3s 22ms/step - loss: 1.5558 - accuracy: 0.4151 - val_loss: 1.0019 - val_accuracy: 0.6136\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00188, saving model to dnn_model.h5\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 3s 21ms/step - loss: 0.7367 - accuracy: 0.7602 - val_loss: 0.6560 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00188 to 0.65597, saving model to dnn_model.h5\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.5010 - accuracy: 0.8441 - val_loss: 0.5716 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65597 to 0.57165, saving model to dnn_model.h5\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.3843 - accuracy: 0.8827 - val_loss: 0.5491 - val_accuracy: 0.8252\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.57165 to 0.54913, saving model to dnn_model.h5\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.3095 - accuracy: 0.9061 - val_loss: 0.5579 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54913\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 3s 19ms/step - loss: 0.2606 - accuracy: 0.9204 - val_loss: 0.5814 - val_accuracy: 0.8262\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.54913\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "cnn_train1, cnn_test1, cnn_train2, cnn_test2 = get_cnn_feature(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66298031",
   "metadata": {},
   "outputs": [],
   "source": [
    "kobert_train1 = pd.read_csv(\"results/kobert_train_pred.csv\")\n",
    "kobert_test1 = pd.read_csv(\"results/kobert_test_pred.csv\")\n",
    "kobert_train2 = pd.read_csv(\"results/kobert_best_train_pred.csv\")\n",
    "kobert_test2 = pd.read_csv(\"results/kobert_best_test_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b57ae304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.342314</td>\n",
       "      <td>-0.205254</td>\n",
       "      <td>1.046838</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>-2.038419</td>\n",
       "      <td>-3.073816</td>\n",
       "      <td>-2.568758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.420760</td>\n",
       "      <td>-2.314045</td>\n",
       "      <td>-0.398929</td>\n",
       "      <td>8.449143</td>\n",
       "      <td>-1.850893</td>\n",
       "      <td>-1.624655</td>\n",
       "      <td>0.115052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071454</td>\n",
       "      <td>0.151517</td>\n",
       "      <td>7.751219</td>\n",
       "      <td>-1.214466</td>\n",
       "      <td>-3.102966</td>\n",
       "      <td>-3.573472</td>\n",
       "      <td>-0.161871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.981777</td>\n",
       "      <td>-1.558864</td>\n",
       "      <td>1.980390</td>\n",
       "      <td>-0.964467</td>\n",
       "      <td>-1.347920</td>\n",
       "      <td>-3.438751</td>\n",
       "      <td>-1.385832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.993988</td>\n",
       "      <td>-1.011461</td>\n",
       "      <td>0.392555</td>\n",
       "      <td>8.746573</td>\n",
       "      <td>-2.075460</td>\n",
       "      <td>-2.360103</td>\n",
       "      <td>-1.962877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>-1.439479</td>\n",
       "      <td>-1.662237</td>\n",
       "      <td>0.433224</td>\n",
       "      <td>8.780240</td>\n",
       "      <td>-1.937048</td>\n",
       "      <td>-2.112880</td>\n",
       "      <td>-1.229745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>-1.659509</td>\n",
       "      <td>-1.497266</td>\n",
       "      <td>8.066822</td>\n",
       "      <td>-1.182720</td>\n",
       "      <td>-0.850516</td>\n",
       "      <td>-2.286092</td>\n",
       "      <td>-0.614393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>-2.300084</td>\n",
       "      <td>-1.230400</td>\n",
       "      <td>4.157038</td>\n",
       "      <td>6.802824</td>\n",
       "      <td>-1.551959</td>\n",
       "      <td>-2.549319</td>\n",
       "      <td>-2.754171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>-0.369820</td>\n",
       "      <td>-1.331334</td>\n",
       "      <td>7.999159</td>\n",
       "      <td>-0.094570</td>\n",
       "      <td>-2.301545</td>\n",
       "      <td>-2.433198</td>\n",
       "      <td>-1.426672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9130</th>\n",
       "      <td>-1.556400</td>\n",
       "      <td>-0.881611</td>\n",
       "      <td>1.759504</td>\n",
       "      <td>-2.385018</td>\n",
       "      <td>-1.324765</td>\n",
       "      <td>-2.612632</td>\n",
       "      <td>8.181922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9131 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6\n",
       "0     7.342314 -0.205254  1.046838  0.006421 -2.038419 -3.073816 -2.568758\n",
       "1    -1.420760 -2.314045 -0.398929  8.449143 -1.850893 -1.624655  0.115052\n",
       "2     0.071454  0.151517  7.751219 -1.214466 -3.102966 -3.573472 -0.161871\n",
       "3     6.981777 -1.558864  1.980390 -0.964467 -1.347920 -3.438751 -1.385832\n",
       "4    -0.993988 -1.011461  0.392555  8.746573 -2.075460 -2.360103 -1.962877\n",
       "...        ...       ...       ...       ...       ...       ...       ...\n",
       "9126 -1.439479 -1.662237  0.433224  8.780240 -1.937048 -2.112880 -1.229745\n",
       "9127 -1.659509 -1.497266  8.066822 -1.182720 -0.850516 -2.286092 -0.614393\n",
       "9128 -2.300084 -1.230400  4.157038  6.802824 -1.551959 -2.549319 -2.754171\n",
       "9129 -0.369820 -1.331334  7.999159 -0.094570 -2.301545 -2.433198 -1.426672\n",
       "9130 -1.556400 -0.881611  1.759504 -2.385018 -1.324765 -2.612632  8.181922\n",
       "\n",
       "[9131 rows x 7 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2159cbdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-75f0dc200614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkobert_train1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"6\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkobert_train1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"6\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# kobert_test1 = pd.read_csv(\"results/kobert_test_pred.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# kobert_train2 = pd.read_csv(\"results/kobert_best_train_pred.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# kobert_test2 = pd.read_csv(\"results/kobert_best_test_pred.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# kobert_train1[[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]] = kobert_train1[[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]].apply(pd.to_numeric)\n",
    "# # kobert_test1 = pd.read_csv(\"results/kobert_test_pred.csv\")\n",
    "# # kobert_train2 = pd.read_csv(\"results/kobert_best_train_pred.csv\")\n",
    "# # kobert_test2 = pd.read_csv(\"results/kobert_best_test_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f20e5a80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# kobert_test1 = kobert_test1.drop([\"Unnamed: 0\"],axis=1)\n",
    "# kobert_train2 = kobert_train2.drop([\"Unnamed: 0\"],axis=1)\n",
    "# kobert_test2 = kobert_test2.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4673ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kobert_train1 = kobert_train1.values\n",
    "kobert_test1 = kobert_test1.values\n",
    "kobert_train2 = kobert_train2.values\n",
    "kobert_test2 = kobert_test2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1326e815",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"tensor(-2.4922, device='cuda:0')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-5f62d7762f41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkobert_train1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkobert_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkobert_test1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkobert_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkobert_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkobert_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkobert_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkobert_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: \"tensor(-2.4922, device='cuda:0')\""
     ]
    }
   ],
   "source": [
    "# kobert_train1 = np.array(kobert_train1).astype(np.float)\n",
    "# kobert_test1 = np.array(kobert_test1).astype(np.float)\n",
    "# kobert_train2 = np.array(kobert_train2).astype(np.float)\n",
    "# kobert_test2 = np.array(kobert_test2).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0ad8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nn_train = np.hstack([lstm_train1, lstm_train2, \n",
    "                        dnn_train1, dnn_train2,\n",
    "                          cnn_train1, cnn_train2,\n",
    "                          kobert_train1, kobert_train2\n",
    "                        ])\n",
    "\n",
    "all_nn_test = np.hstack([lstm_test1, lstm_test2, \n",
    "                        dnn_test1, dnn_test2,\n",
    "                         cnn_test1, cnn_test2,\n",
    "                         kobert_test1, kobert_test2\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b86770ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['index', 'title','tokenized']\n",
    "train_X = train.drop(cols_to_drop+['topic_idx'], axis=1).values\n",
    "test_X = test.drop(cols_to_drop, axis=1).values\n",
    "\n",
    "\n",
    "# print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b3a41bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.hstack([train_X, all_nn_train])\n",
    "test_X = np.hstack([test_X, all_nn_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "343efcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45654, 56) (9131, 56)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52de08db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.81084\tvalid-mlogloss:1.81103\n",
      "[200]\ttrain-mlogloss:0.03099\tvalid-mlogloss:0.05008\n",
      "[257]\ttrain-mlogloss:0.02579\tvalid-mlogloss:0.05039\n",
      "train log loss 0.025787835209833437 valid log loss 0.0503856437130458\n",
      "rev 19.846923177069208\n",
      "[0]\ttrain-mlogloss:1.81082\tvalid-mlogloss:1.81087\n",
      "[200]\ttrain-mlogloss:0.03140\tvalid-mlogloss:0.04789\n",
      "[266]\ttrain-mlogloss:0.02546\tvalid-mlogloss:0.04799\n",
      "train log loss 0.025461232586177083 valid log loss 0.04799155942389039\n",
      "rev 20.83699742213828\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.81089\tvalid-mlogloss:1.81098\n",
      "[200]\ttrain-mlogloss:0.03325\tvalid-mlogloss:0.04202\n",
      "[292]\ttrain-mlogloss:0.02502\tvalid-mlogloss:0.04184\n",
      "train log loss 0.02502236755339625 valid log loss 0.0418393163584838\n",
      "rev 23.900964141763016\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.81090\tvalid-mlogloss:1.81103\n",
      "[200]\ttrain-mlogloss:0.03234\tvalid-mlogloss:0.04508\n",
      "[276]\ttrain-mlogloss:0.02539\tvalid-mlogloss:0.04531\n",
      "train log loss 0.025392708434050164 valid log loss 0.045310528874580105\n",
      "rev 22.069925574429018\n",
      "[0]\ttrain-mlogloss:1.81089\tvalid-mlogloss:1.81117\n",
      "[200]\ttrain-mlogloss:0.03225\tvalid-mlogloss:0.04518\n",
      "[256]\ttrain-mlogloss:0.02704\tvalid-mlogloss:0.04524\n",
      "train log loss 0.027036349863049345 valid log loss 0.04523794668919193\n",
      "rev 22.10533574546424\n",
      "108.76014606086376\n",
      "local average valid loss 0.0461529990118384\n",
      "train log loss 0.027388628572790383\n"
     ]
    }
   ],
   "source": [
    "rnd = 42\n",
    "k_cnt = 5\n",
    "\n",
    "kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "\n",
    "test_pred = None\n",
    "weighted_test_pred = None\n",
    "org_train_pred = None\n",
    "avg_k_score = 0\n",
    "reverse_score = 0\n",
    "best_loss = 100\n",
    "best_single_pred = None\n",
    "\n",
    "train_Y = train_y\n",
    "\n",
    "for train_index, test_index in kf.split(train_X,train_Y):\n",
    "    X_train, X_test = train_X[train_index], train_X[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    \n",
    "    params = {\n",
    "            'colsample_bytree': 0.7,\n",
    "            'subsample': 0.8,\n",
    "            'eta': 0.04,\n",
    "            'max_depth': 3,\n",
    "            'eval_metric':'mlogloss',\n",
    "            'objective':'multi:softprob',\n",
    "            'num_class':7,\n",
    "            'tree_method':'gpu_hist'\n",
    "    }\n",
    "    \n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_test, y_test)\n",
    "    d_test = xgb.DMatrix(test_X)\n",
    "    \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=200)\n",
    "    \n",
    "    train_pred = m.predict(d_train)\n",
    "    valid_pred = m.predict(d_valid)\n",
    "    tmp_train_pred = m.predict(xgb.DMatrix(train_X))\n",
    "    \n",
    "    train_score = log_loss(y_train,train_pred)\n",
    "    valid_score = log_loss(y_test,valid_pred)\n",
    "    print('train log loss',train_score,'valid log loss',valid_score)\n",
    "    avg_k_score += valid_score\n",
    "    rev_valid_score = 1.0/valid_score\n",
    "    reverse_score += rev_valid_score\n",
    "    print('rev',rev_valid_score)\n",
    "    \n",
    "    if test_pred is None:\n",
    "        test_pred = m.predict(d_test)\n",
    "        weighted_test_pred = test_pred*rev_valid_score\n",
    "        org_train_pred = tmp_train_pred\n",
    "        best_loss = valid_score\n",
    "        best_single_pred = test_pred\n",
    "    else:\n",
    "        curr_pred = m.predict(d_test)\n",
    "        test_pred += curr_pred\n",
    "        weighted_test_pred += curr_pred*rev_valid_score\n",
    "        org_train_pred += tmp_train_pred\n",
    "\n",
    "        if valid_score < best_loss:\n",
    "            print('BETTER')\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = curr_pred\n",
    "\n",
    "test_pred = test_pred / k_cnt\n",
    "test_pred = np.round(test_pred,4)\n",
    "org_train_pred = org_train_pred / k_cnt\n",
    "avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss['0']=test_pred[:,0]\n",
    "submiss['1']=test_pred[:,1]\n",
    "submiss['2']=test_pred[:,2]\n",
    "submiss['3']=test_pred[:,3]\n",
    "submiss['4']=test_pred[:,4]\n",
    "submiss['5']=test_pred[:,5]\n",
    "submiss['6']=test_pred[:,6]\n",
    "submiss.to_csv(\"results/xgb_{}_3.csv\".format(k_cnt),index=False)\n",
    "print(reverse_score)\n",
    "\n",
    "# weigthed\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "weighted_test_pred = weighted_test_pred / reverse_score\n",
    "weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "submiss['0']=weighted_test_pred[:,0]\n",
    "submiss['1']=weighted_test_pred[:,1]\n",
    "submiss['2']=weighted_test_pred[:,2]\n",
    "submiss['3']=weighted_test_pred[:,3]\n",
    "submiss['4']=weighted_test_pred[:,4]\n",
    "submiss['5']=weighted_test_pred[:,5]\n",
    "submiss['6']=weighted_test_pred[:,6]\n",
    "submiss.to_csv(\"results/weighted_{}_3.csv\".format(k_cnt),index=False)\n",
    "\n",
    "# best single\n",
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "weighted_test_pred = np.round(best_single_pred,4)\n",
    "submiss['0']=weighted_test_pred[:,0]\n",
    "submiss['1']=weighted_test_pred[:,1]\n",
    "submiss['2']=weighted_test_pred[:,2]\n",
    "submiss['3']=weighted_test_pred[:,3]\n",
    "submiss['4']=weighted_test_pred[:,4]\n",
    "submiss['4']=weighted_test_pred[:,5]\n",
    "submiss['4']=weighted_test_pred[:,6]\n",
    "submiss.to_csv(\"results/single_{}_3.csv\".format(k_cnt),index=False)\n",
    "\n",
    "# train log loss\n",
    "print('local average valid loss',avg_k_score)\n",
    "print('train log loss', log_loss(train_Y,org_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5310d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single = pd.read_csv(\"results/single_5_2.csv\")\n",
    "weighted = pd.read_csv(\"results/weighted_5_2.csv\")\n",
    "xgboosted = pd.read_csv(\"results/xgb_5_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "37f9361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = []\n",
    "for i in xgboosted.index:\n",
    "    val = xgboosted.loc[i,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]]\n",
    "    val.to_list()\n",
    "    idx = np.argmax(val)\n",
    "    idx_list.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "967d4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss[\"topic_idx\"] = idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "20450ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  topic_idx\n",
       "0  45654          2\n",
       "1  45655          3\n",
       "2  45656          2\n",
       "3  45657          2\n",
       "4  45658          3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submiss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4fcb1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss.to_csv(\"results/xgboost_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b8ff78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = []\n",
    "for i in weighted.index:\n",
    "    val = weighted.loc[i,[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]]\n",
    "    val.to_list()\n",
    "    idx = np.argmax(val)\n",
    "    idx_list.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c96c6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss=pd.read_csv(\"data/sample_submission.csv\")\n",
    "submiss[\"topic_idx\"] = idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "08908fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  topic_idx\n",
       "0  45654          2\n",
       "1  45655          3\n",
       "2  45656          2\n",
       "3  45657          2\n",
       "4  45658          3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submiss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68b82952",
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss.to_csv(\"results/weighted_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6870a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
