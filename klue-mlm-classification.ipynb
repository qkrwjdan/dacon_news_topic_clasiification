{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0c9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForMaskedLM, AutoModelForPreTraining, TrainingArguments, Trainer\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d691109b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5016d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"klue/bert-base\"\n",
    "# model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "batch_size = 32\n",
    "task = \"nli\"\n",
    "MODEL_P = \"models/klue-bert-base-mlm.pth\"\n",
    "RANDOM_SEED = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2edc05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e3c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/train_data.csv\",index_col=False)\n",
    "test = pd.read_csv(\"data/test_data.csv\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed9dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_token_dict = {0:4038,1:3674,2:3647,3:3697,4:3665,5:4559,6:3713}\n",
    "token_topic_dict = {4038 : 0, 3674 : 1, 3647 : 2, 3697 : 3, 3665 : 4, 4559 : 5, 3713 : 6}\n",
    "topic_dict = {0: \"과학\", 1:\"경제\", 2:\"사회\", 3:\"문화\", 4:\"세계\", 5:\"스포츠\", 6 : \"정치\"}\n",
    "tmp = []\n",
    "\n",
    "for title, topic_idx in zip(dataset[\"title\"],dataset[\"topic_idx\"]):\n",
    "    sentence = title + \".[SEP] 이 문장은 [MASK]\"\n",
    "    tmp.append(sentence)\n",
    "dataset[\"title\"] = tmp\n",
    "    \n",
    "tmp = []\n",
    "for title in test[\"title\"]:\n",
    "    sentence = title + \".[SEP] 이 문장은 [MASK]\"\n",
    "    tmp.append(sentence)\n",
    "\n",
    "test[\"title\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc028bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = train_test_split(dataset,test_size = 0.2,random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9235e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25339</th>\n",
       "      <td>25339</td>\n",
       "      <td>더민주 서영교 여파 지역위원장 심사기준 강화.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24704</th>\n",
       "      <td>24704</td>\n",
       "      <td>맛집에 너그러운 한국인 해외여행서도 JMT 찾았다.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>1834</td>\n",
       "      <td>특징주 삼성물산 지배구조 이슈 부각에 강세종합.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17604</th>\n",
       "      <td>17604</td>\n",
       "      <td>생필품난 베네수엘라 콜롬비아와의 국경 1년 만에 재개방.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19362</th>\n",
       "      <td>19362</td>\n",
       "      <td>금태섭 국민 10명 중 8명 판결문 공개 원해.[SEP] 이 문장은 [MASK]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                              title  topic_idx\n",
       "25339  25339        더민주 서영교 여파 지역위원장 심사기준 강화.[SEP] 이 문장은 [MASK]          6\n",
       "24704  24704     맛집에 너그러운 한국인 해외여행서도 JMT 찾았다.[SEP] 이 문장은 [MASK]          3\n",
       "1834    1834       특징주 삼성물산 지배구조 이슈 부각에 강세종합.[SEP] 이 문장은 [MASK]          1\n",
       "17604  17604  생필품난 베네수엘라 콜롬비아와의 국경 1년 만에 재개방.[SEP] 이 문장은 [MASK]          4\n",
       "19362  19362       금태섭 국민 10명 중 8명 판결문 공개 원해.[SEP] 이 문장은 [MASK]          6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74b1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c243da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(dataset,sent_key,label_key,tokenizer):\n",
    "    if label_key is None :\n",
    "        labels = [np.int64(0) for i in dataset[sent_key]]\n",
    "    else :\n",
    "        labels = [np.int64(i) for i in dataset[label_key]]\n",
    "    \n",
    "    sentences = tokenizer(dataset[sent_key].tolist(),truncation=True,padding=True)\n",
    "#     sentences = tokenizer(dataset[sent_key].tolist(),truncation=True)\n",
    "\n",
    "    input_ids = sentences.input_ids\n",
    "    token_type_ids = sentences.token_type_ids\n",
    "    attention_mask = sentences.attention_mask\n",
    "    masked_token_idx = []\n",
    "    \n",
    "    for input_id in input_ids:\n",
    "        masked_token_idx.append(input_id.index(4))\n",
    "        \n",
    "    \n",
    "    return list([input_ids, token_type_ids, attention_mask, labels, masked_token_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c14b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = bert_tokenize(dataset_train,\"title\",\"topic_idx\",tokenizer)\n",
    "validation_inputs = bert_tokenize(dataset_val,\"title\",\"topic_idx\",tokenizer)\n",
    "test_inputs = bert_tokenize(test,\"title\",None,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea6e06d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2, 24905,  1042,  4795, 19982,  2129,   121,  6904, 16311,     1,\n",
       "         14392,    18,     3,  1504,  6265,  2073,     4,     3,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor(16))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[0][1], test_inputs[4][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e6a3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_inputs)):\n",
    "    train_inputs[i] = torch.tensor(train_inputs[i])\n",
    "    \n",
    "for i in range(len(validation_inputs)):\n",
    "    validation_inputs[i] = torch.tensor(validation_inputs[i])\n",
    "    \n",
    "for i in range(len(test_inputs)):\n",
    "    test_inputs[i] = torch.tensor(test_inputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9fd7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(*train_inputs)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(*validation_inputs)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(*test_inputs)\n",
    "# test_sampler = RandomSampler(test_data)\n",
    "# test_dataloader = DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb99fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0a028e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    2,  1804,  2146,  ...,     0,     0,     0],\n",
       "         [    2, 10898, 11079,  ...,     0,     0,     0],\n",
       "         [    2,   594,  2398,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2,  4004, 11298,  ...,     0,     0,     0],\n",
       "         [    2, 16761, 29712,  ...,     0,     0,     0],\n",
       "         [    2, 30072,    25,  ...,     0,     0,     0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([1, 1, 3, 3, 6, 6, 1, 5, 4, 5, 4, 1, 1, 1, 3, 6, 5, 1, 1, 5, 2, 4, 2, 4,\n",
       "         1, 6, 0, 6, 1, 1, 0, 1, 5, 2, 0, 4, 6, 2, 4, 5, 2, 2, 0, 5, 5, 6, 3, 0,\n",
       "         2, 6, 3, 0, 6, 6, 5, 0, 5, 3, 2, 1, 0, 2, 1, 5]),\n",
       " tensor([25, 14, 15, 18, 19, 20, 14, 22, 18, 25, 22, 19, 19, 21, 19, 19, 26, 10,\n",
       "         26, 21, 23, 21, 19, 16, 18, 20, 18, 22, 24, 28, 18, 20, 17, 14, 21, 21,\n",
       "         19, 16, 21, 19, 19, 17, 19, 27, 19, 19,  9, 18, 20, 21, 19, 14, 17, 21,\n",
       "         20, 22, 20, 15, 21, 25, 26, 11, 16, 16])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476d36ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=7)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForPreTraining.from_pretrained(model_checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cbbcc973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForPreTrainingOutput(loss=None, prediction_logits=tensor([[[ -6.0406,   3.1712,  -4.7043,  ...,  -6.7852,  -6.0219,  -6.4100],\n",
      "         [ -6.5202,   6.0657,  -7.2749,  ...,  -6.4878,  -8.5244,  -6.5877],\n",
      "         [ -8.7517,   7.6491,  -6.4469,  ...,  -2.6874,  -5.8746,  -6.4186],\n",
      "         ...,\n",
      "         [ -7.9496,   2.6318,  -6.0783,  ...,  -5.1762,  -6.1159,  -7.9961],\n",
      "         [ -6.8191,   5.3924,  -4.8108,  ...,  -4.5111,  -5.3303,  -6.0203],\n",
      "         [ -7.1067,   5.0341,  -4.6863,  ...,  -5.6264,  -5.4639,  -6.7108]],\n",
      "\n",
      "        [[ -6.5271,   3.6221,  -5.4519,  ...,  -5.8584,  -5.7478,  -5.1125],\n",
      "         [ -5.3117,   8.8945,  -4.5603,  ...,  -4.1524,  -2.3396,  -1.6568],\n",
      "         [ -6.4958,   5.0262,  -3.3150,  ...,  -5.1708,  -4.7854,  -4.2634],\n",
      "         ...,\n",
      "         [ -6.1085,   4.6625,  -5.5953,  ...,  -5.1957,  -3.4544,  -3.6032],\n",
      "         [ -5.6021,   8.5952,  -4.4087,  ...,  -4.1917,  -3.5906,  -4.4249],\n",
      "         [ -6.1020,   7.6569,  -4.2453,  ...,  -4.7865,  -4.0938,  -4.5708]],\n",
      "\n",
      "        [[ -5.9488,   2.8386,  -4.0771,  ...,  -6.1484,  -6.0774,  -5.9659],\n",
      "         [ -2.5275,   4.9975,  -6.1570,  ...,  -4.3374,  -2.8192,  -5.3540],\n",
      "         [ -5.5104,   6.1125,  -6.3826,  ...,  -6.7354,  -6.9716,  -6.6409],\n",
      "         ...,\n",
      "         [ -7.6446,   6.9945,  -6.2364,  ...,  -4.3830,  -4.3423,  -6.5522],\n",
      "         [ -6.3700,   5.8174,  -4.2275,  ...,  -4.4335,  -4.4233,  -4.3578],\n",
      "         [ -6.6230,   6.5226,  -5.1433,  ...,  -3.8284,  -3.0146,  -4.9783]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -5.8794,   2.4451,  -4.6033,  ...,  -7.1471,  -7.7325,  -6.5544],\n",
      "         [ -8.5412,   5.3535,  -8.4227,  ...,  -9.6394, -13.1222,  -9.8478],\n",
      "         [ -7.3186,   5.0227,  -5.3977,  ...,  -8.1875, -13.1281, -10.3766],\n",
      "         ...,\n",
      "         [ -5.8594,   4.9117,  -5.0512,  ...,  -6.9743,  -6.8061,  -6.2154],\n",
      "         [ -5.0884,   4.2951,  -4.3389,  ...,  -7.5672,  -6.9911,  -6.2124],\n",
      "         [ -4.8520,   4.0257,  -3.6387,  ...,  -5.2541,  -5.4684,  -5.1262]],\n",
      "\n",
      "        [[ -6.7143,   3.2108,  -3.6201,  ...,  -7.0249,  -5.6752,  -5.9616],\n",
      "         [ -5.2832,   5.7297,  -4.3567,  ...,  -4.3934,  -5.7794,  -5.8441],\n",
      "         [ -9.9044,   6.6343,  -5.9840,  ...,  -9.6083,  -9.3314,  -9.4698],\n",
      "         ...,\n",
      "         [ -5.2840,   6.3634,  -3.9041,  ...,  -6.6885,  -5.5581,  -6.3134],\n",
      "         [ -4.4788,   6.8700,  -5.0612,  ...,  -6.5279,  -4.8341,  -5.3368],\n",
      "         [ -6.2839,   4.7060,  -4.4142,  ...,  -6.6591,  -4.6636,  -5.1879]],\n",
      "\n",
      "        [[ -5.5589,   3.2959,  -5.0378,  ...,  -6.8225,  -5.6584,  -5.6049],\n",
      "         [ -7.1254,   5.9205,  -8.2911,  ...,  -9.2973,  -9.8846,  -6.5766],\n",
      "         [ -5.4208,   3.5982,  -7.0756,  ...,  -6.0980,  -4.1374,  -4.9395],\n",
      "         ...,\n",
      "         [ -4.1116,   6.5282,  -4.4576,  ...,  -6.3953,  -5.3517,  -4.4537],\n",
      "         [ -6.5038,   2.7152,  -5.6953,  ...,  -6.7531,  -5.5363,  -6.1570],\n",
      "         [ -4.1436,   7.5610,  -4.9506,  ...,  -5.9867,  -4.3230,  -5.0980]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), seq_relationship_logits=tensor([[-1.8245e-01, -4.5039e-01],\n",
      "        [ 2.5835e-01, -2.2467e-03],\n",
      "        [-1.6771e-01,  3.7928e-01],\n",
      "        [ 1.4854e-01,  5.9186e-01],\n",
      "        [ 6.0955e-01, -2.3492e-01],\n",
      "        [ 7.7759e-01, -2.4707e-01],\n",
      "        [-6.5508e-02,  1.1314e-01],\n",
      "        [-2.9042e-01,  3.5132e-01],\n",
      "        [ 7.5760e-01,  5.6575e-01],\n",
      "        [ 3.0261e-02,  8.0406e-02],\n",
      "        [-4.3801e-01, -6.0914e-01],\n",
      "        [ 1.8459e-01, -1.1994e-01],\n",
      "        [-4.6875e-01,  5.9108e-01],\n",
      "        [ 6.4062e-03, -6.1376e-01],\n",
      "        [ 1.2940e-02, -5.3038e-01],\n",
      "        [-2.1283e-01, -2.0505e-01],\n",
      "        [-3.1686e-01,  2.5653e-01],\n",
      "        [-3.0973e-02,  1.9723e-01],\n",
      "        [ 4.3504e-01,  9.1065e-01],\n",
      "        [-7.3757e-02,  1.5536e+00],\n",
      "        [-2.6622e-01,  2.9285e-01],\n",
      "        [ 4.0355e-01,  4.4695e-01],\n",
      "        [ 6.2121e-01, -2.4258e-01],\n",
      "        [ 2.8712e-01, -1.6897e-01],\n",
      "        [ 2.8439e-01, -7.5023e-01],\n",
      "        [-2.3326e-01,  4.7146e-02],\n",
      "        [ 7.5561e-02, -1.7255e-01],\n",
      "        [-5.1129e-01, -5.5289e-01],\n",
      "        [-1.6690e-01, -3.0613e-01],\n",
      "        [ 3.5367e-01, -5.7550e-01],\n",
      "        [-2.4718e-01, -8.6891e-01],\n",
      "        [ 2.7165e-01,  1.0020e+00],\n",
      "        [ 3.6869e-01, -2.4713e-01],\n",
      "        [ 3.5877e-01,  1.8352e+00],\n",
      "        [-2.5297e-01, -5.5938e-02],\n",
      "        [-6.5970e-02, -3.6575e-01],\n",
      "        [-3.0233e-01,  4.3340e-02],\n",
      "        [-2.1417e-01, -5.4391e-01],\n",
      "        [-4.9661e-01,  5.6658e-01],\n",
      "        [-3.4358e-01,  4.8478e-01],\n",
      "        [ 3.8704e-01, -4.0449e-01],\n",
      "        [-2.7779e-01, -2.5236e-01],\n",
      "        [ 8.8769e-01, -3.2177e-01],\n",
      "        [-4.6197e-01, -9.0757e-01],\n",
      "        [-1.7924e-01,  6.3616e-01],\n",
      "        [-1.7590e-01, -1.1027e-03],\n",
      "        [ 2.2017e-01,  4.7816e-02],\n",
      "        [-6.6353e-01, -2.6659e-01],\n",
      "        [ 6.6580e-02,  1.2833e-01],\n",
      "        [-8.6170e-01, -6.1770e-01],\n",
      "        [-5.2136e-01, -5.8495e-01],\n",
      "        [-1.9444e-01, -8.0233e-01],\n",
      "        [-3.7479e-01, -7.1073e-01],\n",
      "        [-4.3800e-01, -3.7249e-01],\n",
      "        [ 3.6965e-01, -7.2223e-01],\n",
      "        [ 9.3438e-02,  4.8960e-01],\n",
      "        [-5.0483e-03, -5.2299e-01],\n",
      "        [ 1.9774e-01, -1.1690e+00],\n",
      "        [ 6.8513e-01, -2.4889e-01],\n",
      "        [ 3.3310e-01,  4.5200e-01],\n",
      "        [-8.6836e-03, -7.4636e-02],\n",
      "        [-4.6375e-01,  9.1718e-01],\n",
      "        [-1.6168e-01,  2.7390e-01],\n",
      "        [-2.5259e-01,  5.1182e-01]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data = tuple(t.to(device) for t in data)\n",
    "input_ids, token_ids, mask, label, masked_token_idx = data\n",
    "outputs = model(input_ids, token_type_ids=token_ids, attention_mask=mask)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be35fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_cls, logits_lm = outputs[1], outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "96d18841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2240, 32000])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_lm.view(-1, logits_lm.size(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ace2bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 35, 32000])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76aab14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 24, 16, 13, 21, 21, 25, 19, 23, 18, 22, 22, 16, 21, 19, 17, 23, 20,\n",
       "        21, 16, 18, 18, 21, 18, 20, 19, 19, 22, 19, 19, 22, 10, 23, 16, 25, 25,\n",
       "        20, 27, 21, 16, 20, 26, 19, 21, 19, 17, 26, 21, 20, 20, 21, 20, 26, 18,\n",
       "        15, 22, 19, 20, 17, 19, 24, 26, 21, 23], device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "445364c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "24\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "16\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "13\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "25\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "23\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "18\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "22\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "22\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "16\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "17\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "23\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "16\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "18\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "18\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "18\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "22\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "22\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "10\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "23\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "16\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "25\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "25\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "27\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "16\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "26\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "17\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "26\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "26\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "18\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "15\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "22\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "20\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "17\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "19\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "24\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "26\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "21\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "23\n",
      "<class 'numpy.int64'>\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 35\n",
    "mask_label = [topic_token_dict[lb] for lb in label.to('cpu').numpy() ]\n",
    "label_lms = []\n",
    "for idx, label in zip(masked_token_idx.to('cpu').numpy(),mask_label):\n",
    "    label_lm = np.full(SEQUENCE_LENGTH, dtype=np.int, fill_value=-1)\n",
    "    print(idx)\n",
    "    print(type(idx))\n",
    "    print(label_lm)\n",
    "    label_lm[idx] = label\n",
    "    label_lms.append(label_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "12ff4703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 35)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(label_lms)\n",
    "label_lms_np = np.array(label_lms)\n",
    "label_lms_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09521d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1, -1,  ..., -1, -1, -1], device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lms_pt = torch.tensor(label_lms_np,dtype=torch.int64).to(device)\n",
    "label_lms_pt.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cbcdeed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2240, 32000]) torch.Size([2240])\n",
      "tensor(7.8615, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(logits_lm.view(-1, logits_lm.size(2)).shape , label_lms_pt.view(-1).shape)\n",
    "criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), label_lms_pt.view(-1))\n",
    "print(loss_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "811a7232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8cad83fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (43) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33724/1715209599.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabels_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlabels_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mloss_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2468\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2261\u001b[1;33m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[0;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (43) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "labels_cls = [1 for _ in range(len(input_ids))]\n",
    "labels_cls = torch.tensor(labels_cls).to(device)\n",
    "loss_cls = criterion_cls(logits_cls, labels_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "60e33284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b5845916",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_cls + loss_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9a989270",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41f72ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33724/383757286.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmasked_token_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# masked_token_ids.to('cpu').numpy(), label.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b6d1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = outputs[0].to(\"cpu\").detach().numpy()\n",
    "# print(logits.shape)\n",
    "# print(len(logits[0][0]))\n",
    "# print(np.argmax(logits,axis=2).shape)\n",
    "# out = np.argmax(logits,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6fb2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_tensor(logits,idx):\n",
    "    tmp = []\n",
    "    for b,i in zip(logits,idx):\n",
    "        tmp.append(b[i])\n",
    "    \n",
    "    pred = torch.tensor(tmp,requires_grad=True)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# pred = get_predict_tensor(logits,[18])\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e371ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = get_predict_tensor(logits,masked_token_ids.to('cpu').numpy())\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3536c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = [topic_token_dict[lb] for lb in label.to('cpu').numpy() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e864ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = torch.tensor(answer)\n",
    "# loss_fn(pred,ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34decab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(outputs[0],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dd94490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(tokenizer.convert_ids_to_tokens(data[0][0]))\n",
    "# print(data[0][0])\n",
    "# print(data[1][0])\n",
    "# print(data[2][0])\n",
    "# print(tokenizer.convert_ids_to_tokens(out[0]))\n",
    "# print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2717503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 1\n",
    "\n",
    "num_warmup_steps = 0\n",
    "\n",
    "warmup_ratio = 0.1\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "warmup_step = int(num_training_steps * warmup_ratio)\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3dd3f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_24908/3185985768.py:11: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1,epochs+1,desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d0a058b9d04c0793a28474ba5ccdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_24908/3185985768.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d8f880771a4628b736038f5c7749f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tAverage Training loss: 0.2860492701510101\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "    batch_loss = 0\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx = batch\n",
    "#         print(b_input_ids)\n",
    "#         print(b_token_type_ids)\n",
    "#         print(b_input_mask)\n",
    "#         print(b_labels)\n",
    "#         print(b_masked_token_idx)\n",
    "        outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        # calculate loss\n",
    "        logits_cls, logits_lm = outputs[1], outputs[0]\n",
    "        \n",
    "        mask_label = [topic_token_dict[lb] for lb in b_labels.to('cpu').numpy() ]\n",
    "        label_lms = []\n",
    "        SEQUENCE_LENGTH = 35\n",
    "        \n",
    "        for idx, label in zip(b_masked_token_idx.to('cpu').numpy(),mask_label):\n",
    "            label_lm = np.full(SEQUENCE_LENGTH, dtype=np.int, fill_value=-1)\n",
    "            label_lm[idx] = label\n",
    "            label_lms.append(label_lm)\n",
    "            \n",
    "        label_lms_np = np.array(label_lms)\n",
    "        label_lms_pt = torch.tensor(label_lms_np,dtype=torch.int64).to(device)\n",
    "        loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), label_lms_pt.view(-1))\n",
    "        \n",
    "        labels_cls = [1 for _ in range(len(b_input_ids))]\n",
    "        labels_cls = torch.tensor(labels_cls).to(device)\n",
    "        loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "        \n",
    "        loss = loss_cls + loss_lm\n",
    "        loss.backward()        \n",
    "#         logits = outputs[0].to(\"cpu\").detach().numpy()\n",
    "        \n",
    "#         pred = get_predict_tensor(logits,b_masked_token_idx.to('cpu').numpy())\n",
    "#         answer = [topic_token_dict[lb] for lb in b_labels.to('cpu').numpy() ]\n",
    "        \n",
    "#         answer = torch.tensor(answer)\n",
    "#         loss = loss_fn(pred,answer)\n",
    "#         loss = loss.to(device)\n",
    "#         loss.backward()\n",
    "#         print(loss)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = batch_loss / len(train_dataloader)\n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12027515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_24908/3892437971.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(validation_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064eadc82b2748c9bffaa57c5a797e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tAverage validation loss: 1.839752100798842\n"
     ]
    }
   ],
   "source": [
    "predict_li = []\n",
    "label_li = []\n",
    "\n",
    "# eval\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm_notebook(validation_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx = batch\n",
    "    \n",
    "    outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits_cls, logits_lm = outputs[1], outputs[0]\n",
    "    \n",
    "    logits_lm_cpu = logits_lm.to('cpu').detach().numpy()\n",
    "    out = np.argmax(logits_lm_cpu,axis=2)\n",
    "    \n",
    "    masked_token_idx_np = b_masked_token_idx.to('cpu').numpy()\n",
    "    \n",
    "    for i in range(len(out)):\n",
    "        l = token_topic_dict[out[i][masked_token_idx_np[i]]]\n",
    "        predict_li.append(l)\n",
    "        \n",
    "    for l in b_labels.to('cpu').numpy():\n",
    "        label_li.append(l)\n",
    "    \n",
    "    mask_label = [topic_token_dict[lb] for lb in b_labels.to('cpu').numpy() ]\n",
    "    label_lms = []\n",
    "    SEQUENCE_LENGTH = 35\n",
    "\n",
    "    for idx, label in zip(b_masked_token_idx.to('cpu').numpy(),mask_label):\n",
    "        label_lm = np.full(SEQUENCE_LENGTH, dtype=np.int, fill_value=-1)\n",
    "        label_lm[idx] = label\n",
    "        label_lms.append(label_lm)\n",
    "\n",
    "    label_lms_np = np.array(label_lms)\n",
    "    label_lms_pt = torch.tensor(label_lms_np,dtype=torch.int64).to(device)\n",
    "    loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), label_lms_pt.view(-1))\n",
    "\n",
    "    labels_cls = [1 for _ in range(len(b_input_ids))]\n",
    "    labels_cls = torch.tensor(labels_cls).to(device)\n",
    "    loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "    \n",
    "    loss = loss_cls + loss_lm\n",
    "    \n",
    "    batch_loss += loss.item()\n",
    "    \n",
    "avg_validation_loss = batch_loss / len(validation_dataloader)\n",
    "print(F'\\n\\tAverage validation loss: {avg_validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "816c9d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8774504435439711"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(label_li, predict_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96cb535c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_24908/1987576123.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad377fe330a4bc2a17c0168232bc853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_li = []\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx = batch\n",
    "    \n",
    "    outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits_cls, logits_lm = outputs[1], outputs[0]\n",
    "    \n",
    "    logits_lm = logits_lm.to('cpu').detach().numpy()\n",
    "    out = np.argmax(logits_lm,axis=2)\n",
    "    \n",
    "    masked_token_idx_np = b_masked_token_idx.to('cpu').numpy()\n",
    "    \n",
    "#     print(out)\n",
    "    \n",
    "    for i in range(len(out)):\n",
    "        l = token_topic_dict[out[i][masked_token_idx_np[i]]]\n",
    "        predict_li.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10c4d019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9131"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc717f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb63b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['topic_idx'] = predict_li\n",
    "submission.to_csv(\"results/klue-bert-mlm-classification-2epoch-norandom.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2efc5ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(validation_dataloader))\n",
    "data[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cf165ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForPreTrainingOutput(loss=None, prediction_logits=tensor([[[ -7.5750,   4.0583,  -5.7115,  ...,  -8.4080, -10.1831,  -6.9428],\n",
      "         [ -5.7624,   6.3970,  -5.7372,  ...,  -8.4756,  -7.7428,  -6.1240],\n",
      "         [ -6.1303,   4.9622,  -5.0594,  ...,  -7.7942,  -7.3606,  -8.2195],\n",
      "         ...,\n",
      "         [ -6.7994,   5.5160,  -4.6602,  ...,  -7.2120,  -6.9814,  -6.1145],\n",
      "         [ -6.7101,   5.8034,  -4.9487,  ...,  -7.9012,  -7.3610,  -5.1996],\n",
      "         [ -7.5329,   5.2829,  -4.8685,  ...,  -8.5925,  -7.0635,  -6.2882]],\n",
      "\n",
      "        [[ -7.3972,   4.0784,  -4.7739,  ...,  -7.8792, -10.2142,  -7.2260],\n",
      "         [ -5.4033,   7.5581,  -4.3354,  ...,  -7.0303,  -7.8286,  -5.9605],\n",
      "         [ -4.9273,   7.4306,  -4.3602,  ...,  -4.0567,  -4.7388,  -3.5071],\n",
      "         ...,\n",
      "         [ -6.2394,   4.7532,  -3.5773,  ...,  -7.1833,  -8.6217,  -5.8134],\n",
      "         [ -6.1106,   4.8454,  -4.0421,  ...,  -7.2750,  -8.5017,  -5.9248],\n",
      "         [ -5.9491,   6.4712,  -3.8178,  ...,  -5.6931,  -6.9110,  -5.0750]],\n",
      "\n",
      "        [[ -7.2549,   4.1167,  -5.5277,  ...,  -8.3278, -10.8050,  -7.4397],\n",
      "         [ -8.7201,   5.5792,  -6.3156,  ...,  -9.7018,  -6.4578,  -6.3143],\n",
      "         [ -7.5654,   4.2910,  -6.3322,  ...,  -7.6457,  -6.4475,  -6.7077],\n",
      "         ...,\n",
      "         [ -7.1973,   6.1752,  -5.6638,  ...,  -8.0961,  -6.7556,  -6.0956],\n",
      "         [ -7.1850,   6.0159,  -5.7232,  ...,  -8.0097,  -6.6893,  -6.0249],\n",
      "         [ -9.3135,   2.6623,  -4.5092,  ...,  -9.3736,  -7.3296,  -6.8187]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -7.3125,   3.7029,  -6.1729,  ...,  -7.7698,  -9.5332,  -6.5098],\n",
      "         [ -8.3580,   4.9176,  -8.6866,  ...,  -9.6402, -10.0915,  -8.6335],\n",
      "         [ -7.5616,   3.6869,  -7.0234,  ...,  -8.6194,  -8.7625,  -8.0406],\n",
      "         ...,\n",
      "         [ -6.8385,   4.1223,  -5.5127,  ...,  -7.6647,  -7.5966,  -4.7778],\n",
      "         [ -5.7248,   6.5853,  -4.2553,  ...,  -6.9058,  -7.2861,  -4.0763],\n",
      "         [ -5.8517,   6.5886,  -6.3464,  ...,  -6.7595,  -7.4399,  -4.1669]],\n",
      "\n",
      "        [[ -6.8322,   4.2029,  -5.6145,  ...,  -8.0769,  -9.8695,  -6.5307],\n",
      "         [ -5.1338,   7.8699,  -6.8049,  ...,  -5.4731,  -5.0539,  -4.4469],\n",
      "         [ -6.1668,   9.2534,  -6.4425,  ...,  -5.3864,  -5.0195,  -4.8949],\n",
      "         ...,\n",
      "         [ -7.3692,   3.7930,  -5.1245,  ...,  -8.7545,  -8.4321,  -6.5474],\n",
      "         [ -7.4128,   3.1075,  -4.3954,  ...,  -8.6694,  -8.7118,  -6.3908],\n",
      "         [ -6.8126,   4.8206,  -5.3005,  ...,  -6.5592,  -6.8895,  -5.4916]],\n",
      "\n",
      "        [[ -8.2185,   4.3043,  -5.7977,  ...,  -8.9044,  -9.8600,  -7.6821],\n",
      "         [ -8.2862,   6.9724,  -8.8395,  ...,  -6.8367,  -7.6141,  -8.2981],\n",
      "         [ -7.9461,   5.7974,  -6.8450,  ...,  -8.1928,  -9.3984,  -6.6741],\n",
      "         ...,\n",
      "         [ -9.0918,   4.5934,  -6.5685,  ...,  -7.7655,  -7.9328,  -6.4962],\n",
      "         [ -8.3552,   6.2746,  -7.1749,  ...,  -8.1648,  -6.4933,  -6.6368],\n",
      "         [ -9.0506,   5.4385,  -7.2100,  ...,  -7.8298,  -7.3830,  -6.9113]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), seq_relationship_logits=tensor([[-4.6890,  5.9755],\n",
      "        [-5.3445,  5.5925],\n",
      "        [-4.6278,  6.4208],\n",
      "        [-4.4958,  6.9508],\n",
      "        [-5.2649,  5.5195],\n",
      "        [-4.8125,  6.1236],\n",
      "        [-4.4961,  6.6109],\n",
      "        [-4.9402,  6.0110],\n",
      "        [-5.2829,  5.7504],\n",
      "        [-4.8759,  6.0065],\n",
      "        [-4.8722,  6.1668],\n",
      "        [-4.8324,  6.2782],\n",
      "        [-4.7602,  6.2227],\n",
      "        [-4.6969,  6.6166],\n",
      "        [-4.6327,  6.2502],\n",
      "        [-5.0901,  5.9510],\n",
      "        [-4.7213,  6.4094],\n",
      "        [-4.9108,  6.1961],\n",
      "        [-4.9804,  6.0245],\n",
      "        [-4.5220,  6.3945],\n",
      "        [-5.2783,  5.7013],\n",
      "        [-4.7402,  6.0861],\n",
      "        [-5.2510,  5.7400],\n",
      "        [-4.8935,  6.0633],\n",
      "        [-5.0064,  5.8353],\n",
      "        [-4.7738,  6.2739],\n",
      "        [-4.5194,  6.2237],\n",
      "        [-4.7940,  6.3075],\n",
      "        [-4.8556,  6.1382],\n",
      "        [-4.8881,  6.2557],\n",
      "        [-4.8725,  6.2774],\n",
      "        [-4.9090,  6.2581],\n",
      "        [-4.7699,  5.9022],\n",
      "        [-4.7123,  6.2255],\n",
      "        [-4.7343,  6.5112],\n",
      "        [-5.1606,  5.8191],\n",
      "        [-4.5682,  6.7885],\n",
      "        [-4.8130,  5.9953],\n",
      "        [-4.7139,  5.8537],\n",
      "        [-4.6749,  6.5193],\n",
      "        [-5.0632,  5.8823],\n",
      "        [-4.8342,  6.2063],\n",
      "        [-5.2588,  5.6792],\n",
      "        [-4.6439,  5.9259],\n",
      "        [-4.7928,  6.2576],\n",
      "        [-4.9029,  6.3579],\n",
      "        [-5.0392,  5.9997],\n",
      "        [-3.9859,  5.3718],\n",
      "        [-4.4356,  6.9741],\n",
      "        [-4.5855,  6.2102],\n",
      "        [-4.8909,  6.0236],\n",
      "        [-4.9096,  6.1852],\n",
      "        [-4.6613,  6.4449],\n",
      "        [-4.7254,  5.9858],\n",
      "        [-4.8187,  6.1403],\n",
      "        [-4.7004,  6.4489],\n",
      "        [-4.6490,  6.1694],\n",
      "        [-4.8895,  6.2219],\n",
      "        [-4.7708,  6.5018],\n",
      "        [-4.8669,  6.2657],\n",
      "        [-4.9752,  5.8890],\n",
      "        [-4.6304,  6.2307],\n",
      "        [-4.9449,  6.1200],\n",
      "        [-4.7771,  6.0144]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data = tuple(t.to(device) for t in data)\n",
    "input_ids, token_ids, mask, label, masked_token_idx = data\n",
    "outputs = model(input_ids, token_type_ids=token_ids, attention_mask=mask)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82f3299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_cls, logits_lm = outputs[1], outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b1893b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 35, 32000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f905e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 35, 32000)\n",
      "32000\n",
      "(64, 35)\n"
     ]
    }
   ],
   "source": [
    "logits_lm = logits_lm.to('cpu').detach().numpy()\n",
    "print(logits_lm.shape)\n",
    "print(len(logits_lm[0][0]))\n",
    "print(np.argmax(logits_lm,axis=2).shape)\n",
    "out = np.argmax(logits_lm,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f2778e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4559,  4559,  2630,  4559, 22550,  8387,  4559,  4559, 21155,\n",
       "        2016,  2170,  5002, 10470,  5419,    18,    18,    18,  4559,\n",
       "          18,  1504,  4559,  4559,  4559,    18,  4559,  4559,  4559,\n",
       "        4559,  4559,  4559,  4559,  4559,  4559,  4559,  4559],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52541142",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_token_idx_np = masked_token_idx.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d76738f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = []\n",
    "for i in range(len(out)):\n",
    "    l = token_topic_dict[out[i][masked_token_idx_np[i]]]\n",
    "    flatten.append(l)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ca38dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8fe7072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 6, 3, 5, 6, 2, 1, 6, 6, 4, 5, 4, 4, 2, 0, 4, 1, 3, 6, 0, 6, 3, 6, 4,\n",
       "        2, 3, 1, 5, 3, 5, 5, 6, 2, 1, 4, 6, 1, 1, 2, 5, 2, 2, 6, 1, 2, 1, 2, 5,\n",
       "        5, 3, 2, 3, 4, 3, 4, 5, 2, 2, 4, 5, 3, 1, 6, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_np = label.to(\"cpu\")\n",
    "label_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66e8863d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921875"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(label_np, flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "307b90aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(tokenizer.convert_ids_to_tokens(data[0][3]))\n",
    "# print(data[0][3])\n",
    "# print(data[1][3])\n",
    "# print(data[2][3])\n",
    "# print(data[3][3])\n",
    "# print(data[4][3])\n",
    "# print(tokenizer.convert_ids_to_tokens(out[3]))\n",
    "# print(out[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49c33498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 20134,  8527,  ...,     0,     0,     0],\n",
      "        [    2,   393,  3698,  ...,     0,     0,     0],\n",
      "        [    2,   268,  7054,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,  5621, 13344,  ...,     0,     0,     0],\n",
      "        [    2,  1485, 29945,  ...,     0,     0,     0],\n",
      "        [    2,  4989,  3698,  ...,     0,     0,     0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([19, 21, 21, 16, 22, 21, 23, 27, 17, 17, 19, 19, 14, 19, 24, 20, 21, 17,\n",
      "        22, 14, 24, 17, 19, 18, 18, 23, 17, 19, 22, 20, 18, 23, 25, 21, 23, 24,\n",
      "        22, 21, 15, 15, 20, 22, 18, 16, 22, 21, 15, 16, 19, 20, 21, 27, 18, 21,\n",
      "        18, 18, 23, 15, 18, 22, 18, 20, 21, 23])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(test_dataloader))\n",
    "for i in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f445b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del batch\n",
    "# del data\n",
    "# del outputs\n",
    "# del logits_lm\n",
    "# del logits_cls\n",
    "# del label_lms_pt\n",
    "# del b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx \n",
    "del train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d211824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\or7l0\\AppData\\Local\\Temp/ipykernel_31924/2249644255.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad9511c7ce34ceaadafbe2a5284d918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 6.57 GiB already allocated; 0 bytes free; 6.78 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31924/2249644255.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_token_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_masked_token_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_token_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlogits_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits_lm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, next_sentence_label, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1090\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 984\u001b[1;33m         embedding_output = self.embeddings(\n\u001b[0m\u001b[0;32m    985\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32md:\\workspace\\dacon_news_topic_clasiification\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 6.57 GiB already allocated; 0 bytes free; 6.78 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "predict_li = []\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_token_type_ids, b_input_mask, b_labels, b_masked_token_idx = batch\n",
    "    \n",
    "    outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits_cls, logits_lm = outputs[1], outputs[0]\n",
    "    \n",
    "    logits_lm = logits_lm.to('cpu').detach().numpy()\n",
    "    out = np.argmax(logits_lm,axis=2)\n",
    "    \n",
    "    masked_token_idx_np = b_masked_token_idx.to('cpu').numpy()\n",
    "    \n",
    "    print(out)\n",
    "    \n",
    "    for i in range(len(out)):\n",
    "        l = token_topic_dict[out[i][masked_token_idx_np[i]]]\n",
    "        predict_li.append(l)\n",
    "        \n",
    "    \n",
    "#     mask_label = [ topic_token_dict[lb] for lb in b_labels.to('cpu').numpy() ]\n",
    "#     label_lms = []\n",
    "#     SEQUENCE_LENGTH = 35\n",
    "\n",
    "#     for idx, label in zip(b_masked_token_idx.to('cpu').numpy(),mask_label):\n",
    "#         label_lm = np.full(SEQUENCE_LENGTH, dtype=np.int, fill_value=-1)\n",
    "#         label_lm[idx] = label\n",
    "#         label_lms.append(label_lm)\n",
    "\n",
    "#     label_lms_np = np.array(label_lms)\n",
    "#     label_lms_pt = torch.tensor(label_lms_np,dtype=torch.int64).to(device)\n",
    "#     loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), label_lms_pt.view(-1))\n",
    "\n",
    "#     labels_cls = [1 for _ in range(len(b_input_ids))]\n",
    "#     labels_cls = torch.tensor(labels_cls).to(device)\n",
    "#     loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "    \n",
    "#     loss = loss_cls + loss_lm\n",
    "    \n",
    "#     batch_loss += loss.item()\n",
    "    \n",
    "# avg_validation_loss = batch_loss / len(validation_dataloader)\n",
    "# print(F'\\n\\tAverage validation loss: {avg_validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ceb403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
